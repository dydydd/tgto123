from operator import inv
from pickle import NONE
import requests
import os
import shutil
import requests
from dotenv import load_dotenv
from bs4 import BeautifulSoup
import time
import sqlite3
from datetime import datetime, timedelta
from datetime import time as time_datetime
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from p123client import P123Client, check_response
from urllib.parse import urlsplit, parse_qs
import re
import telebot
import threading
import schedule
import json
import logging
from logging.handlers import TimedRotatingFileHandler
from collections import defaultdict
from content_check import check_porn_content

# è®¾ç½®httpxæ—¥å¿—çº§åˆ«ä¸ºWARNINGï¼Œé¿å…INFOçº§åˆ«çš„è¾“å‡º
logging.getLogger("httpx").setLevel(logging.ERROR)
logging.getLogger("urllib3.connectionpool").setLevel(logging.ERROR)
logging.getLogger("telebot").setLevel(logging.ERROR)
version = "6.7.5"  # ç‰ˆæœ¬æ›´æ–°
newest_id = 47
# åŠ è½½.envæ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv(dotenv_path="db/user.env",override=True)
load_dotenv(dotenv_path="sys.env",override=True)
# 1. ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
log_dir = os.path.join("db", "log")
os.makedirs(log_dir, exist_ok=True)
class MsFormatter(logging.Formatter):
    # é‡å†™æ—¶é—´æ ¼å¼åŒ–æ–¹æ³•
    def formatTime(self, record, datefmt=None):
        # å°†æ—¶é—´æˆ³è½¬æ¢ä¸ºåŒ…å«æ¯«ç§’çš„datetimeå¯¹è±¡
        dt = datetime.fromtimestamp(record.created)
        # æ ¼å¼åŒ–åˆ°æ¯«ç§’ï¼ˆå–å¾®ç§’çš„å‰3ä½ï¼‰
        return dt.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]  # ä¿ç•™åˆ°æ¯«ç§’
# ä½¿ç”¨è‡ªå®šä¹‰çš„Formatter
formatter = MsFormatter(
    fmt='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S.%f'  # è¿™é‡Œå¯ä»¥æ­£å¸¸ä½¿ç”¨%fäº†
)

root_logger = logging.getLogger()  # è·å–æ ¹æ—¥å¿—å™¨
root_logger.setLevel(logging.INFO)  # å…¨å±€æ—¥å¿—çº§åˆ«

if __name__ == "__mp_main__":
    file_handler = TimedRotatingFileHandler(
        filename=os.path.join(log_dir, "log.log"),
        when='D',          # æ¯å¤©è½®è½¬
        interval=1,        # é—´éš”1å¤©
        backupCount=3,     # æœ€å¤šä¿ç•™3å¤©æ—¥å¿—
        encoding='utf-8',
        atTime=time_datetime(0, 0, 1)
    )
    # è·å–å½“å‰æ—¥æœŸ
    today = datetime.now().date()
    # è®¡ç®—ä»Šå¤©çš„atTimeæ—¶é—´æˆ³
    today_at_time = datetime.combine(today, file_handler.atTime).timestamp()
    # å½“å‰æ—¶é—´æˆ³
    now = datetime.now().timestamp()
    # å¦‚æœå½“å‰æ—¶é—´åœ¨ä»Šå¤©çš„atTimeä¹‹å‰ï¼Œåˆ™é¦–æ¬¡è½®è½¬æ—¶é—´ä¸ºä»Šå¤©atTime
    # å¦‚æœå½“å‰æ—¶é—´å·²è¿‡ä»Šå¤©çš„atTimeï¼Œåˆ™é¦–æ¬¡è½®è½¬æ—¶é—´ä¸ºæ˜å¤©atTime
    if now < today_at_time:
        target_rollover = today_at_time
    else:
        target_rollover = datetime.combine(today + timedelta(days=1), file_handler.atTime).timestamp()
    # å¼ºåˆ¶ä¿®æ­£ä¸‹ä¸€æ¬¡è½®è½¬æ—¶é—´
    file_handler.rolloverAt = target_rollover
    
if __name__ == "__main__":
    file_handler = logging.FileHandler(
                        filename=os.path.join(log_dir, "start-log.log"),
                        encoding='utf-8'
                    )
console_handler = logging.StreamHandler()

# 4. å®šä¹‰å…¨å±€æ—¥å¿—æ ¼å¼ï¼ˆæ‰€æœ‰æ—¥å¿—å…±ç”¨ï¼‰
file_handler.setFormatter(formatter)
console_handler.setFormatter(formatter)
# 6. å°†å¤„ç†å™¨æ·»åŠ åˆ°æ ¹æ—¥å¿—å™¨ï¼ˆå…³é”®ï¼šæ ¹æ—¥å¿—å™¨çš„é…ç½®ä¼šè¢«æ‰€æœ‰å­loggerç»§æ‰¿ï¼‰
root_logger.addHandler(file_handler)
root_logger.addHandler(console_handler)
# ----------------------
# æµ‹è¯•ï¼šä»»æ„æ¨¡å—çš„loggeréƒ½ä¼šéµå¾ªå…¨å±€é…ç½®
# ----------------------
# ç¤ºä¾‹1ï¼šå½“å‰æ¨¡å—çš„logger
logger = logging.getLogger(__name__)
import threading
import concurrent.futures
# åˆ›å»ºå¤§å°ä¸º1çš„çº¿ç¨‹æ± ç”¨äºå‘é€æ¶ˆæ¯
reply_thread_pool = concurrent.futures.ThreadPoolExecutor(max_workers=20)
# å®‰å…¨åœ°è·å–æ•´æ•°å€¼ï¼Œé¿å…å¼‚å¸¸
def get_int_env(env_name, default_value=0):
    try:
        value = os.getenv(env_name, str(default_value))
        return int(value) if value else default_value
    except (ValueError, TypeError):
        reply_thread_pool.submit(send_message,f"[è­¦å‘Š] ç¯å¢ƒå˜é‡ {env_name} å€¼ä¸æ˜¯æœ‰æ•ˆçš„æ•´æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼ {default_value}")
        logger.warning(f"ç¯å¢ƒå˜é‡ {env_name} å€¼ä¸æ˜¯æœ‰æ•ˆçš„æ•´æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼ {default_value}")
        return default_value
CHANNEL_URL = os.getenv("ENV_TG_CHANNEL", "")

AUTO_MAKE_JSON = get_int_env("ENV_AUTO_MAKE_JSON", 1)

#TG BOTçš„token
TG_BOT_TOKEN = os.getenv("ENV_TG_BOT_TOKEN", "")
#TG ç”¨æˆ·ID
TG_ADMIN_USER_ID = get_int_env("ENV_TG_ADMIN_USER_ID", 0)

#æ˜¯å¦å¼€å¯ç›‘æ§åŠŸèƒ½ï¼Œ1ä¸ºå¼€å¯ï¼Œ0ä¸ºå…³é—­
AUTHORIZATION = get_int_env("ENV_AUTHORIZATION", 0)
#123è´¦å·
CLIENT_ID = os.getenv("ENV_123_CLIENT_ID", "")
DIY_LINK_PWD = os.getenv("ENV_DIY_LINK_PWD", "")
#123å¯†ç 
CLIENT_SECRET = os.getenv("ENV_123_CLIENT_SECRET", "")
FILTER = os.getenv("ENV_FILTER", "")
filter_pattern = re.compile(FILTER, re.IGNORECASE)
#éœ€è¦è½¬å­˜çš„123ç›®å½•ID
UPLOAD_TARGET_PID = get_int_env("ENV_123_UPLOAD_PID", 0)

UPLOAD_JSON_TARGET_PID = get_int_env("ENV_123_JSON_UPLOAD_PID", 0)
UPLOAD_LINK_TARGET_PID = get_int_env("ENV_123_LINK_UPLOAD_PID", UPLOAD_JSON_TARGET_PID)
USE_METHOD="ğŸ” ä½¿ç”¨æ–¹æ³•ï¼š\n      1ã€åˆ›å»ºåˆ†äº«è¯·ä½¿ç”¨ /share å…³é”®è¯ æ¥æœç´¢æ–‡ä»¶å¤¹ï¼Œä¾‹å¦‚ï¼š/share æƒåŠ›çš„æ¸¸æˆ\n      2ã€è½¬å­˜åˆ†äº«å¯ç›´æ¥æŠŠ123ã€115ã€å¤©ç¿¼é“¾æ¥è½¬å‘è‡³æ­¤ï¼Œæ”¯æŒé¢‘é“ä¸­å¸¦å›¾ç‰‡çš„é‚£ç§åˆ†äº«\n      3ã€è½¬å­˜ç§’ä¼ jsonå¯ç›´æ¥æŠŠjsonè½¬å‘è‡³æ­¤\n      4ã€è½¬å­˜ç§’ä¼ é“¾æ¥å¯ç›´æ¥æŠŠç§’ä¼ é“¾æ¥è½¬å‘è‡³æ­¤\n      5ã€123æ‰¹é‡ç¦»çº¿ç£åŠ›é“¾è¯·ç›´æ¥æŠŠç£åŠ›é“¾å‘é€è‡³æ­¤\n      6ã€åˆ›å»ºå®Œæˆåˆ†äº«é“¾æ¥åå¯ä¸€é”®å‘å¸–è‡³123èµ„æºç¤¾åŒº\n      7ã€123ã€115ã€å¤©ç¿¼ç­‰é¢‘é“ç›‘æ§è½¬å­˜åœ¨åå°å®šæ—¶æ‰§è¡Œ\n      8ã€PTä¸Šä¸‹è½½çš„æœ¬åœ°æ–‡ä»¶æ— é™å°è¯•ç§’ä¼ 123æˆ–115ç½‘ç›˜ï¼Œä»¥é¿å…è¿è¥å•†åˆ¶è£ï¼Œéœ€è¦é…ç½®composeé‡Œçš„è·¯å¾„æ˜ å°„\n      9ã€è®¿é—® http://127.0.0.1:12366/d/file (ä¾‹å¦‚ http://127.0.0.1:12366/d/æƒåŠ›çš„æ¸¸æˆ.mp4) å³å¯è·å–123æ–‡ä»¶ä¸‹è½½ç›´é“¾\n      10ã€æ”¯æŒmisaka_danmu_serverå¼¹å¹•æœåŠ¡ï¼Œå½“è§¦å‘302æ’­æ”¾æ—¶ï¼Œä¼šè‡ªåŠ¨è°ƒç”¨misaka_danmu_server APIæ¥ä¸‹è½½å¯¹åº”é›†ä»¥åŠä¸‹ä¸€é›†çš„å¼¹å¹•\n      11ã€æ”¯æŒ123è½¬å­˜å¤¸å…‹åˆ†äº«ï¼ˆåŸç†æ˜¯ä»å¤¸å…‹åˆ†äº«ç”Ÿæˆç§’ä¼ ç»™123è½¬å­˜ï¼‰\nâš ï¸ æ³¨ï¼šä»¥ä¸ŠåŠŸèƒ½çš„ä½¿ç”¨éœ€è¦åœ¨ NasIP:12366ï¼ˆå¦‚192.168.1.1:12366ï¼‰çš„é…ç½®é¡µé¢å®ŒæˆåŠŸèƒ½é…ç½®"
# æ•°æ®åº“è·¯å¾„ï¼ˆä¿æŒä¸å˜ï¼‰
DB_DIR = "db"
if not os.path.exists(DB_DIR):
    os.makedirs(DB_DIR)
DATABASE_FILE = os.path.join(DB_DIR, "TG_monitor-123.db")
USER_STATE_DB = os.path.join(DB_DIR, "user_states.db")
CHECK_INTERVAL = get_int_env("ENV_CHECK_INTERVAL", 0)
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.5 Safari/605.1.15"
]
RETRY_TIMES = 3
TIMEOUT = 15

TOKENSHARE = os.getenv("TOKEN", "")
if TOKENSHARE:
    botshare = telebot.TeleBot(TOKENSHARE)
#TG ç”¨æˆ·ID
    TARGET_CHAT_ID_SHARE = get_int_env("TARGET_CHAT_ID", 0)

from share import get_quality
import re
from urllib.parse import urlparse, parse_qs

def parse_share_url(share_url):
    """è§£æåˆ†äº«é“¾æ¥ï¼Œæå–ShareKeyå’Œæå–ç """
    try:
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…åˆ†äº«é“¾æ¥
        pattern = r'(https?://(?:[a-zA-Z0-9-]+\.)*123[a-zA-Z0-9-]*\.[a-z]{2,6}/s/([a-zA-Z0-9\-_]+))'
        match = re.search(pattern, share_url, re.IGNORECASE)

        if match:
            # å®Œæ•´URL
            full_url = match.group(1)
            # ShareKey
            share_key = match.group(2)
            # å°è¯•ä»åŸå§‹URLæŸ¥è¯¢å‚æ•°ä¸­è·å–æå–ç 
            parsed = urlparse(share_url)
            query_params = parse_qs(parsed.query)
            share_pwd = query_params.get('pwd', [''])[0]
            return share_key, share_pwd

        logger.warning(f"æ— æ³•è§£æåˆ†äº«é“¾æ¥: {share_url}")
        return '', ''
    except Exception as e:
        logger.error(f"è§£æåˆ†äº«é“¾æ¥å¤±è´¥: {str(e)}")
        return '', ''

def recursive_count_files(client: P123Client, parent_file_id, share_key, share_pwd):
    """é€’å½’è·å–åˆ†äº«ä¸­çš„æ–‡ä»¶å¹¶ç»Ÿè®¡è§†é¢‘æ–‡ä»¶æ•°é‡"""
    logger.info(f"å¼€å§‹é€’å½’è·å–åˆ†äº«ä¸­çš„æ–‡ä»¶æ•°é‡ï¼Œæ–‡ä»¶å¤¹ID: {parent_file_id}")
    video_extensions = {'.mkv', '.ts', '.mp4', '.avi', '.rmvb', '.wmv', '.m2ts', '.mpg', '.flv', '.rm', '.mov', '.iso'}
    video_count = 0
    try:
        page = 1
        while True:
            resp = client.share_fs_list({
                "ShareKey": share_key,
                "SharePwd": share_pwd,
                "parentFileId": parent_file_id,
                "limit": 100,
                "Page": page
            })
            check_response(resp)
            data = resp["data"]

            if data and "InfoList" in data:
                for item in data["InfoList"]:
                    if item["Type"] == 1:  # ç›®å½•
                        # é€’å½’è®¡ç®—å­ç›®å½•ä¸­çš„è§†é¢‘æ–‡ä»¶
                        video_count += recursive_count_files(client, item["FileId"], share_key, share_pwd)
                    else:  # æ–‡ä»¶
                        # æ£€æŸ¥æ˜¯å¦ä¸ºè§†é¢‘æ–‡ä»¶
                        ext = os.path.splitext(item["FileName"])[1].lower()
                        if ext in video_extensions:
                            video_count += 1
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€åä¸€é¡µ
            if not data or len(data.get("InfoList", [])) < 100:
                break            
            page += 1
    except Exception as e:
        logger.error(f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥ï¼ˆçˆ¶ID: {parent_file_id}ï¼‰: {str(e)}")
        raise
    return video_count

def build_share_message(metadata, client, file_id, folder_name, file_name, share_info):
    # ä½¿ç”¨å…ƒæ•°æ®ç¾åŒ–æ¶ˆæ¯
    #logger.info(get_first_video_file(client, file_id))
    get_quality(file_name)

    poster_url = metadata.get('backdrop', '').strip('` ') or metadata.get('poster', '').strip('` ')
    # å†…å®¹ç±»å‹åˆ¤æ–­ 
    content_type = 'ğŸ“º ç”µè§†å‰§' if 'seasons' in metadata and 'episodes' in metadata else 'ğŸ¬ ç”µå½±' 
    # æ„å»ºæ ‡é¢˜è¡Œ 
    share_message = f"{content_type}ï½œ{metadata.get('title')} ({metadata.get('year')})\n\n" 
    # è¯„åˆ† 
    genres = metadata.get('genres', [])[0] if metadata.get('genres', []) else ''
    share_message += f"â­ï¸ è¯„åˆ†: {metadata.get('rating')} / åœ°åŒº: {', '.join(metadata.get('countries', []))} / ç±»å‹: {genres[:15]}{'...' if len(genres) > 15 else ''}\n" 
    # ç±»å‹ 
    #genres = ', '.join(metadata.get('genres', []))
    #share_message += f"ğŸ“½ï¸ ç±»å‹: {genres[:15]}{'...' if len(genres) > 15 else ''}\n" 
    # åœ°åŒº 
    #share_message += f"ğŸŒ åœ°åŒº: {', '.join(metadata.get('countries', []))}\n" 
    # è¯­è¨€ 
    # share_message += f"ğŸ—£ è¯­è¨€: {', '.join(metadata.get('languages', ['æœªçŸ¥']))}\n" 
    # å¯¼æ¼” 
    if metadata.get('director'): 
        share_message += f"ğŸ¬ å¯¼æ¼”: {metadata.get('director', '')[:10]}{'...' if len(metadata.get('director', '')) > 10 else ''}\n" 
    # ä¸»æ¼” 
    share_message += f"ğŸ‘¥ ä¸»æ¼”: {metadata.get('cast', '')[:10]}{'...' if len(metadata.get('cast', '')) > 10 else ''}\n" 
    # é›†æ•°ï¼ˆå¦‚é€‚ç”¨ï¼‰ 
    if 'seasons' in metadata and 'episodes' in metadata: 
        share_message += f"ğŸ“º å…±{metadata.get('seasons')}å­£ ({metadata.get('episodes')}é›†)\n" 
    # ç®€ä»‹ï¼ˆä½¿ç”¨blockquoteï¼‰ 
    # ä»åˆ†äº«é“¾æ¥ä¸­è§£æShareKeyå’Œæå–ç 
    share_key, share_pwd = parse_share_url(share_info['url'])
    share_pwd = share_pwd or share_info.get('password','')  
    # è·å–æ–‡ä»¶å¤¹å†…æ–‡ä»¶åˆ—è¡¨
    files = get_directory_files(client, file_id, folder_name)
    logger.info(f"è·å–å®é™…æ–‡ä»¶æ•°é‡: {len(files)}")
    actual_video_count = recursive_count_files(client, file_id, share_key, share_pwd)
    logger.info(f"è·å–åˆ†äº«ä¸­çš„æ–‡ä»¶æ•°é‡: {actual_video_count}")
    # å®šä¹‰è§†é¢‘æ–‡ä»¶æ‰©å±•å
    video_extensions = {'.mkv', '.ts', '.mp4', '.avi', '.rmvb', '.wmv', '.m2ts', '.mpg', '.flv', '.rm', '.mov', '.iso'}
    # ç­›é€‰è§†é¢‘æ–‡ä»¶
    video_files = []
    for file_info in files:
        filename = file_info["path"]
        ext = os.path.splitext(filename)[1].lower()
        if ext in video_extensions:
            video_files.append(file_info)
    
    if not video_files:
        file_info_text = f"ğŸ“ æ²¡æœ‰æ‰¾åˆ°è§†é¢‘æ–‡ä»¶ | å®é™…è§†é¢‘æ•°é‡: {actual_video_count}"
        file_info_text2 = f"ğŸ“ æ²¡æœ‰æ‰¾åˆ°è§†é¢‘æ–‡ä»¶"
    else:
        total_files_count = len(video_files)
        total_size = sum(file_info["size"] for file_info in video_files)
        # è®¡ç®—å¹³å‡å¤§å°
        avg_size = total_size / total_files_count if total_files_count > 0 else 0
        # æ ¼å¼åŒ–æ–‡ä»¶å¤§å°
        if total_size < 1024:
            size_str = f"{total_size} B"
        elif total_size < 1024 * 1024:
            size_str = f"{total_size / 1024:.2f} KB"
        elif total_size < 1024 * 1024 * 1024:
            size_str = f"{total_size / (1024 * 1024):.2f} MB"
        elif total_size < 1024 * 1024 * 1024 * 1024:
            size_str = f"{total_size / (1024 * 1024 * 1024):.2f} GB"
        else:
            size_str = f"{total_size / (1024 * 1024 * 1024 * 1024):.2f} TB"
        # æ ¼å¼åŒ–å¹³å‡å¤§å°
        if avg_size < 1024:
            avg_size_str = f"{avg_size:.2f} B"
        elif avg_size < 1024 * 1024:
            avg_size_str = f"{avg_size / 1024:.2f} KB"
        elif avg_size < 1024 * 1024 * 1024:
            avg_size_str = f"{avg_size / (1024 * 1024):.2f} MB"
        elif avg_size < 1024 * 1024 * 1024 * 1024:
            avg_size_str = f"{avg_size / (1024 * 1024 * 1024):.2f} GB"
        else:
            avg_size_str = f"{avg_size / (1024 * 1024 * 1024 * 1024):.2f} TB"
        file_info_text = f"ğŸ¬ è§†é¢‘æ•°é‡: {total_files_count} | æ€»å¤§å°: {size_str} | å¹³å‡å¤§å°ï¼š{avg_size_str} | å®é™…è§†é¢‘æ•°é‡: {actual_video_count} | å·²å’Œè°ï¼š{total_files_count-actual_video_count}"
        file_info_text2 = f"ğŸ¬ è§†é¢‘æ•°é‡: {total_files_count} | æ€»å¤§å°: {size_str} | å¹³å‡å¤§å°ï¼š{avg_size_str}" 
    share_message2 = share_message
    share_message2 += f"\nğŸ“– ç®€ä»‹: <blockquote expandable=\"\">{metadata.get('plot')[:500]}{'...' if len(metadata.get('plot')) > 500 else ''}</blockquote>\n\n{file_info_text2}\n"
    share_message += f"\nğŸ“– ç®€ä»‹: <blockquote expandable=\"\">{metadata.get('plot')[:500]}{'...' if len(metadata.get('plot')) > 500 else ''}</blockquote>\n\n{file_info_text}\n" 
    quality = get_quality(get_first_video_file(client, file_id))
    if quality:
        share_message += f"ğŸ· è§†é¢‘è´¨é‡: {quality}\n"
        share_message2 += f"ğŸ· è§†é¢‘è´¨é‡: {quality}\n"
    share_message += f"ğŸ”— é“¾æ¥: {share_info['url']}{'?pwd=' + share_info['password'] if share_info.get('password') else ''}\n" 
    #share_message += f"ğŸ”— é“¾æ¥: <a href=\"{share_info['url']}{'?pwd=' + share_info['password'] if share_info.get('password') else ''}\" target=\"_blank\" rel=\"noopener\" onclick=\"return confirm('Open this link?\n\n'+this.href);\">æŸ¥çœ‹é“¾æ¥</a>\n"
    share_message += f"ğŸ™‹ æ¥è‡ª123botè‡ªåŠ¨åˆ›å»ºçš„åˆ†äº«" 
    share_message2 += f"ğŸ™‹ æ¥è‡ª123botè‡ªåŠ¨åˆ›å»ºçš„åˆ†äº«" 
    return share_message, share_message2, poster_url, files

def get_directory_files(client: P123Client, directory_id, folder_name, current_path="", is_root=True):
    """
    è·å–ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ï¼ˆä½¿ç”¨V2 APIï¼‰
    directory_id: ç›®å½•ID
    folder_name: æ–‡ä»¶å¤¹åç§°
    current_path: å½“å‰è·¯å¾„ï¼Œç”¨äºæ„å»ºå®Œæ•´çš„ç›¸å¯¹è·¯å¾„
    """
    # å¯¹äºæ ¹ç›®å½•ï¼ŒcommonPathå°±æ˜¯folder_name
    # å¯¹äºå­ç›®å½•ï¼Œcurrent_pathæ˜¯ç›¸å¯¹äºcommonPathçš„è·¯å¾„
    if is_root:
        common_path = folder_name
        # æ ¹ç›®å½•çš„current_pathä¸ºç©º
        current_path = ""
    else:
        common_path = current_path.split('/')[0] if current_path else folder_name

    # æ„å»ºå½“å‰ç›¸å¯¹äºcommonPathçš„è·¯å¾„
    # å¯¹äºæ ¹ç›®å½•ï¼Œrelative_pathä¸ºç©º
    # å¯¹äºå­ç›®å½•ï¼Œrelative_pathæ˜¯ç›¸å¯¹äºcommonPathçš„è·¯å¾„
    if is_root:
        relative_path = ""
    else:
        relative_path = f"{current_path}/{folder_name}" if current_path else folder_name
        # ç§»é™¤å¼€å¤´å¯èƒ½çš„/
        relative_path = relative_path.lstrip('/')
    logger.info(f"è·å–ç›®å½•å†…å®¹ (ID: {directory_id}, commonPath: '{common_path}', ç›¸å¯¹è·¯å¾„: '{relative_path}')")
    all_files = []
    OPEN_API_HOST = "https://open-api.123pan.com"
    API_PATHS = {
        'LIST_FILES_V2': '/api/v2/file/list'
    }
    retry_delay = 31  # é‡è¯•å»¶è¿Ÿç§’æ•°

    # ä½¿ç”¨V2 APIè·å–ç›®å½•å†…å®¹
    last_file_id = 0  # åˆå§‹å€¼ä¸º0
    while True:
        url = f"{OPEN_API_HOST}{API_PATHS['LIST_FILES_V2']}"
        params = {
            "parentFileId": directory_id,
            "trashed": 0,  # æ’é™¤å›æ”¶ç«™æ–‡ä»¶
            "limit": 100,   # æœ€å¤§ä¸è¶…è¿‡100
            "lastFileId": last_file_id
        }
        headers = {
            "Authorization": f"Bearer {client.token}",
            "Platform": "open_platform",
            "Content-Type": "application/json"
        }

        try:
            logger.info(f"è¯·æ±‚ç›®å½•åˆ—è¡¨: {url}, å‚æ•°: {params}")
            response = requests.get(url, params=params, headers=headers, timeout=30)
            if not response:
                logger.error(f"è·å–ç›®å½•åˆ—è¡¨å¤±è´¥")
                return all_files

            if response.status_code != 200:
                logger.error(f"è·å–ç›®å½•åˆ—è¡¨å¤±è´¥: HTTP {response.status_code}")
                return all_files

            try:
                data = response.json()
            except json.JSONDecodeError as e:
                logger.error(f"å“åº”JSONè§£æå¤±è´¥: {str(e)}")
                logger.error(f"å®Œæ•´å“åº”: {response.text}")
                return all_files

            if data.get("code") != 0:
                error_msg = data.get("message", "æœªçŸ¥é”™è¯¯")
                
                # å¦‚æœæ˜¯é™æµé”™è¯¯ï¼Œç­‰å¾…åé‡è¯•
                if "æ“ä½œé¢‘ç¹" in error_msg or "é™æµ" in error_msg:
                    logger.warning(f"APIé™æµ: {error_msg}, ç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                    time.sleep(retry_delay)
                    continue
                
                logger.error(f"APIé”™è¯¯: {error_msg}")
                return all_files

            # å¤„ç†å½“å‰é¡µçš„æ–‡ä»¶
            for item in data["data"].get("fileList", []):
                # æ’é™¤å›æ”¶ç«™æ–‡ä»¶
                if item.get("trashed", 1) != 0:
                    continue
                
                # æ„å»ºæ–‡ä»¶ç›¸å¯¹è·¯å¾„
                item_path = item['filename']
                
                if item["type"] == 0:  # æ–‡ä»¶
                    # æ„å»ºç›¸å¯¹äºcommonPathçš„è·¯å¾„ï¼ˆä½¿ç”¨/ä½œä¸ºåˆ†éš”ç¬¦ï¼‰
                    # æ³¨æ„ï¼šä¸åŒ…å«commonPath
                    if relative_path:
                        full_item_path = f"{relative_path}/{item_path}"
                    else:
                        full_item_path = item_path
                    # ç¡®ä¿ä½¿ç”¨/ä½œä¸ºåˆ†éš”ç¬¦
                    full_item_path = full_item_path.replace('\\', '/')
                    file_info = {
                        "path": full_item_path,  # å­˜å‚¨ç›¸å¯¹äºcommonPathçš„è·¯å¾„
                        "etag": item["etag"],
                        "size": item["size"]
                    }
                    all_files.append(file_info)
                elif item["type"] == 1:  # æ–‡ä»¶å¤¹
                    # é€’å½’è·å–å­ç›®å½•ï¼ˆæ·»åŠ å»¶è¿Ÿé¿å…é™æµï¼‰
                    #time.sleep(0.05)  # å¢åŠ å»¶è¿Ÿ
                    sub_files = get_directory_files(
                        client,
                        item["fileId"],
                        item['filename'],
                        relative_path,
                        False
                    )
                    all_files.extend(sub_files)

            # æ£€æŸ¥æ˜¯å¦æœ‰æ›´å¤šé¡µé¢
            last_file_id = data["data"].get("lastFileId", -1)
            #time.sleep(0.05)
            if last_file_id == -1:
                break
                
        except Exception as e:
            logger.error(f"è·å–ç›®å½•åˆ—è¡¨å‡ºé”™: {str(e)}")
            return all_files

    logger.info(f"æ‰¾åˆ° {len(all_files)} ä¸ªæ–‡ä»¶ (ID: {directory_id})")
    return all_files

# å…¨å±€å˜é‡ï¼ˆä½¿ç”¨å®‰å…¨çš„æ–¹å¼åˆå§‹åŒ–botï¼‰
# å¤„ç†JSONæ–‡ä»¶è½¬å­˜

import time
# åˆ›å»ºé”å¯¹è±¡ç¡®ä¿æ–‡ä»¶ä¾æ¬¡è½¬å­˜
json_process_lock = threading.Lock()

# è·Ÿè¸ªä¸Šæ¬¡å‘é€æ¶ˆæ¯çš„æ—¶é—´
last_send_time = 0
RETRY_DELAY = 60  # é‡è¯•ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
MAX_RETRIES = 30  # æœ€å¤§é‡è¯•æ¬¡æ•°
# å®šä¹‰çº¿ç¨‹æ± ä¸­çš„å‘é€å‡½æ•°
def send_message(text):
    send_retry_count = 0
    while send_retry_count < MAX_RETRIES:
        try:
            bot.send_message(TG_ADMIN_USER_ID, text)
            logger.info(f"æ¶ˆæ¯ '{text.replace('\n', '').replace('\r', '')[:20]}...' ï¼Œå·²æˆåŠŸå‘é€ç»™ç”¨æˆ· {TG_ADMIN_USER_ID}ï¼ˆç¬¬{send_retry_count+1}/{MAX_RETRIES}æ¬¡å°è¯•ï¼‰")
            break
        except Exception as e:
            logger.error(f"å‘é€å›å¤å¤±è´¥ï¼Œ{RETRY_DELAY}ç§’åé‡å‘ï¼Œæ¶ˆæ¯ï¼š{text}ï¼Œé”™è¯¯ï¼š{str(e)}")
            time.sleep(RETRY_DELAY)
            send_retry_count += 1

def send_message_with_id(chatid, text):
    send_retry_count = 0
    while send_retry_count < MAX_RETRIES:
        try:
            bot.send_message(chatid, text)
            logger.info(f"æ¶ˆæ¯ '{text.replace('\n', '').replace('\r', '')[:20]}...' ï¼Œå·²æˆåŠŸå‘é€ç»™ç”¨æˆ· {chatid}ï¼ˆç¬¬{send_retry_count+1}/{MAX_RETRIES}æ¬¡å°è¯•ï¼‰")
            break
        except Exception as e:
            logger.error(f"å‘é€å›å¤å¤±è´¥ï¼Œ{RETRY_DELAY}ç§’åé‡å‘ï¼Œæ¶ˆæ¯ï¼š{text}ï¼Œé”™è¯¯ï¼š{str(e)}")
            time.sleep(RETRY_DELAY)
            send_retry_count += 1

def send_reply(message, text):
    send_retry_count = 0
    while send_retry_count < MAX_RETRIES:
        try:
            bot.reply_to(message, text)
            logger.info(f"æ¶ˆæ¯ '{text.replace('\n', '').replace('\r', '')[:20]}...' ï¼Œå·²æˆåŠŸå‘é€ç»™ç”¨æˆ· {message.chat.id}ï¼ˆç¬¬{send_retry_count+1}/{MAX_RETRIES}æ¬¡å°è¯•ï¼‰")
            break
        except Exception as e:
            logger.error(f"å‘é€å›å¤å¤±è´¥ï¼Œ{RETRY_DELAY}ç§’åé‡å‘ï¼Œæ¶ˆæ¯ï¼š{text}ï¼Œé”™è¯¯ï¼š{str(e)}")
            time.sleep(RETRY_DELAY)
            send_retry_count += 1

def send_reply_delete(message, text):
    global last_send_time
    current_time = time.time()
    if current_time - last_send_time < 10:
        #logger.info(f"[èŠ‚æµ] 10ç§’å†…å·²å‘é€æ¶ˆæ¯ï¼Œå¿½ç•¥å½“å‰æ¶ˆæ¯: {text}")
        return
    # é™åˆ¶æ–‡æœ¬é•¿åº¦ï¼Œä¿ç•™å¼€å¤´å’Œæœ«å°¾çš„200å­—ç¬¦
    max_length = 400
    if len(text) > max_length:
        text = text[:200] + '\n     ......\n' + text[-200:]  
    try:
        sent_message = bot.reply_to(message, text)
        # æ›´æ–°ä¸Šæ¬¡å‘é€æ—¶é—´
        last_send_time = current_time
        time.sleep(12)  # ç­‰å¾…10ç§’ååˆ é™¤æ¶ˆæ¯
        bot.delete_message(chat_id=sent_message.chat.id, message_id=sent_message.message_id)
    except Exception as e:
        logger.error(f"å‘é€å›å¤å¤±è´¥: {str(e)}")
bot = telebot.TeleBot(TG_BOT_TOKEN)
from telebot.types import BotCommand
# å®‰å…¨åˆå§‹åŒ–TeleBot
while True and __name__ == "__mp_main__":
    try:
        bot = telebot.TeleBot(TG_BOT_TOKEN)
        # å®šä¹‰å‘½ä»¤èœå•ï¼ˆåŒ…å«/startå’Œ/shareï¼‰
        commands = [
            BotCommand("start", "å¼€å§‹ä½¿ç”¨æœºå™¨äºº"),
            BotCommand("share", "åˆ›å»ºåˆ†äº«é“¾æ¥"),
            BotCommand("info", "æ‰“å°å½“å‰è´¦æˆ·çš„ä¿¡æ¯"),
            BotCommand("add", "æ·»åŠ 123ç›‘æ§è¿‡æ»¤è¯ï¼Œå‘é€/addå¯æŸ¥çœ‹ä½¿ç”¨æ–¹æ³•"),
            BotCommand("remove", "åˆ é™¤123ç›‘æ§è¿‡æ»¤è¯ï¼Œå‘é€/removeå¯æŸ¥çœ‹ä½¿ç”¨æ–¹æ³•"),
            BotCommand("zhuli115", "115æœ€æ–°æ´»åŠ¨å¹¸è¿5åˆ†é’Ÿè‡ªåŠ¨åŠ©åŠ›ï¼Œæ”¯æŒå¤šä¸ªåŠ©åŠ›ç ï¼Œä¾‹å¦‚/zhuli115 AAAAAA BBBBBB CCCCCC")
        ]
        # è®¾ç½®å‘½ä»¤èœå•
        bot.set_my_commands(commands)
        logger.info("å·²è®¾ç½®Botå‘½ä»¤èœå•ï¼š/start, /share, /info, /add, /remove")
        logger.info("TeleBotåˆå§‹åŒ–æˆåŠŸ")
        break  # åˆå§‹åŒ–æˆåŠŸï¼Œé€€å‡ºå¾ªç¯
    except Exception as e:
        logger.error(f"ç”±äºç½‘ç»œç­‰åŸå› æ— æ³•ä¸TG Botå»ºç«‹é€šä¿¡ï¼Œ30ç§’åé‡è¯•...: {str(e)}")
        time.sleep(30)

# åˆå§‹åŒ–123å®¢æˆ·ç«¯
def init_123_client(retry: bool = False) -> P123Client:
    import requests
    token_path = os.path.join(DB_DIR, "config.txt")
    token = None
    
    # å°è¯•åŠ è½½æŒä¹…åŒ–çš„token
    if os.path.exists(token_path):
        try:
            with open(token_path, "r", encoding="utf-8") as f:
                token = f.read().strip()
            logger.info("å·²åŠ è½½æŒä¹…åŒ–token")
        except Exception as e:
            logger.warning(f"è¯»å–tokenæ–‡ä»¶å¤±è´¥ï¼š{e}ï¼Œå°†é‡æ–°è·å–")
    
    # å°è¯•ä½¿ç”¨tokenåˆå§‹åŒ–å®¢æˆ·ç«¯
    if token:
        while True:
            try:
                client = P123Client(token=token)
                res = client.user_info()  # éªŒè¯tokenæœ‰æ•ˆæ€§

                # æ£€æŸ¥APIè¿”å›ç»“æœæ˜¯å¦è¡¨ç¤ºtokenè¿‡æœŸ
                if res.get('code') != 0 or res.get('message') != "ok":
                    reply_thread_pool.submit(send_message, "123 tokenè¿‡æœŸï¼Œå°†é‡æ–°è·å–")
                    logger.info("æ£€æµ‹åˆ°tokenè¿‡æœŸï¼Œå°†é‡æ–°è·å–")
                    if os.path.exists(token_path):
                        os.remove(token_path)
                    break
                else:
                    logger.info("123å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼ˆä½¿ç”¨æŒä¹…åŒ–tokenï¼‰")
                    return client
            except Exception as e:
                if "token is expired" in str(e).lower() or (
                        hasattr(e, 'args') and "token is expired" in str(e.args).lower()):
                    logger.info("æ£€æµ‹åˆ°tokenè¿‡æœŸï¼Œå°†é‡æ–°è·å–")
                    if os.path.exists(token_path):
                        os.remove(token_path)
                    break
                else:
                    logger.warning(f"tokenå¥åº·æ£€æŸ¥å¼‚å¸¸ï¼Œç¨åé‡è¯•ï¼š{e}")
                    time.sleep(RETRY_DELAY)
                

    # é€šè¿‡APIæ¥å£è·å–æ–°token
    try:

        # ä½¿ç”¨æ–°tokenåˆå§‹åŒ–å®¢æˆ·ç«¯
        client = P123Client(CLIENT_ID, CLIENT_SECRET)
        with open(token_path, "w", encoding="utf-8") as f:
            f.write(client.token)

        logger.info("123å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼ˆä½¿ç”¨æ–°è·å–çš„tokenï¼‰")
        return client
    except Exception as e:
        if not retry:
            logger.error(f"è·å–tokenå¤±è´¥ï¼š{e}ï¼Œå°è¯•é‡è¯•...")
            return init_123_client(retry=True)
        logger.error(f"è·å–tokenå¤±è´¥ï¼ˆå·²é‡è¯•ï¼‰ï¼š{e}")
        raise


# æ•°æ®åº“ç›¸å…³å‡½æ•°ï¼ˆä¿æŒä¸å˜ï¼‰
def init_database():
    conn = sqlite3.connect(DATABASE_FILE)
    conn.execute('''CREATE TABLE IF NOT EXISTS messages
                  (msg_id INTEGER PRIMARY KEY AUTOINCREMENT, id TEXT, date TEXT, message_url TEXT, target_url TEXT, 
                   transfer_status TEXT, transfer_time TEXT, transfer_result TEXT)''')
    conn.commit()
    conn.close()


def is_message_processed(message_url):
    """æ£€æŸ¥æ¶ˆæ¯æ˜¯å¦å·²å¤„ç†ï¼ˆæ— è®ºè½¬å­˜æ˜¯å¦æˆåŠŸï¼‰"""
    conn = sqlite3.connect(DATABASE_FILE)
    result = conn.execute("SELECT 1 FROM messages WHERE message_url = ?",
                          (message_url,)).fetchone()
    conn.close()
    return result is not None


def save_message(message_id, date, message_url, target_url,
                 status="å¾…è½¬å­˜", result="", transfer_time=None):
    conn = sqlite3.connect(DATABASE_FILE)
    try:
        conn.execute("INSERT INTO messages (id, date, message_url, target_url, transfer_status, transfer_time, transfer_result) VALUES (?, ?, ?, ?, ?, ?, ?)",
                     (message_id, date, message_url, target_url,
                      status, transfer_time or datetime.now().isoformat(), result))
        conn.commit()
        logger.info(f"å·²è®°å½•: {message_id} | {target_url} | çŠ¶æ€: {status}")
    except sqlite3.IntegrityError:
        conn.execute("UPDATE messages SET transfer_status=?, transfer_result=?, transfer_time=? WHERE id=?",
                     (status, result, transfer_time or datetime.now().isoformat(), message_id))
        conn.commit()
    finally:
        conn.close()


# è·å–æœ€æ–°æ¶ˆæ¯ï¼ˆä¿æŒä¸å˜ï¼‰
def get_latest_messages():
    try:
        # ä»ç¯å¢ƒå˜é‡è·å–å¤šä¸ªé¢‘é“é“¾æ¥
        channel_urls = os.getenv("ENV_TG_CHANNEL", "").split('|')
        if not channel_urls or channel_urls == ['']:
            logger.warning("æœªè®¾ç½®ENV_TG_CHANNELç¯å¢ƒå˜é‡")
            return []
            
        all_new_messages = []
        
        # å¯¹æ¯ä¸ªé¢‘é“é“¾æ¥æ‰§è¡Œè·å–æ¶ˆæ¯é€»è¾‘
        for channel_url in channel_urls:
            channel_url = channel_url.strip()
            if not channel_url:
                continue
                
            # é¢„å¤„ç†channel_urlï¼Œç¡®ä¿æ ¼å¼æ­£ç¡®
            if channel_url.startswith('https://t.me/') and '/s/' not in channel_url:
                # æå–é¢‘é“åç§°éƒ¨åˆ†
                channel_name = channel_url.split('https://t.me/')[-1]
                # é‡æ„URLï¼Œæ·»åŠ /s/
                channel_url = f'https://t.me/s/{channel_name}'
            logger.info(f"===== å¤„ç†é¢‘é“: {channel_url} =====")
            
            session = requests.Session()
            retry = Retry(total=RETRY_TIMES, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
            session.mount("https://", HTTPAdapter(max_retries=retry))
            headers = {"User-Agent": USER_AGENTS[int(time.time()) % len(USER_AGENTS)]}
            response = session.get(channel_url, headers=headers, timeout=TIMEOUT)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            message_divs = soup.find_all('div', class_='tgme_widget_message')
            total = len(message_divs)
            logger.info(f"å…±è§£æåˆ°{total}æ¡æ¶ˆæ¯ï¼ˆæœ€æ–°çš„åœ¨æœ€åï¼‰")
            new_messages = []
            for i in range(total):
                msg_index = total - 1 - i
                msg = message_divs[msg_index]
                data_post = msg.get('data-post', '')
                message_id = data_post.split('/')[-1] if data_post else f"æœªçŸ¥ID_{msg_index}"
                logger.info(f"æ£€æŸ¥ç¬¬{i + 1}æ–°æ¶ˆæ¯ï¼ˆå€’æ•°ç¬¬{i + 1}æ¡ï¼ŒID: {message_id}ï¼‰")
                time_elem = msg.find('time')
                date_str = time_elem.get('datetime') if time_elem else datetime.now().isoformat()
                link_elem = msg.find('a', class_='tgme_widget_message_date')
                message_url = f"{link_elem.get('href').lstrip('/')}" if link_elem else ''
                text_elem = msg.find('div', class_='tgme_widget_message_text')
                #print(str(text_elem))
                if text_elem:
                    message_text = text_elem.get_text(separator='\\n', strip=True)
                    target_urls = extract_target_url(f"{msg}")
                    if target_urls:
                        for url in target_urls:
                            # æ£€æŸ¥æ˜¯å¦æœ‰æå–ç ä½†URLä¸­æ²¡æœ‰pwdå‚æ•°
                            pwd_match = re.search(r'æå–ç \s*[:ï¼š]\s*(\w+)', str(text_elem), re.IGNORECASE)
                            if pwd_match and 'pwd=' not in url:
                                pwd = pwd_match.group(1)
                                # ç¡®ä¿URLæ ¼å¼æ­£ç¡®ï¼Œæ·»åŠ pwdå‚æ•°
                                if '?' in url:
                                    url = f"{url}&pwd={pwd}"
                                else:
                                    url = f"{url}?pwd={pwd}"
                                logger.info(f"å·²ä¸ºURLæ·»åŠ æå–ç : {url}")
                            if not is_message_processed(message_url):
                                new_messages.append((message_id, date_str, message_url, url, message_text))                               
                            else:
                                logger.info(f"ç¬¬{i + 1}æ–°æ¶ˆæ¯å·²å¤„ç†ï¼Œè·³è¿‡")
                            #print(f"tgæ¶ˆæ¯é“¾æ¥ï¼š{message_url}")
                            #print(f"123é“¾æ¥ï¼š{url}")
                    else:
                        if not is_message_processed(message_url):
                            new_messages.append((message_id, date_str, message_url, "", message_text))
                        else:
                            logger.info(f"ç¬¬{i + 1}æ–°æ¶ˆæ¯å·²å¤„ç†ï¼Œè·³è¿‡")                       
                        #print("æœªå‘ç°ç›®æ ‡123é“¾æ¥")
            new_messages.reverse()
            logger.info(f"å‘ç°{len(new_messages)}æ¡æ–°çš„123åˆ†äº«é“¾æ¥")
            all_new_messages.extend(new_messages)
        
        # æŒ‰æ—¶é—´æ’åºæ‰€æœ‰æ¶ˆæ¯
        all_new_messages.sort(key=lambda x: x[1])
        logger.info(f"===== æ‰€æœ‰é¢‘é“å…±å‘ç°{len(all_new_messages)}æ¡æ–°çš„123åˆ†äº«é“¾æ¥ =====")
        return all_new_messages
    except requests.exceptions.RequestException as e:
        logger.error(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)[:100]}")
        return []


def extract_target_url(text):
    pattern = r'https?:\/\/www\.123(?:\d+|pan)\.\w+\/s\/[\w-]+(?:\?pwd=\w+|(?:\s*æå–ç \s*[:ï¼š]\s*\w+))?'
    matches = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)
    if matches:
        # å»é™¤é‡å¤é“¾æ¥
        unique_matches = list(set([match.strip() for match in matches]))
        return unique_matches
    return []


# è½¬å­˜åˆ†äº«é“¾æ¥ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
from collections import defaultdict, deque
def transfer_shared_link_optimize(client: P123Client, target_url: str, UPLOAD_TARGET_PID: int | str) -> bool:
    parsed_url = urlsplit(target_url)
    if '/s/' in parsed_url.path:
        after_s = parsed_url.path.split('/s/')[-1]
        temp_key = after_s.split('/')[0]
        pwd_sep_index = re.search(r'æå–ç [:ï¼š]', temp_key)
        share_key = temp_key[:pwd_sep_index.start()].strip() if pwd_sep_index else temp_key
    else:
        share_key = None
    if not share_key:
        logger.error(f"æ— æ•ˆçš„åˆ†äº«é“¾æ¥: {target_url}")
        reply_thread_pool.submit(send_message, f"æ— æ•ˆçš„åˆ†äº«é“¾æ¥: {target_url}")
        return False

    # è§£æå¯†ç 
    query_params = parse_qs(parsed_url.query)
    share_pwd = query_params.get('pwd', [None])[0]
    if not share_pwd:
        pwd_match = re.search(r'æå–ç \s*[:ï¼š]\s*(\w+)', parsed_url.path, re.IGNORECASE)
        if not pwd_match:
            pwd_match = re.search(r'æå–ç \s*[:ï¼š]\s*(\w+)', target_url, re.IGNORECASE)
        share_pwd = pwd_match.group(1) if pwd_match else ""

    # å­˜å‚¨æ‰€æœ‰æ–‡ä»¶å’Œç›®å½•ä¿¡æ¯
    all_items = []  # {"file_id": "", "name": "", "etag": "", "parent_dir_id": "", "size": "", "Type": 0}

    def recursive_fetch(parent_file_id: int = 0) -> None:
        """é€’å½’è·å–åˆ†äº«ä¸­çš„æ–‡ä»¶å’Œç›®å½•"""
        try:
            page = 1
            while True:
                resp = client.share_fs_list({
                    "ShareKey": share_key,
                    "SharePwd": share_pwd,
                    "parentFileId": parent_file_id,
                    "limit": 100,
                    "Page": page
                })
                check_response(resp)
                data = resp["data"]
                
                # å¤„ç†å½“å‰é¡µæ•°æ®
                if data and "InfoList" in data:
                    for item in data["InfoList"]:
                        # å°†æ‰€æœ‰é¡¹ç›®ï¼ˆç›®å½•å’Œæ–‡ä»¶ï¼‰éƒ½æ·»åŠ åˆ°all_itemsåˆ—è¡¨
                        all_items.append({
                            "file_id": item["FileId"],
                            "name": item["FileName"],
                            "etag": item.get("Etag", ""),
                            "parent_dir_id": parent_file_id,
                            "size": item.get("Size", 0),
                            "Type": item["Type"]  # ä¿ç•™åŸå§‹ç±»å‹å€¼
                        })
                # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€åä¸€é¡µ
                if not data or len(data.get("InfoList", [])) < 100:
                    break
                page += 1
        except Exception as e:
            logger.error(f"è·å–åˆ—è¡¨å¤±è´¥ï¼ˆçˆ¶ID: {parent_file_id}ï¼‰: {str(e)}")
            raise
    try:
        # é€’å½’è·å–æ–‡ä»¶å’Œç›®å½•åˆ—è¡¨
        recursive_fetch()
        # ç»Ÿè®¡æ–‡ä»¶å’Œç›®å½•æ•°é‡
        file_count = sum(1 for item in all_items if item["Type"] != 1)
        dir_count = sum(1 for item in all_items if item["Type"] == 1)
        logger.info(f"å…±å‘ç°{file_count}ä¸ªæ–‡ä»¶å’Œ{dir_count}ä¸ªç›®å½•ï¼Œå‡†å¤‡è½¬å­˜ï¼ˆé¡¶å±‚ç›®å½•: {UPLOAD_TARGET_PID}ï¼‰")
    except Exception as e:
        logger.error(f"è·å–èµ„æºç»“æ„å¤±è´¥: {str(e)}")
        reply_thread_pool.submit(send_message, f"è·å–èµ„æºç»“æ„å¤±è´¥: {str(e)}")
        return False
    # æ„å»ºfileListæ•°ç»„
    fileList = [
        {
            "fileID": item["file_id"],
            "size": item["size"],
            "etag": item["etag"],
            "type": item["Type"],  # ä½¿ç”¨åŸå§‹ç±»å‹å€¼
            "parentFileID": UPLOAD_TARGET_PID,  # æ‰€æœ‰é¡¹ç›®éƒ½ç›´æ¥ä¿å­˜åˆ°ç›®æ ‡ç›®å½•
            "fileName": item["name"],
            "driveID": 0
        } for item in all_items
    ]
    logger.info(f"å‡†å¤‡è½¬å­˜æ–‡ä»¶åˆ—è¡¨ï¼ˆé¡¶å±‚ç›®å½•: {UPLOAD_TARGET_PID}ï¼‰")
    try:
        # æ„å»ºAPIè¯·æ±‚
        url = "https://www.123pan.com/b/api/restful/goapi/v1/file/copy/save"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {client.token}"
        }
        
        payload = {
            "fileList": fileList,
            "shareKey": share_key,
            "sharePwd": share_pwd,
            "currentLevel": 0
        }
        # å‘é€è¯·æ±‚
        response = requests.post(url, json=payload, headers=headers)
        # å¤„ç†å“åº”
        if response.status_code == 200:
            try:
                response_json = response.json()
                logger.info(response_json)
                if response_json.get("message") == "ok":
                    logger.info(f"{target_url} è½¬å­˜æˆåŠŸ")
                    return True
                else:
                    logger.error(f"{target_url} è½¬å­˜å¤±è´¥: {response_json.get('message')}")
                    reply_thread_pool.submit(send_message, f"{target_url} è½¬å­˜å¤±è´¥: {response_json.get('message')}")
                    return False
            except ValueError:
                logger.error(f"å“åº”ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼: {response.text}")
                reply_thread_pool.submit(send_message, f"å“åº”ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼: {response.text}")
                return False
        else:
            logger.error(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}ï¼Œå“åº”å†…å®¹: {response.text}")
            reply_thread_pool.submit(send_message, f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}ï¼Œå“åº”å†…å®¹: {response.text}")
            return False
    except Exception as e:
        logger.error(f"è½¬å­˜è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        reply_thread_pool.submit(send_message, f"è½¬å­˜è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        return False

def transfer_shared_link(client: P123Client, target_url: str, UPLOAD_TARGET_PID: int | str) -> bool:
    parsed_url = urlsplit(target_url)
    if '/s/' in parsed_url.path:
        after_s = parsed_url.path.split('/s/')[-1]
        temp_key = after_s.split('/')[0]
        pwd_sep_index = re.search(r'æå–ç [:ï¼š]', temp_key)
        share_key = temp_key[:pwd_sep_index.start()].strip() if pwd_sep_index else temp_key
    else:
        share_key = None
    if not share_key:
        logger.error(f"æ— æ•ˆçš„åˆ†äº«é“¾æ¥: {target_url}")
        reply_thread_pool.submit(send_message, f"æ— æ•ˆçš„åˆ†äº«é“¾æ¥: {target_url}")
        return False

    # è§£æå¯†ç 
    query_params = parse_qs(parsed_url.query)
    share_pwd = query_params.get('pwd', [None])[0]
    if not share_pwd:
        pwd_match = re.search(r'æå–ç \s*[:ï¼š]\s*(\w+)', parsed_url.path, re.IGNORECASE)
        if not pwd_match:
            pwd_match = re.search(r'æå–ç \s*[:ï¼š]\s*(\w+)', target_url, re.IGNORECASE)
        share_pwd = pwd_match.group(1) if pwd_match else ""

    # å­˜å‚¨æ‰€æœ‰ç›®å½•å’Œæ–‡ä»¶ä¿¡æ¯
    all_dirs = []  # {"dir_id": "", "name": "", "parent_dir_id": ""}
    all_files = []  # {"file_id": "", "name": "", "etag": "", "parent_dir_id": "", "size": ""}

    def recursive_fetch(parent_file_id: int = 0) -> None:
        """é€’å½’è·å–åˆ†äº«ä¸­çš„ç›®å½•å’Œæ–‡ä»¶"""
        try:
            page = 1
            while True:
                resp = client.share_fs_list({
                    "ShareKey": share_key,
                    "SharePwd": share_pwd,
                    "parentFileId": parent_file_id,
                    "limit": 100,
                    "Page": page
                })
                check_response(resp)
                data = resp["data"]
                
                # å¤„ç†å½“å‰é¡µæ•°æ®
                if data and "InfoList" in data:
                    for item in data["InfoList"]:
                        if item["Type"] == 1:  # ç›®å½•
                            all_dirs.append({
                                "dir_id": item["FileId"],
                                "name": item["FileName"],
                                "parent_dir_id": parent_file_id
                            })
                            # é€’å½’å¤„ç†å­ç›®å½•
                            recursive_fetch(item["FileId"])
                        else:  # æ–‡ä»¶
                            all_files.append({
                                "file_id": item["FileId"],
                                "name": item["FileName"],
                                "etag": item["Etag"],
                                "parent_dir_id": parent_file_id,
                                "size": item["Size"]
                            })
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€åä¸€é¡µ
                if not data or len(data.get("InfoList", [])) < 100:
                    break
                
                page += 1
            # å®‰å…¨å¤„ç†Nextå‚æ•°ï¼Œç¡®ä¿æ˜¯æœ‰æ•ˆçš„æ•´æ•°
            
        except Exception as e:
            logger.error(f"è·å–åˆ—è¡¨å¤±è´¥ï¼ˆçˆ¶ID: {parent_file_id}ï¼‰: {str(e)}")
            raise

    try:
        recursive_fetch()
        logger.info(f"å…±å‘ç°{len(all_dirs)}ä¸ªç›®å½•å’Œ{len(all_files)}ä¸ªæ–‡ä»¶ï¼Œå‡†å¤‡è½¬å­˜ï¼ˆé¡¶å±‚ç›®å½•: {UPLOAD_TARGET_PID}ï¼‰")
    except Exception as e:
        logger.error(f"è·å–èµ„æºç»“æ„å¤±è´¥: {str(e)}")
        reply_thread_pool.submit(send_message, f"è·å–èµ„æºç»“æ„å¤±è´¥: {str(e)}")
        return False

    # ç¬¬ä¸€æ­¥ï¼šæ„å»ºç›®å½•å±‚çº§ï¼ˆæ ¸å¿ƒè°ƒæ•´ï¼‰
    # 1. è¯†åˆ«åˆ†äº«ä¸­çš„æ‰€æœ‰ç›®å½•çš„çˆ¶-å­å…³ç³»ï¼ˆæ— è®ºåŸå§‹parent_dir_idæ˜¯ä»€ä¹ˆï¼‰
    dir_children = defaultdict(list)  # åŸç›®å½•ID â†’ å­ç›®å½•åˆ—è¡¨
    all_dir_ids = {d["dir_id"] for d in all_dirs}  # æ‰€æœ‰ç›®å½•IDé›†åˆ
    share_top_dirs = []  # åˆ†äº«ä¸­çš„é¡¶å±‚ç›®å½•ï¼ˆå³æ²¡æœ‰ä¸Šçº§ç›®å½•åœ¨all_dirsä¸­çš„ç›®å½•ï¼‰

    for dir_info in all_dirs:
        parent_id = dir_info["parent_dir_id"]
        # è‹¥çˆ¶ç›®å½•IDä¸åœ¨åˆ†äº«çš„ç›®å½•åˆ—è¡¨ä¸­ï¼Œè¯´æ˜æ˜¯åˆ†äº«çš„é¡¶å±‚ç›®å½•
        if parent_id not in all_dir_ids:
            share_top_dirs.append(dir_info)
        else:
            dir_children[parent_id].append(dir_info)

    logger.info(f"åˆ†äº«ä¸­çš„é¡¶å±‚ç›®å½•ï¼ˆç›´æ¥åˆ›å»ºåœ¨ç›®æ ‡ç›®å½•ä¸‹ï¼‰: {[d['name'] for d in share_top_dirs]}")

    # 2. æŒ‰å±‚çº§åˆ›å»ºç›®å½•ï¼ˆä»¥UPLOAD_TARGET_PIDä¸ºæ ¹ï¼‰
    dir_queue = deque(share_top_dirs)  # é˜Ÿåˆ—å­˜å‚¨å¾…åˆ›å»ºç›®å½•
    dir_id_mapping = {}  # åŸç›®å½•ID â†’ æ–°ç›®å½•IDï¼ˆæ–°ç›®å½•çš„çˆ¶ç›®å½•ä¸ºUPLOAD_TARGET_PIDæˆ–å¯¹åº”å­ç›®å½•ï¼‰

    # å¼ºåˆ¶å°†åˆ†äº«çš„é¡¶å±‚ç›®å½•çš„çˆ¶ç›®å½•è®¾ä¸ºUPLOAD_TARGET_PID
    for dir_info in share_top_dirs:
        dir_id_mapping[dir_info["dir_id"]] = None  # æ ‡è®°å¾…åˆ›å»º

    all_success = True

    while dir_queue:
        dir_info = dir_queue.popleft()
        original_dir_id = dir_info["dir_id"]
        dir_name = dir_info["name"]
        original_parent_id = dir_info["parent_dir_id"]

        # ç¡®å®šæ–°çˆ¶ç›®å½•IDï¼š
        # - è‹¥ä¸ºåˆ†äº«çš„é¡¶å±‚ç›®å½• â†’ æ–°çˆ¶ç›®å½•æ˜¯UPLOAD_TARGET_PID
        # - å¦åˆ™ â†’ æ–°çˆ¶ç›®å½•æ˜¯åŸçˆ¶ç›®å½•å¯¹åº”çš„æ–°ç›®å½•ID
        if original_dir_id in [d["dir_id"] for d in share_top_dirs]:
            new_parent_id = UPLOAD_TARGET_PID
        else:
            new_parent_id = dir_id_mapping.get(original_parent_id)

        if not new_parent_id:
            logger.warning(f"ç›®å½• {dir_name}ï¼ˆåŸID: {original_dir_id}ï¼‰çš„æ–°çˆ¶ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡")
            reply_thread_pool.submit(send_message, f"è­¦å‘Šï¼šç›®å½• {dir_name}ï¼ˆåŸID: {original_dir_id}ï¼‰çš„æ–°çˆ¶ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡")
            return False

        # åˆ›å»ºç›®å½•
        try:
            # æ­£ç¡®ä¼ é€’å‚æ•°ï¼ˆnameä½œä¸ºä½ç½®å‚æ•°ï¼‰
            create_resp = client.fs_mkdir(
                dir_name,  # ç›®å½•åï¼ˆä½ç½®å‚æ•°ï¼‰
                parent_id=new_parent_id,
                duplicate=1
            )
            check_response(create_resp)
            # print(f"fs_mkdirå®Œæ•´å“åº”: {create_resp}")  # è°ƒè¯•ç”¨

            # å…³é”®ä¿®å¤ï¼šä»data.Info.FileIdæå–æ–°ç›®å½•ID
            new_dir_id = create_resp["data"]["Info"]["FileId"]
            if not new_dir_id:
                raise ValueError(f"æ–°ç›®å½•IDä¸ºç©ºï¼Œå“åº”: {create_resp}")

            dir_id_mapping[original_dir_id] = new_dir_id
            logger.info(f"åˆ›å»ºç›®å½•æˆåŠŸï¼š{dir_name} â†’ æ–°ID: {new_dir_id}ï¼Œçˆ¶ç›®å½•: {new_parent_id}")

            # æ·»åŠ å­ç›®å½•åˆ°é˜Ÿåˆ—
            child_dirs = dir_children.get(original_dir_id, [])
            dir_queue.extend(child_dirs)
            logger.info(f"å¾…åˆ›å»ºå­ç›®å½•: {[d['name'] for d in child_dirs]}")
        except Exception as e:
            logger.error(f"åˆ›å»ºç›®å½• {dir_name} å¤±è´¥: {str(e)}ï¼Œè·³è¿‡è¯¥ç›®å½•åŠå­ç›®å½•")
            reply_thread_pool.submit(send_message, f"åˆ›å»ºç›®å½• {dir_name} å¤±è´¥: {str(e)}ï¼Œè·³è¿‡è¯¥ç›®å½•åŠå­ç›®å½•")
            return False

    logger.info(f"ç›®å½•æ˜ å°„å…³ç³»: {dir_id_mapping}")

    # ç¬¬äºŒæ­¥ï¼šæŒ‰æ–‡ä»¶æ•°é‡åˆ†ç»„ï¼Œæ¯ç»„æœ€å¤š100ä¸ªæ–‡ä»¶è¿›è¡Œè½¬å­˜
    # 1. æŒ‰ç›®æ ‡ç›®å½•IDå’Œæ–‡ä»¶æ•°é‡åˆ†ç»„
    MAX_BATCH_SIZE = 100
    file_batches = defaultdict(list)  # (target_parent_id) â†’ æ‰¹æ¬¡åˆ—è¡¨
    batch_by_dir = defaultdict(list)  # (target_parent_id, batch_index) â†’ æ–‡ä»¶åˆ—è¡¨
    
    # å…ˆæŒ‰ç›®æ ‡ç›®å½•IDåˆ†ç»„
    for file_info in all_files:
        file_id = file_info["file_id"]
        file_name = file_info["name"]
        original_parent_id = file_info["parent_dir_id"]
        
        # ç¡®å®šæ–‡ä»¶çš„ç›®æ ‡ç›®å½•ID
        target_parent_id = dir_id_mapping.get(original_parent_id, UPLOAD_TARGET_PID)
        logger.info(f"æ–‡ä»¶ {file_name} çš„åŸçˆ¶ç›®å½•ID: {original_parent_id} â†’ ç›®æ ‡ç›®å½•ID: {target_parent_id}")
        
        # æ„å»ºæ–‡ä»¶ä¿¡æ¯
        file_data = {
            "file_id": file_id,
            "file_name": file_name,
            "etag": file_info["etag"],
            "parent_file_id": original_parent_id,
            "size": file_info["size"]
        }
        
        # æ·»åŠ åˆ°å¯¹åº”ç›®å½•çš„æ‰¹æ¬¡åˆ—è¡¨
        file_batches[target_parent_id].append(file_data)
    
    # 2. å¯¹æ¯ä¸ªç›®å½•çš„æ–‡ä»¶åˆ—è¡¨æŒ‰æœ€å¤§æ‰¹æ¬¡å¤§å°åˆ†å‰²
    all_batches = []
    for target_parent_id, files_in_dir in file_batches.items():
        # å°†ç›®å½•ä¸­çš„æ–‡ä»¶åˆ†æˆå¤šä¸ªæ‰¹æ¬¡ï¼Œæ¯æ‰¹æœ€å¤šMAX_BATCH_SIZEä¸ªæ–‡ä»¶
        for i in range(0, len(files_in_dir), MAX_BATCH_SIZE):
            batch_files = files_in_dir[i:i + MAX_BATCH_SIZE]
            all_batches.append((target_parent_id, batch_files))
    
    # 3. é€ä¸ªæ‰¹æ¬¡è½¬å­˜æ–‡ä»¶
    total_batches = len(all_batches)
    logger.info(f"å…±åˆ†ä¸º {total_batches} ä¸ªæ‰¹æ¬¡è½¬å­˜æ–‡ä»¶ï¼Œæ¯æ‰¹æœ€å¤š {MAX_BATCH_SIZE} ä¸ªæ–‡ä»¶")
    
    for batch_index, (target_parent_id, batch_files) in enumerate(all_batches, 1):
        try:
            copy_resp = client.share_fs_copy({
                "share_key": share_key,
                "share_pwd": share_pwd,
                "file_list": batch_files,
                "current_level": 1,
                "event": "transfer"
            }, parent_id=target_parent_id)
            
            check_response(copy_resp)
            if copy_resp.get("code") in (0, 200):
                file_names = [f["file_name"] for f in batch_files]
                logger.info(f"æ‰¹æ¬¡ {batch_index}/{total_batches} æˆåŠŸè½¬å­˜ {len(batch_files)} ä¸ªæ–‡ä»¶åˆ°ç›®å½•ID: {target_parent_id} â†’ æ–‡ä»¶å: {', '.join(file_names[:3])}{'...' if len(file_names) > 3 else ''}")
            else:
                file_names = [f["file_name"] for f in batch_files]
                logger.error(f"æ‰¹æ¬¡ {batch_index}/{total_batches} è½¬å­˜ {len(batch_files)} ä¸ªæ–‡ä»¶åˆ°ç›®å½•ID: {target_parent_id} å¤±è´¥ï¼ˆå“åº”: {copy_resp}ï¼‰")
                reply_thread_pool.submit(send_message, f"æ‰¹æ¬¡ {batch_index}/{total_batches} è½¬å­˜ {len(batch_files)} ä¸ªæ–‡ä»¶åˆ°ç›®å½•ID: {target_parent_id} å¤±è´¥ï¼ˆå“åº”: {copy_resp}ï¼‰")
                return False
        except Exception as e:
            file_names = [f["file_name"] for f in batch_files]
            logger.error(f"æ‰¹æ¬¡ {batch_index}/{total_batches} è½¬å­˜ {len(batch_files)} ä¸ªæ–‡ä»¶åˆ°ç›®å½•ID: {target_parent_id} å¼‚å¸¸: {str(e)}")
            reply_thread_pool.submit(send_message, f"æ‰¹æ¬¡ {batch_index}/{total_batches} è½¬å­˜ {len(batch_files)} ä¸ªæ–‡ä»¶åˆ°ç›®å½•ID: {target_parent_id} å¼‚å¸¸: {str(e)}")
            return False
    
    logger.info(f"æ‰€æœ‰ {len(all_files)} ä¸ªæ–‡ä»¶å·²æˆåŠŸè½¬å­˜ï¼")
    return True
class UserStateManager:
    def __init__(self, db_file):
        self.db_file = db_file
        self.init_db()

    def init_db(self):
        conn = sqlite3.connect(self.db_file)
        conn.execute('''CREATE TABLE IF NOT EXISTS user_states
                     (user_id INTEGER PRIMARY KEY, state TEXT, data TEXT)''')
        conn.commit()
        conn.close()

    def set_state(self, user_id, state, data=None):
        conn = sqlite3.connect(self.db_file)
        conn.execute("INSERT OR REPLACE INTO user_states VALUES (?, ?, ?)",
                     (user_id, state, data))
        conn.commit()
        conn.close()

    def get_state(self, user_id):
        conn = sqlite3.connect(self.db_file)
        result = conn.execute("SELECT state, data FROM user_states WHERE user_id = ?",
                              (user_id,)).fetchone()
        conn.close()
        return result if result else (None, None)

    def clear_state(self, user_id):
        conn = sqlite3.connect(self.db_file)
        conn.execute("DELETE FROM user_states WHERE user_id = ?", (user_id,))
        conn.commit()
        conn.close()


# åˆå§‹åŒ–ç”¨æˆ·çŠ¶æ€ç®¡ç†å™¨
user_state_manager = UserStateManager(USER_STATE_DB)


# æœç´¢123ç½‘ç›˜æ–‡ä»¶å¤¹ï¼ˆä¿®æ”¹ç»“æœæ•°é‡ä¸º15ï¼‰
async def search_123_files(client: P123Client, keyword: str) -> list:
    """æœç´¢123ç½‘ç›˜ä¸­çš„æ–‡ä»¶å¤¹ï¼ˆè¿”å›æœ€å¤š15ä¸ªç»“æœï¼‰"""
    all_items = []
    last_file_id = 0
    try:
        for i in range(5):  # æœ€å¤š3é¡µ
            response = requests.get(
                f"https://open-api.123pan.com/api/v2/file/list?parentFileId=0&searchData={encodeURIComponent(keyword)}&searchMode=1&limit=100&lastFileId={last_file_id}",
                headers={
                    'Authorization': f'Bearer {client.token}',
                    'Platform': 'open_platform'
                },
                timeout=TIMEOUT
            )
            data = response.json()
            if data.get('code') == 401 or 'expired' in str(data.get('message', '')).lower():
                raise Exception("token expired")
            if data.get('code') != 0:
                raise Exception(f"æœç´¢å¤±è´¥: {data.get('message', 'æœªçŸ¥é”™è¯¯')}")
            items = data.get('data', {}).get('fileList', [])
            # ä»…ç­›é€‰æ–‡ä»¶å¤¹ï¼ˆtype=1ï¼‰
            folder_items = [item for item in items if item.get('type') == 1]
            all_items.extend(folder_items)
            last_file_id = data.get('data', {}).get('lastFileId', -1)
            if last_file_id == -1:
                break

        # é™åˆ¶æœ€å¤šè¿”å›15ä¸ªç»“æœ
        results = []
        # æ‰¹é‡å¤„ç†15ä¸ªç»“æœï¼Œè·å–å®Œæ•´è·¯å¾„
        items_to_process = all_items[:20]  # é™åˆ¶ä¸º15ä¸ªç»“æœ
        logger.info(f"å‡†å¤‡æ‰¹é‡å¤„ç†{len(items_to_process)}ä¸ªæ–‡ä»¶å¤¹ç»“æœ")
        
        # ä½¿ç”¨æ‰¹é‡æ„å»ºè·¯å¾„å‡½æ•°
        # æ³¨æ„ï¼šå³ä½¿åªæœ‰15ä¸ªæ–‡ä»¶å¤¹é¡¹ç›®ï¼Œç”±äºéœ€è¦è·å–å„çº§çˆ¶ç›®å½•ä¿¡æ¯ï¼Œæ‰€ä»¥å®é™…æŸ¥è¯¢çš„IDæ•°é‡ä¼šå¤šäº15ä¸ª
        # è¿™ç§è®¾è®¡å¯ä»¥æ˜¾è‘—å‡å°‘APIè°ƒç”¨æ¬¡æ•°ï¼Œæé«˜è·¯å¾„æ„å»ºæ•ˆç‡

        paths_map = await batch_build_full_paths(client, items_to_process)
        
        # åˆ›å»ºæ˜ å°„ï¼Œä»¥ä¾¿å¿«é€ŸæŸ¥æ‰¾itemä¿¡æ¯
        item_map = {str(item.get('fileId', '')): item for item in items_to_process if str(item.get('fileId', ''))}
        
        # éå†paths_mapçš„é”®å€¼å¯¹ï¼Œä½¿resultsçš„é¡ºåºä¸paths_mapçš„é¡ºåºä¿æŒä¸€è‡´
        for file_id, full_path in paths_map.items():
            item = item_map.get(file_id)
            if not item:
                continue
            
            results.append({
                "id": file_id,
                "name": item.get('filename'),
                "type": "æ–‡ä»¶å¤¹",
                "path": full_path,  # å®Œæ•´è·¯å¾„
                "create_time": item.get('createTime')
            })
        
        # å¦‚æœè¿˜æœ‰æœªåœ¨paths_mapä¸­çš„é¡¹ç›®ï¼Œä¹Ÿæ·»åŠ åˆ°resultsä¸­
        for item in items_to_process:
            file_id = str(item.get('fileId', ''))
            if not file_id or file_id in paths_map:
                continue
            
            full_path = item.get('filename', '')
            results.append({
                "id": file_id,
                "name": item.get('filename'),
                "type": "æ–‡ä»¶å¤¹",
                "path": full_path,  # å®Œæ•´è·¯å¾„
                "create_time": item.get('createTime')
            })
        return results
    except Exception as e:
        logger.error(f"æœç´¢æ–‡ä»¶å¤¹å¤±è´¥: {str(e)}")
        raise


def get_folder_detail(client: P123Client, file_id: str) -> dict:
    """è·å–æ–‡ä»¶å¤¹è¯¦æƒ…"""
    if not file_id:
        logger.error("æ–‡ä»¶å¤¹IDä¸ºç©º")
        return {"filename": ""}
    try:
        response = requests.get(
            f"https://open-api.123pan.com/api/v1/file/detail?fileID={file_id}",
            headers={
                'Authorization': f'Bearer {client.token}',
                'Platform': 'open_platform'
            },
            timeout=TIMEOUT
        )
        data = response.json()
        if data.get('code') != 0:
            logger.error(f"è·å–æ–‡ä»¶å¤¹{file_id}è¯¦æƒ…å¤±è´¥: {data.get('message')}")
            return {"filename": ""}
        return data.get('data', {})
    except Exception as e:
        logger.error(f"è·å–æ–‡ä»¶å¤¹{file_id}è¯¦æƒ…å¼‚å¸¸: {str(e)}")
        return {"filename": ""}


def get_files_details(client: P123Client, file_ids: list) -> dict:
    """æ‰¹é‡è·å–æ–‡ä»¶/æ–‡ä»¶å¤¹è¯¦æƒ…"""
    if not file_ids:
        logger.error("æ–‡ä»¶IDåˆ—è¡¨ä¸ºç©º")
        return {}
    try:
        logger.info(f"è¯·æ±‚ä»¥ä¸‹çˆ¶ç›®å½•IDè¯¦æƒ…ï¼š{file_ids}")
        response = requests.post(
            "https://open-api.123pan.com/api/v1/file/infos",
            headers={
                'Authorization': f'Bearer {client.token}',
                'Platform': 'open_platform',
                'Content-Type': 'application/json'
            },
            json={"fileIds": file_ids},
            timeout=TIMEOUT
        )
        data = response.json()
        #logger.info(f"ä»¥ä¸‹çˆ¶ç›®å½•è¯¦æƒ…ï¼š{data}")
        if data.get('code') != 0:
            logger.error(f"æ‰¹é‡è·å–æ–‡ä»¶è¯¦æƒ…å¤±è´¥: {data.get('message', 'æœªçŸ¥é”™è¯¯')}")
            return {}
        details_map = {}
        # æ³¨æ„ï¼šAPIè¿”å›çš„å­—æ®µåæ˜¯fileListï¼Œä¸æ˜¯list
        for item in data.get('data', {}).get('fileList', []):
            file_id = str(item.get('fileId'))
            details_map[file_id] = item
        return details_map
    except Exception as e:
        logger.error(f"æ‰¹é‡è·å–æ–‡ä»¶è¯¦æƒ…å¼‚å¸¸: {str(e)}")
        return {}


async def build_full_path(client: P123Client, item: dict) -> str:
    """æ„å»ºæ–‡ä»¶å¤¹å®Œæ•´è·¯å¾„ï¼ˆç”¨äºæ˜¾ç¤ºï¼‰ - å•ä¸ªå¤„ç†ç‰ˆæœ¬ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
    # ç”±äºå·²ç»å®ç°äº†æ‰¹é‡æ„å»ºè·¯å¾„çš„åŠŸèƒ½ï¼Œè¿™é‡Œå¯ä»¥ä¿ç•™ä¸ºå‘åå…¼å®¹æˆ–ç®€å•è°ƒç”¨
    paths_map = await batch_build_full_paths(client, [item])
    file_id = str(item.get('fileId', ''))
    return paths_map.get(file_id, item.get('filename', ''))


async def batch_build_full_paths(client: P123Client, items: list) -> dict:
    """æ‰¹é‡æ„å»ºå¤šä¸ªæ–‡ä»¶å¤¹çš„å®Œæ•´è·¯å¾„ï¼ˆä¿®å¤å…¨å±€ç¼“å­˜é—®é¢˜ï¼Œç¡®ä¿çˆ¶IDè¯¦æƒ…ä¸ä¸¢å¤±ï¼‰"""
    path_map = {}
    if not items:
        return path_map
    
    query_level = 4  # ä¿æŒå›ºå®š4å±‚
    temp_path_map = {}
    queried_ids = set()  # å·²æŸ¥è¯¢è¿‡çš„IDï¼ˆé¿å…é‡å¤è¯·æ±‚ï¼‰
    current_query_ids = set()  # å½“å‰è½®éœ€æŸ¥è¯¢çš„ID
    global_details_cache = {}  # æ–°å¢ï¼šå…¨å±€ç¼“å­˜ï¼Œä¿å­˜æ‰€æœ‰å·²æŸ¥è¯¢çš„çˆ¶ç›®å½•è¯¦æƒ…ï¼ˆè·¨è½®å¤ç”¨ï¼‰
    
    # åˆå§‹åŒ–ï¼šæ”¶é›†æ¯ä¸ªæ–‡ä»¶çš„åˆå§‹ä¿¡æ¯
    logger.info(f"å¼€å§‹å¤„ç†{len(items)}ä¸ªæ–‡ä»¶å¤¹é¡¹ç›®ï¼Œquery_level={query_level}")
    for item in items:
        file_id = str(item.get('fileId', ''))
        if not file_id:
            continue
        
        temp_path_map[file_id] = {
            'path_parts': [item.get('filename', '')],
            'current_parent_id': item.get('parentFileId'),
            'remaining_levels': query_level
        }
        
        parent_id = item.get('parentFileId')
        if parent_id and parent_id != 0:
            current_query_ids.add(str(parent_id))
    
    logger.info(f"ç¬¬ä¸€è½®æŸ¥è¯¢ï¼ˆç¬¬1å±‚çˆ¶ç›®å½•ï¼‰ï¼š{len(current_query_ids)}ä¸ªIDï¼Œå¤„ç†{len(temp_path_map)}ä¸ªæ–‡ä»¶")
    
    # è¿­ä»£æŸ¥è¯¢çˆ¶ç›®å½•ï¼ˆ4è½®ï¼‰
    for level in range(query_level):
        if not current_query_ids:
            logger.info(f"ç¬¬{level+1}è½®æ— çˆ¶IDå¯æŸ¥ï¼Œæå‰ç»“æŸ")
            break
        
        logger.info(f"ç¬¬{level+1}è½®æŸ¥è¯¢ï¼ˆå‰©ä½™å±‚çº§ï¼š{query_level - level}ï¼‰ï¼š{len(current_query_ids)}ä¸ªID")
        
        # 1. æ–°å¢ï¼šæŸ¥è¯¢å½“å‰è½®IDï¼Œåˆå¹¶åˆ°å…¨å±€ç¼“å­˜
        current_details = get_files_details(client, list(current_query_ids))
        global_details_cache.update(current_details)  # å…³é”®ï¼šå°†å½“å‰è½®è¯¦æƒ…å­˜å…¥å…¨å±€ç¼“å­˜
        
        next_query_ids = set()
        
        # 2. å¤„ç†æ¯ä¸ªæ–‡ä»¶çš„çˆ¶ç›®å½•é“¾ï¼šä»å…¨å±€ç¼“å­˜è·å–è¯¦æƒ…ï¼Œè€Œéå½“å‰è½®ç¼“å­˜
        for file_id, info in temp_path_map.items():
            if info['remaining_levels'] <= 0:
                continue
            
            current_parent_id = info['current_parent_id']
            if not current_parent_id or current_parent_id == 0:
                continue
            
            current_parent_id_str = str(current_parent_id)
            # å…³é”®ï¼šä»å…¨å±€ç¼“å­˜è·å–è¯¦æƒ…ï¼Œè€Œéå½“å‰è½®ç¼“å­˜
            parent_detail = global_details_cache.get(current_parent_id_str)
            
            if not parent_detail:
                logger.warning(f"ç¬¬{level+1}è½®ï¼šå…¨å±€ç¼“å­˜ä¸­æœªæ‰¾åˆ°ID[{current_parent_id_str}]çš„è¯¦æƒ…ï¼Œåœæ­¢è¯¥æ–‡ä»¶çš„ä¸Šå±‚æŸ¥è¯¢")
                info['remaining_levels'] = 0
                continue
            
            # æå–çˆ¶ç›®å½•åç§°ï¼Œæ›´æ–°è·¯å¾„
            parent_name = parent_detail.get('filename', '')
            if parent_name:
                # æ–°å¢ï¼šé¿å…é‡å¤æ·»åŠ åŒä¸€ç›®å½•ï¼ˆé˜²æ­¢å¼‚å¸¸æƒ…å†µä¸‹çš„é‡å¤ï¼‰
                if not info['path_parts'] or info['path_parts'][0] != parent_name:
                    info['path_parts'].insert(0, parent_name)
                logger.debug(f"æ–‡ä»¶[{file_id}]ç¬¬{level+1}å±‚çˆ¶ç›®å½•ï¼š{parent_name}ï¼Œå½“å‰è·¯å¾„ï¼š{'/'.join(info['path_parts'])}")
            
            # è·å–ä¸‹ä¸€å±‚çˆ¶IDï¼ŒåŠ å…¥ä¸‹è½®æŸ¥è¯¢ï¼ˆéœ€æœªæŸ¥è¯¢è¿‡ï¼‰
            next_parent_id = parent_detail.get('parentFileId')
            if next_parent_id and next_parent_id != 0:
                next_parent_id_str = str(next_parent_id)
                if (next_parent_id_str not in queried_ids and 
                    next_parent_id_str not in current_query_ids and 
                    next_parent_id_str not in next_query_ids):
                    next_query_ids.add(next_parent_id_str)
                info['current_parent_id'] = next_parent_id
            else:
                info['remaining_levels'] = 0
            
            # å‰©ä½™å±‚çº§-1
            info['remaining_levels'] -= 1
        
        # æ›´æ–°å·²æŸ¥è¯¢IDå’Œä¸‹è½®æŸ¥è¯¢ID
        queried_ids.update(current_query_ids)
        current_query_ids = next_query_ids
    
    # 4è½®æŸ¥è¯¢å®Œæˆåï¼Œä»å…¨å±€ç¼“å­˜ä¸­ç»§ç»­æ„å»ºè·¯å¾„ï¼ˆä¸å‘èµ·æ–°è¯·æ±‚ï¼‰
    logger.info("4è½®æŸ¥è¯¢å·²å®Œæˆï¼Œå¼€å§‹ä»å…¨å±€ç¼“å­˜ä¸­ç»§ç»­æ„å»ºè·¯å¾„ï¼ˆä¸å‘èµ·æ–°è¯·æ±‚ï¼‰")
    has_more_to_process = True
    while has_more_to_process:
        has_more_to_process = False
        for file_id, info in temp_path_map.items():
            current_parent_id = info['current_parent_id']
            if not current_parent_id or current_parent_id == 0:
                continue
            
            current_parent_id_str = str(current_parent_id)
            # åªä»å…¨å±€ç¼“å­˜ä¸­è·å–è¯¦æƒ…ï¼Œä¸å‘èµ·æ–°è¯·æ±‚
            parent_detail = global_details_cache.get(current_parent_id_str)
            
            if parent_detail:
                # æå–çˆ¶ç›®å½•åç§°ï¼Œæ›´æ–°è·¯å¾„
                parent_name = parent_detail.get('filename', '')
                if parent_name:
                    if not info['path_parts'] or info['path_parts'][0] != parent_name:
                        info['path_parts'].insert(0, parent_name)
                    logger.debug(f"ä»ç¼“å­˜ä¸­è¡¥å……è·¯å¾„ï¼šæ–‡ä»¶[{file_id}]æ–°å¢çˆ¶ç›®å½•ï¼š{parent_name}ï¼Œå½“å‰è·¯å¾„ï¼š{'/'.join(info['path_parts'])}")
                
                # æ›´æ–°ä¸‹ä¸€å±‚çˆ¶ID
                next_parent_id = parent_detail.get('parentFileId')
                if next_parent_id and next_parent_id != 0:
                    info['current_parent_id'] = next_parent_id
                    has_more_to_process = True  # è¿˜æœ‰æ›´å¤šçˆ¶IDå¯ä»¥ä»ç¼“å­˜ä¸­æŸ¥æ‰¾
                else:
                    info['current_parent_id'] = 0
            else:
                info['current_parent_id'] = 0  # ç¼“å­˜ä¸­æ²¡æœ‰ï¼Œåœæ­¢æŸ¥æ‰¾
    
    # æ„å»ºæœ€ç»ˆè·¯å¾„ - æŒ‰è·¯å¾„å­—ç¬¦ä¸²æ’åºï¼Œä½¿ç›¸åŒå…¬å…±å‰ç¼€çš„æ–‡ä»¶å¤¹ä¼˜å…ˆæ”¾åœ¨ä¸€èµ·
    # é¦–å…ˆè·å–æ‰€æœ‰é¡¹ï¼Œç„¶åæŒ‰è·¯å¾„å­—ç¬¦ä¸²æ’åº
    sorted_items = sorted(temp_path_map.items(), key=lambda x: '/'.join(x[1]['path_parts']))

    for file_id, info in sorted_items:
        full_path = '/'.join(info['path_parts'])
        path_map[file_id] = full_path
        logger.debug(f"æ–‡ä»¶[{file_id}]æœ€ç»ˆè·¯å¾„ï¼š{full_path}")
    logger.info(f"æ‰¹é‡è·¯å¾„æ„å»ºå®Œæˆï¼Œç”Ÿæˆ{len(path_map)}ä¸ªæ–‡ä»¶è·¯å¾„ï¼ˆquery_level=4ï¼Œç¼“å­˜è¡¥å……å®Œæˆï¼‰")
    return path_map


def encodeURIComponent(s: str) -> str:
    import urllib.parse
    return urllib.parse.quote(s, safe='~()*!.\'')


def create_share_link(client: P123Client, file_id: str, expiry_days: int = 0, password: str = None) -> dict:
    """åˆ›å»ºåˆ†äº«é“¾æ¥"""
    if not file_id or not str(file_id).strip():
        raise ValueError("æ–‡ä»¶å¤¹IDä¸ºç©ºæˆ–æ— æ•ˆ")

    valid_expire_days = {0, 1, 7, 30}
    if expiry_days not in valid_expire_days:
        logger.warning(f"è¿‡æœŸå¤©æ•°{expiry_days}æ— æ•ˆï¼Œè‡ªåŠ¨ä½¿ç”¨7å¤©")
        expiry_days = 7

    try:
        folder_detail = get_folder_detail(client, file_id)
        folder_name = folder_detail.get('filename', f"åˆ†äº«æ–‡ä»¶å¤¹_{file_id}")
        if not folder_name:
            logger.warning(f"æ–‡ä»¶å¤¹ID{file_id}ä¸å­˜åœ¨ï¼Œå¯èƒ½å·²è¢«åˆ é™¤")

        response = requests.post(
            "https://open-api.123pan.com/api/v1/share/create",
            headers={
                'Authorization': f'Bearer {client.token}',
                'Platform': 'open_platform',
                'Content-Type': 'application/json'
            },
            json={
                "shareName": folder_name,
                "shareExpire": expiry_days,
                "fileIDList": file_id,
                "sharePwd": DIY_LINK_PWD
            },
            timeout=TIMEOUT
        )
        data = response.json()
        if data.get('code') != 0:
            raise Exception(f"åˆ›å»ºåˆ†äº«å¤±è´¥: {data.get('message', 'æœªçŸ¥é”™è¯¯')}ï¼ˆID: {file_id}ï¼‰")
        share_info = data.get('data', {})
        if expiry_days == 0:
            expiry_str = "æ°¸ä¹…æœ‰æ•ˆ"
        else:
            expiry_time = int(time.time()) + expiry_days * 86400
            expiry_str = datetime.fromtimestamp(expiry_time).strftime('%Y-%m-%d %H:%M:%S')
        return {
            "url": f"https://www.123pan.com/s/{share_info.get('shareKey')}{'?pwd=' + DIY_LINK_PWD if DIY_LINK_PWD else ''}",
            "password": share_info.get('sharePwd'),
            "expiry": expiry_str
        }
    except Exception as e:
        logger.error(f"åˆ›å»ºåˆ†äº«é“¾æ¥å¤±è´¥: {str(e)}")
        raise


def get_first_video_file(client: P123Client, file_id: str) -> str:
    """è·å–æ–‡ä»¶å¤¹æˆ–å­æ–‡ä»¶å¤¹ä¸­ç¬¬ä¸€ä¸ªè§†é¢‘æ–‡ä»¶çš„åç§°"""
    video_extensions = {'.mkv', '.ts', '.mp4', '.avi', '.rmvb', '.wmv', '.m2ts', '.mpg', '.flv', '.rm', '.mov', '.iso'}

    def recursive_search(folder_id: str) -> str:
        try:
            # è°ƒç”¨123ç½‘ç›˜APIåˆ—å‡ºæ–‡ä»¶å¤¹å†…å®¹
            resp = client.fs_list(folder_id)
            check_response(resp)
            items = resp["data"]["InfoList"]

            # ä¼˜å…ˆæ£€æŸ¥å½“å‰æ–‡ä»¶å¤¹çš„æ–‡ä»¶
            for item in items:
                if item["Type"] == 0:  # ç±»å‹ä¸ºæ–‡ä»¶
                    filename = item["FileName"]
                    ext = os.path.splitext(filename)[1].lower()
                    if ext in video_extensions:
                        return filename

            # é€’å½’æ£€æŸ¥å­æ–‡ä»¶å¤¹
            for item in items:
                if item["Type"] == 1:  # ç±»å‹ä¸ºæ–‡ä»¶å¤¹
                    sub_result = recursive_search(item["FileId"])
                    if sub_result:
                        return sub_result
            return None
        except Exception as e:
            logger.error(f"æœç´¢è§†é¢‘æ–‡ä»¶å¤±è´¥: {str(e)}")
            return None

    return recursive_search(file_id)
@bot.message_handler(commands=['info'])
def handle_info(message):
    user_id = message.from_user.id
    if user_id != TG_ADMIN_USER_ID:
        reply_thread_pool.submit(send_reply, message, "æ‚¨æ²¡æœ‰æƒé™ä½¿ç”¨æ­¤æœºå™¨äººã€‚")
        return
    client = init_123_client()
    response = client.user_info()  # éªŒè¯tokenæœ‰æ•ˆæ€§
    def mask_uid(uid):
        """è´¦æˆ·IDè„±æ•ï¼š1846764956 â†’ 184****956"""
        uid_str = str(uid)
        return f"{uid_str[:3]}****{uid_str[-3:]}" if len(uid_str)>=6 else uid_str

    def mask_mobile(mobile):
        """æ‰‹æœºå·è„±æ•ï¼š18221643386 â†’ 182****3386"""
        mobile_str = str(mobile)
        return f"{mobile_str[:3]}****{mobile_str[-4:]}" if len(mobile_str)==11 else mobile_str

    def format_size(size):
        """å­—èŠ‚è½¬TB/GBï¼ˆè‡ªåŠ¨é€‚é…å•ä½ï¼‰"""
        if size <= 0:
            return "0.00 GB"
        tb = size / (1024 **4)
        return f"{tb:.2f} TB" if tb >= 1 else f"{size / (1024** 3):.2f} GB"

    def space_progress(used, total, bar_len=10):
        """ç”Ÿæˆè¿›åº¦æ¡ï¼šâ–“=å·²ç”¨ï¼Œâ–‘=å‰©ä½™"""
        if total == 0:
            return "â–¡â–¡â–¡â–¡â–¡â–¡â–¡â–¡â–¡â–¡ (0%)"
        ratio = used / total
        filled = int(ratio * bar_len)
        bar = "â–“" * filled + "â–‘" * (bar_len - filled)
        percent = f"{ratio*100:.1f}%"
        return f"{bar} ({percent})"

    # å‡è®¾å“åº”æ•°æ®ä¸º `response`
    data = response["data"]

    # 1. æ ‡é¢˜ä¸è´¦æˆ·ä¿¡æ¯
    base_title = "ğŸš€ 123äº‘ç›˜ä¿¡æ¯"

    account_info = f"""ğŸ‘¤ è´¦æˆ·ä¿¡æ¯
    â”œâ”€ æ˜µç§°ï¼š{data['Nickname']} {'ğŸ–ï¸VIP' if data['Vip'] else ''}
    â”œâ”€ è´¦æˆ·IDï¼š{mask_uid(data['UID'])}
    â”œâ”€ æ‰‹æœºå·ï¼š{mask_mobile(data['Passport'])}
    â””â”€ å¾®ä¿¡ç»‘å®šï¼š{"âœ…å·²ç»‘" if data['BindWechat'] else "âŒæœªç»‘"}"""

    # 2. å­˜å‚¨ç©ºé—´ï¼ˆå¸¦è¿›åº¦æ¡ï¼‰
    used = data['SpaceUsed']
    total = data['SpacePermanent']
    storage_progress = space_progress(used, total)

    storage_info = f"""ğŸ’¾ å­˜å‚¨ç©ºé—´ {storage_progress}
    â”œâ”€ å·²ç”¨ï¼š{format_size(used)}
    â”œâ”€ æ°¸ä¹…ï¼š{format_size(total)}
    â””â”€ æ–‡ä»¶æ€»æ•°ï¼š{data['FileCount']:,} ä¸ª"""

    # 3. VIPè¯¦æƒ…ï¼ˆæ‹†åˆ†å¤šä¸ªæƒç›Šï¼‰
    vip_details = []
    # æ·»åŠ åŸºç¡€VIPä¿¡æ¯
    #vip_details.append(f"â”œâ”€ ç­‰çº§ï¼š{data['VipLevel']} | ç±»å‹ï¼š{data['VipExplain']}")
    #vip_details.append(f"â”œâ”€ åˆ°æœŸæ—¶é—´ï¼š{data['VipExpire']}")
    #vip_details.append(f"â””â”€ æƒç›Šåˆ—è¡¨ï¼š")

    # é€ä¸ªæ·»åŠ VIPæƒç›Šï¼ˆå•ç‹¬æˆé¡¹ï¼‰
    for i, vip in enumerate(data['VipInfo'], 1):
        # æœ€åä¸€ä¸ªæƒç›Šç”¨ç‰¹æ®Šç¬¦å·
        symbol = "    â””â”€" if i == len(data['VipInfo']) else "    â”œâ”€"
        vip_details.append(f"{symbol} {vip['vip_label']}ï¼š{vip['start_time']} â†’ {vip['end_time']}")

    vip_info = "ğŸ’ VIPä¼šå‘˜\n" + "\n".join(vip_details)

    # 4. æµé‡ä¸åŠŸèƒ½çŠ¶æ€
    traffic_info = f"""ğŸš€ æµé‡ä¸åŠŸèƒ½
    â”œâ”€ ç›´è¿æµé‡ï¼š{format_size(data['DirectTraffic'])}
    â”œâ”€ åˆ†äº«æµé‡ï¼š{format_size(data['ShareTraffic'])}
    â””â”€ ç›´é“¾åŠŸèƒ½ï¼š{"âœ…å¼€å¯" if data['StraightLink'] else "âŒå…³é—­"}"""

    # 5. å¤‡ä»½ä¿¡æ¯
    backup_info = f"""ğŸ“¦ å¤‡ä»½é…ç½®
    â”œâ”€ ç§»åŠ¨ç«¯ï¼š{data['BackupFileInfo']['MobileTerminalBackupFileName']}
    â””â”€ æ¡Œé¢ç«¯ï¼š{data['BackupFileInfo']['DesktopTerminalBackupFileName']}"""

    # æ‹¼æ¥æœ€ç»ˆæ¶ˆæ¯
    tg_message = "\n\n".join([
        base_title,
        account_info,
        storage_info,
        vip_info,
        traffic_info,
        backup_info
    ])
    # æœ€åä¸€æ¬¡æ€§æ‰“å°å®Œæ•´æ¶ˆæ¯
    reply_thread_pool.submit(send_reply, message, tg_message)
from zhuli115 import accept_invite
# Telegramæœºå™¨äººæ¶ˆæ¯å¤„ç†ï¼ˆä¿®æ”¹æ˜¾ç¤ºæ ¼å¼ï¼‰
@bot.message_handler(commands=['zhuli115'])
def handle_start(message):
    user_id = message.from_user.id
    if user_id != TG_ADMIN_USER_ID:
        reply_thread_pool.submit(send_reply, message, "æ‚¨æ²¡æœ‰æƒé™ä½¿ç”¨æ­¤æœºå™¨äººã€‚")
        return
    if os.getenv("ENV_115_COOKIES", ""):
        reply_thread_pool.submit(send_reply, message, "å¼€å§‹è‡ªåŠ¨åŠ©åŠ›ã€‚åŠ©åŠ›æˆåŠŸåè¯·åˆ·æ–°115æ´»åŠ¨é¡µé¢æŸ¥çœ‹ã€‚")
        accept_invite(f"{message.text}")
        reply_thread_pool.submit(send_reply, message, "å·²å®Œæˆè‡ªåŠ¨åŠ©åŠ›ï¼Œè¯·åˆ·æ–°115æ´»åŠ¨é¡µé¢æŸ¥çœ‹ã€‚")
    else:
        reply_thread_pool.submit(send_reply, message, "115è´¦å·æœªé…ç½®ï¼Œæ— æ³•åŠ©åŠ›ï¼Œè¯·å…ˆé…ç½®è´¦å·ä¿¡æ¯ã€‚ä½¿ç”¨æ–¹æ³•ä¾‹å¦‚/zhuli115 AAAAAA BBBBBB CCCCCC")

# Telegramæœºå™¨äººæ¶ˆæ¯å¤„ç†ï¼ˆä¿®æ”¹æ˜¾ç¤ºæ ¼å¼ï¼‰
@bot.message_handler(commands=['start'])
def handle_start(message):
    user_id = message.from_user.id
    if user_id != TG_ADMIN_USER_ID:
        reply_thread_pool.submit(send_reply, message, "æ‚¨æ²¡æœ‰æƒé™ä½¿ç”¨æ­¤æœºå™¨äººã€‚")
        return
    reply_thread_pool.submit(send_reply, message, "æœºå™¨äººå·²å¯åŠ¨")
    # ç‰ˆæœ¬æ£€æŸ¥å·²ç¦ç”¨
    # try:
    #     # ç­‰å¾…botå¯¹è±¡åˆå§‹åŒ–å®Œæˆ
    #     if bot:
    #         # è·å–é¢‘é“ä¿¡æ¯ï¼ˆè¿”å›Chatå¯¹è±¡ï¼Œè€Œéå­—å…¸ï¼‰
    #         channel_chat = bot.get_chat('@tgto123update')
    #         # è·å–ç½®é¡¶æ¶ˆæ¯ï¼ˆç›´æ¥è®¿é—®å¯¹è±¡å±æ€§ï¼Œè€Œéå­—å…¸getï¼‰
    #         pinned_message = channel_chat.pinned_message
    #         reply_thread_pool.submit(send_message,f"ğŸš€ tgto123 å½“å‰ç‰ˆæœ¬ä¸º{version}ï¼Œæœ€æ–°ç‰ˆæœ¬è¯·è§ï¼š\nhttps://t.me/tgto123update/{pinned_message.message_id}")
    # except Exception as e:
    #     logger.error(f"è½¬å‘é¢‘é“æ¶ˆæ¯å¤±è´¥: {str(e)}")

def save_env_filter(new_filter_value):
    """æŒä¹…åŒ–ä¿å­˜è¿‡æ»¤è¯åˆ°db/user.envæ–‡ä»¶"""
    env_file_path = os.path.join('db', 'user.env')
    
    # ç¡®ä¿æ–‡ä»¶å­˜åœ¨
    if not os.path.exists(env_file_path):
        logger.warning(f"{env_file_path} æ–‡ä»¶ä¸å­˜åœ¨")
        return False
    
    try:
        # è¯»å–æ–‡ä»¶å†…å®¹
        with open(env_file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        # æŸ¥æ‰¾å¹¶æ›¿æ¢ENV_FILTERè¡Œ
        updated_lines = []
        found = False
        for line in lines:
            if line.startswith('ENV_FILTER='):
                updated_lines.append(f'ENV_FILTER={new_filter_value}\n')
                found = True
            else:
                updated_lines.append(line)
        
        # å¦‚æœæ²¡æ‰¾åˆ°ENV_FILTERè¡Œï¼Œåˆ™æ·»åŠ 
        if not found:
            # æ‰¾åˆ°é¢‘é“ç›‘æ§é…ç½®éƒ¨åˆ†ï¼Œåœ¨åˆé€‚çš„ä½ç½®æ·»åŠ 
            insert_index = -1
            for i, line in enumerate(lines):
                if '# æ£€æŸ¥æ–°æ¶ˆæ¯çš„æ—¶é—´é—´éš”ï¼ˆåˆ†é’Ÿï¼‰' in line:
                    insert_index = i + 2
                    break
            if insert_index != -1:
                updated_lines.insert(insert_index, f'ENV_FILTER={new_filter_value}\n')
            else:
                # å¦‚æœæ‰¾ä¸åˆ°åˆé€‚ä½ç½®ï¼Œå°±æ·»åŠ åˆ°æ–‡ä»¶æœ«å°¾
                updated_lines.append(f'\nENV_FILTER={new_filter_value}\n')
        
        # å†™å›æ–‡ä»¶
        with open(env_file_path, 'w', encoding='utf-8') as f:
            f.writelines(updated_lines)
        
        return True
    except Exception as e:
        logger.error(f"ä¿å­˜ç¯å¢ƒå˜é‡å¤±è´¥ï¼š{str(e)}")
        return False

@bot.message_handler(commands=['add'])
def add_filter(message):
    user_id = message.from_user.id
    if user_id != TG_ADMIN_USER_ID:
        reply_thread_pool.submit(send_reply, message, "æ‚¨æ²¡æœ‰æƒé™ä½¿ç”¨æ­¤æœºå™¨äººã€‚")
        return
    global FILTER, filter_pattern
    try:
        # å±•ç¤ºå½“å‰è¿‡æ»¤è¯å’Œç”¨æ³•
        current_filters_text = FILTER if FILTER else "æ— ï¼ˆæœªè®¾ç½®ä»»ä½•è¿‡æ»¤è¯ï¼‰"
        usage_text = "â„¹ï¸ ç”¨æ³•ï¼š\n- æ·»åŠ è¿‡æ»¤è¯ï¼š/add å…³é”®è¯\nï¼ˆä¾‹ï¼š/add WALK   /add WALK|æƒåŠ›çš„æ¸¸æˆï¼‰\n- åˆ é™¤è¿‡æ»¤è¯ï¼š/remove å…³é”®è¯\nï¼ˆä¾‹ï¼š/remove æƒåŠ›çš„æ¸¸æˆ   /remove WALK|æƒåŠ›çš„æ¸¸æˆï¼‰"
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å‚æ•°
        if len(message.text.split()) < 2:
            reply_thread_pool.submit(send_reply, message, f"ğŸ“Œ å½“å‰è¿‡æ»¤è¯ï¼š{current_filters_text} ï¼ˆå¤šä¸ªç”¨|åˆ†éš”ï¼Œå‘½ä¸­çš„å†…å®¹ä¼šè¢«è½¬å­˜ï¼Œä¸ºç©ºåˆ™ä¼šè½¬å­˜æ‰€æœ‰èµ„æºï¼‰\nâŒ è¯·è¾“å…¥è¦æ·»åŠ çš„è¿‡æ»¤è¯ï¼ˆä¾‹ï¼š/add WALKï¼‰\n\n{usage_text}")
            logger.error(f"ç”¨æˆ· {message.from_user.id} æ‰§è¡Œ/addå¤±è´¥ï¼šæ— è¾“å…¥å‚æ•°")
            return
        
        # è·å–ç”¨æˆ·è¾“å…¥çš„è¿‡æ»¤è¯å¹¶æ¸…ç†
        new_filters_text = message.text.split(maxsplit=1)[1].strip()
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºç©ºå­—ç¬¦ä¸²
        if not new_filters_text:
            reply_thread_pool.submit(send_reply, message, f"ğŸ“Œ å½“å‰è¿‡æ»¤è¯ï¼š{current_filters_text} ï¼ˆå¤šä¸ªç”¨|åˆ†éš”ï¼Œå‘½ä¸­çš„å†…å®¹ä¼šè¢«è½¬å­˜ï¼Œä¸ºç©ºåˆ™ä¼šè½¬å­˜æ‰€æœ‰èµ„æºï¼‰\nâŒ è¯·è¾“å…¥è¦æ·»åŠ çš„è¿‡æ»¤è¯ï¼ˆä¾‹ï¼š/add WALK æˆ– /add WALK|æƒåŠ›çš„æ¸¸æˆï¼‰\n\n{usage_text}")
            logger.error(f"ç”¨æˆ· {message.from_user.id} æ‰§è¡Œ/addå¤±è´¥ï¼šå‚æ•°ä¸ºç©º")
            return
        
        # æ‹†åˆ†ç”¨æˆ·è¾“å…¥çš„å¤šä¸ªè¿‡æ»¤è¯
        new_filters_list = [f.strip() for f in new_filters_text.split("|") if f.strip()]
        
        # æ‹†åˆ†ç°æœ‰è¿‡æ»¤è¯
        current_filters = FILTER.split("|") if FILTER else []
        
        # è®°å½•æ·»åŠ ç»“æœ
        added_filters = []
        existing_filters = []
        
        # æ£€æŸ¥æ¯ä¸ªè¿‡æ»¤è¯æ˜¯å¦å·²å­˜åœ¨å¹¶æ·»åŠ 
        for new_filter in new_filters_list:
            if new_filter not in current_filters:
                added_filters.append(new_filter)
                current_filters.append(new_filter)
            else:
                existing_filters.append(new_filter)
        
        # å¦‚æœæ²¡æœ‰æ·»åŠ ä»»ä½•æ–°è¿‡æ»¤è¯
        if not added_filters:
            reply_thread_pool.submit(send_reply, message, f"ğŸ“Œ å½“å‰è¿‡æ»¤è¯ï¼š{current_filters_text} ï¼ˆå¤šä¸ªç”¨|åˆ†éš”ï¼Œå‘½ä¸­çš„å†…å®¹ä¼šè¢«è½¬å­˜ï¼Œä¸ºç©ºåˆ™ä¼šè½¬å­˜æ‰€æœ‰èµ„æºï¼‰\nâš ï¸ æ‰€æœ‰è¿‡æ»¤è¯ã€Œ{', '.join(existing_filters)}ã€å·²å­˜åœ¨ï¼Œæ— éœ€é‡å¤æ·»åŠ \n\n{usage_text}")
            return
        
        # æ„å»ºæ–°çš„è¿‡æ»¤è¯å­—ç¬¦ä¸²
        FILTER = "|".join(current_filters)
        
        # æŒä¹…åŒ–ä¿å­˜åˆ°æ–‡ä»¶
        if not save_env_filter(FILTER):
            reply_thread_pool.submit(send_reply, message, f"ğŸ“Œ å½“å‰è¿‡æ»¤è¯ï¼š{current_filters_text} ï¼ˆå¤šä¸ªç”¨|åˆ†éš”ï¼Œå‘½ä¸­çš„å†…å®¹ä¼šè¢«è½¬å­˜ï¼Œä¸ºç©ºåˆ™ä¼šè½¬å­˜æ‰€æœ‰èµ„æºï¼‰\nâš ï¸ è¿‡æ»¤è¯æ·»åŠ æˆåŠŸï¼Œä½†ä¿å­˜åˆ°æ–‡ä»¶å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨åœ¨é…ç½®é¡µé¢æ›´æ–°\n\n{usage_text}")
        
        # é‡å»ºæ­£åˆ™å¯¹è±¡
        filter_pattern = re.compile(FILTER, re.IGNORECASE)
        
        # æ„å»ºåé¦ˆæ¶ˆæ¯
        feedback_msg = f"ğŸ“Œ å½“å‰è¿‡æ»¤è¯ï¼š{current_filters_text} ï¼ˆå¤šä¸ªç”¨|åˆ†éš”ï¼Œå‘½ä¸­çš„å†…å®¹ä¼šè¢«è½¬å­˜ï¼Œä¸ºç©ºåˆ™ä¼šè½¬å­˜æ‰€æœ‰èµ„æºï¼‰\n"
        
        if added_filters:
            feedback_msg += f"âœ… å·²æ·»åŠ è¿‡æ»¤è¯ï¼šã€Œ{', '.join(added_filters)}ã€\n"
        
        if existing_filters:
            feedback_msg += f"âš ï¸ å·²å­˜åœ¨çš„è¿‡æ»¤è¯ï¼šã€Œ{', '.join(existing_filters)}ã€\n"
        
        feedback_msg += f"ğŸ“Œ æ›´æ–°åè¿‡æ»¤è¯ï¼š{FILTER}\n\n{usage_text}"
        
        # å‘é€æˆåŠŸåé¦ˆ
        reply_thread_pool.submit(send_reply, message, feedback_msg)
        logger.info(f"ç”¨æˆ· {message.from_user.id} æ‰§è¡Œ/addï¼Œæ·»åŠ è¿‡æ»¤è¯ï¼š{', '.join(added_filters)}ï¼Œå·²å­˜åœ¨ï¼š{', '.join(existing_filters)}ï¼Œæ›´æ–°åï¼š{FILTER}")
        
    except Exception as e:
        reply_thread_pool.submit(send_reply, message, f"æ“ä½œå¤±è´¥ï¼š{str(e)}")
        logger.info(f"ç”¨æˆ· {message.from_user.id} æ‰§è¡Œ/addå‡ºé”™ï¼š{str(e)}")

@bot.message_handler(commands=['remove'])
def remove_filter(message):
    user_id = message.from_user.id
    if user_id != TG_ADMIN_USER_ID:
        reply_thread_pool.submit(send_reply, message, "æ‚¨æ²¡æœ‰æƒé™ä½¿ç”¨æ­¤æœºå™¨äººã€‚")
        return

    global FILTER, filter_pattern
    try:
        # å±•ç¤ºå½“å‰è¿‡æ»¤è¯å’Œç”¨æ³•
        current_filters_text = FILTER if FILTER else "æ— ï¼ˆæœªè®¾ç½®ä»»ä½•è¿‡æ»¤è¯ï¼‰"
        usage_text = "â„¹ï¸ ç”¨æ³•ï¼š\n- æ·»åŠ è¿‡æ»¤è¯ï¼š/add å…³é”®è¯ï¼ˆä¾‹ï¼š/add WALKï¼‰\n- åˆ é™¤è¿‡æ»¤è¯ï¼š/remove å…³é”®è¯ï¼ˆä¾‹ï¼š/remove æƒåŠ›çš„æ¸¸æˆï¼‰"
        
        # æ£€æŸ¥å½“å‰æ˜¯å¦æœ‰è¿‡æ»¤è¯
        if not FILTER:
            reply_thread_pool.submit(send_reply, message, f"ğŸ“Œ å½“å‰è¿‡æ»¤è¯ï¼š{current_filters_text} ï¼ˆå¤šä¸ªç”¨|åˆ†éš”ï¼Œå‘½ä¸­çš„å†…å®¹ä¼šè¢«è½¬å­˜ï¼Œä¸ºç©ºåˆ™ä¼šè½¬å­˜æ‰€æœ‰èµ„æºï¼‰\nâš ï¸ å½“å‰æ— ä»»ä½•è¿‡æ»¤è¯ï¼Œæ— éœ€åˆ é™¤\n\n{usage_text}")
            logger.error(f"ç”¨æˆ· {message.from_user.id} æ‰§è¡Œ/removeå¤±è´¥ï¼šå½“å‰æ— è¿‡æ»¤è¯")
            return
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å‚æ•°
        if len(message.text.split()) < 2:
            reply_thread_pool.submit(send_reply, message, f"ğŸ“Œ å½“å‰è¿‡æ»¤è¯ï¼š{current_filters_text} ï¼ˆå¤šä¸ªç”¨|åˆ†éš”ï¼Œå‘½ä¸­çš„å†…å®¹ä¼šè¢«è½¬å­˜ï¼Œä¸ºç©ºåˆ™ä¼šè½¬å­˜æ‰€æœ‰èµ„æºï¼‰\nâŒ è¯·è¾“å…¥è¦åˆ é™¤çš„è¿‡æ»¤è¯ï¼ˆä¾‹ï¼š/remove æƒåŠ›çš„æ¸¸æˆï¼‰\n\n{usage_text}")
            logger.error(f"ç”¨æˆ· {message.from_user.id} æ‰§è¡Œ/removeå¤±è´¥ï¼šæ— è¾“å…¥å‚æ•°")
            return
        
        # è·å–ç”¨æˆ·è¾“å…¥çš„è¿‡æ»¤è¯å¹¶æ¸…ç†
        del_filters_text = message.text.split(maxsplit=1)[1].strip()
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºç©ºå­—ç¬¦ä¸²
        if not del_filters_text:
            reply_thread_pool.submit(send_reply, message, f"ğŸ“Œ å½“å‰è¿‡æ»¤è¯ï¼š{current_filters_text} ï¼ˆå¤šä¸ªç”¨|åˆ†éš”ï¼Œå‘½ä¸­çš„å†…å®¹ä¼šè¢«è½¬å­˜ï¼Œä¸ºç©ºåˆ™ä¼šè½¬å­˜æ‰€æœ‰èµ„æºï¼‰\nâŒ è¯·è¾“å…¥è¦åˆ é™¤çš„è¿‡æ»¤è¯ï¼ˆä¾‹ï¼š/remove æƒåŠ›çš„æ¸¸æˆ æˆ– /remove WALK|æƒåŠ›çš„æ¸¸æˆï¼‰\n\n{usage_text}")
            logger.error(f"ç”¨æˆ· {message.from_user.id} æ‰§è¡Œ/removeå¤±è´¥ï¼šå‚æ•°ä¸ºç©º")
            return
        
        # æ‹†åˆ†ç”¨æˆ·è¾“å…¥çš„å¤šä¸ªè¿‡æ»¤è¯
        del_filters = [f.strip() for f in del_filters_text.split("|") if f.strip()]
        
        # æ‹†åˆ†ç°æœ‰è¿‡æ»¤è¯
        current_filters = FILTER.split("|") if FILTER else []
        
        # è®°å½•åˆ é™¤ç»“æœ
        deleted_filters = []
        not_found_filters = []
        
        # æ£€æŸ¥æ¯ä¸ªè¿‡æ»¤è¯æ˜¯å¦å­˜åœ¨å¹¶åˆ é™¤
        for del_filter in del_filters:
            if del_filter in current_filters:
                deleted_filters.append(del_filter)
            else:
                not_found_filters.append(del_filter)
        
        # åˆ é™¤å­˜åœ¨çš„è¿‡æ»¤è¯
        new_filters = [f for f in current_filters if f not in deleted_filters]
        FILTER = "|".join(new_filters) if new_filters else ""
        
        # æŒä¹…åŒ–ä¿å­˜åˆ°æ–‡ä»¶
        if not save_env_filter(FILTER):
            reply_thread_pool.submit(send_reply, message, f"ğŸ“Œ å½“å‰è¿‡æ»¤è¯ï¼š{current_filters_text} ï¼ˆå¤šä¸ªç”¨|åˆ†éš”ï¼Œå‘½ä¸­çš„å†…å®¹ä¼šè¢«è½¬å­˜ï¼Œä¸ºç©ºåˆ™ä¼šè½¬å­˜æ‰€æœ‰èµ„æºï¼‰\nâš ï¸ è¿‡æ»¤è¯åˆ é™¤æˆåŠŸï¼Œä½†ä¿å­˜åˆ°æ–‡ä»¶å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨åœ¨é…ç½®é¡µé¢æ›´æ–°\n\n{usage_text}")

        # é‡å»ºæ­£åˆ™å¯¹è±¡
        filter_pattern = re.compile(FILTER, re.IGNORECASE)
        
        # æ„å»ºåé¦ˆæ¶ˆæ¯
        updated_filters_text = FILTER if FILTER else "æ— "
        feedback_msg = f"ğŸ“Œ å½“å‰è¿‡æ»¤è¯ï¼š{current_filters_text} ï¼ˆå¤šä¸ªç”¨|åˆ†éš”ï¼Œå‘½ä¸­çš„å†…å®¹ä¼šè¢«è½¬å­˜ï¼Œä¸ºç©ºåˆ™ä¼šè½¬å­˜æ‰€æœ‰èµ„æºï¼‰\n"
        
        if deleted_filters:
            feedback_msg += f"âœ… å·²åˆ é™¤è¿‡æ»¤è¯ï¼šã€Œ{', '.join(deleted_filters)}ã€\n"
        
        if not_found_filters:
            feedback_msg += f"âš ï¸ æœªæ‰¾åˆ°çš„è¿‡æ»¤è¯ï¼šã€Œ{', '.join(not_found_filters)}ã€\n"
        
        feedback_msg += f"ğŸ“Œ æ›´æ–°åè¿‡æ»¤è¯ï¼š{updated_filters_text}\n\n{usage_text}"
        
        # å‘é€æˆåŠŸåé¦ˆ
        reply_thread_pool.submit(send_reply, message, feedback_msg)
        logger.info(f"ç”¨æˆ· {message.from_user.id} æ‰§è¡Œ/removeï¼Œåˆ é™¤è¿‡æ»¤è¯ï¼š{', '.join(deleted_filters)}ï¼Œæœªæ‰¾åˆ°ï¼š{', '.join(not_found_filters)}ï¼Œæ›´æ–°åï¼š{FILTER}")
        
    except Exception as e:
        reply_thread_pool.submit(send_reply, message, f"æ“ä½œå¤±è´¥ï¼š{str(e)}")
        logger.error(f"ç”¨æˆ· {message.from_user.id} æ‰§è¡Œ/removeå‡ºé”™ï¼š{str(e)}")

@bot.message_handler(commands=['share'])
def handle_share_command(message):
    user_id = message.from_user.id
    if user_id != TG_ADMIN_USER_ID:
        reply_thread_pool.submit(send_reply, message, "æ‚¨æ²¡æœ‰æƒé™ä½¿ç”¨æ­¤å‘½ä»¤ã€‚")
        return
    try:
        command_parts = message.text.split(' ', 1)
        if len(command_parts) < 2 or not command_parts[1].strip():
            reply_thread_pool.submit(send_reply, message, "è¯·æä¾›æœç´¢å…³é”®è¯ï¼Œä¾‹å¦‚ï¼š/share æƒåŠ›çš„æ¸¸æˆ")
            return
        keyword = command_parts[1].strip()
        user_id = message.from_user.id
        reply_thread_pool.submit(send_reply, message, f"æ­£åœ¨æœç´¢åŒ…å« '{keyword}' çš„æ–‡ä»¶å¤¹...")
        client = init_123_client()
        import threading
        threading.Thread(target=perform_search, args=(client, keyword, user_id, message.chat.id)).start()
    except Exception as e:
        reply_thread_pool.submit(send_reply, message, f"æ“ä½œå¤±è´¥: {str(e)}")
        logger.error(f"å¤„ç†/shareå‘½ä»¤å¤±è´¥: {str(e)}")

def build_folder_message(results):
    """
    æ ¸å¿ƒè§„åˆ™ï¼š
    1. ç¼–å·é¡ºåºï¼š1-20ä¸¥æ ¼å¯¹åº”è¾“å…¥é¡ºåºï¼Œä¸æ‰“ä¹±ã€ä¸é‡æ’
    2. å¤§ç»„åˆ’åˆ†ï¼šæŒ‰â€œåŸå§‹ç¼–å·è¿ç»­+å‰ä¸¤å±‚ç›®å½•ç›¸åŒâ€åˆ’å¤§ç»„ï¼ˆéè¿ç»­/å‰ç¼€ä¸åŒåˆ™å•ç‹¬æˆç»„ï¼‰
    3. ç»„å†…åˆå¹¶ï¼šæ¯ä¸ªå¤§ç»„å†…è®¡ç®—æ‰€æœ‰è·¯å¾„çš„å…¬å…±å‰ç¼€ï¼ˆå«å‰ä¸¤å±‚å¤–çš„æ·±å±‚å‰ç¼€ï¼‰ï¼Œåˆå¹¶ä¸ºçˆ¶ç›®å½•
    4. å•ç‹¬ç»„å¤„ç†ï¼šç»„å†…ä»…1æ¡è·¯å¾„æ—¶ï¼Œè‡ªåŠ¨ä½œä¸ºå•ç‹¬ç»„ï¼Œä¸å¼ºåˆ¶åˆå¹¶å…¬å…±å‰ç¼€
    """
    # æ­¥éª¤1ï¼šé¢„å¤„ç†è·¯å¾„ï¼Œæå–å…³é”®ä¿¡æ¯ï¼ˆä¿ç•™åŸå§‹ç¼–å·ï¼‰
    path_info_list = []
    for orig_seq, item in enumerate(results, start=1):  # åŸå§‹ç¼–å·1-20
        raw_path = item.get("path", "").strip("/")
        dir_list = [p.strip() for p in raw_path.split("/") if p.strip()]  # æ‹†åˆ†ç›®å½•åˆ—è¡¨
        dir_len = len(dir_list)
        
        # æå–å‰ä¸¤å±‚ç›®å½•ä½œä¸ºåˆ†ç»„keyï¼ˆä¸è¶³ä¸¤å±‚åˆ™å–å®é™…å±‚æ•°ï¼Œå¦‚1å±‚ï¼‰
        if dir_len >= 2:
            group_key = tuple(dir_list[:2])  # å‰ä¸¤å±‚ç›®å½•ä½œä¸ºkeyï¼ˆå¦‚("Resource","å¤§åŒ…èµ„æº")ï¼‰
        else:
            group_key = tuple(dir_list)  # ä¸è¶³ä¸¤å±‚ï¼Œç”¨å…¨éƒ¨ç›®å½•ä½œä¸ºkeyï¼ˆå¦‚("Video",)ï¼‰
        
        path_info_list.append({
            "orig_seq": orig_seq,
            "raw_path": raw_path,
            "dir_list": dir_list,
            "dir_len": dir_len,
            "group_key": group_key,
            "is_root": dir_len == 1  # æ ¹ç›®å½•åˆ¤æ–­ï¼šä»…1å±‚ç›®å½•
        })
    if not path_info_list:
        return "æœªæ‰¾åˆ°åŒ¹é…æ–‡ä»¶å¤¹"

    # å·¥å…·å‡½æ•°1ï¼šè®¡ç®—ä¸€ç»„è·¯å¾„çš„å…¬å…±å‰ç¼€é•¿åº¦ï¼ˆæ ¸å¿ƒä¿®æ­£ï¼ï¼‰
    def get_group_common_prefix(group_paths):
        if len(group_paths) == 1:
            # å•ç‹¬ç»„ï¼šå…¬å…±å‰ç¼€å–åˆ°â€œå€’æ•°ç¬¬äºŒå±‚â€ï¼Œç¡®ä¿å­è·¯å¾„æ˜¾ç¤ºæœ€å1å±‚
            single_path = group_paths[0]
            return max(0, single_path["dir_len"] - 1)
        # å¤šè·¯å¾„ç»„ï¼šå…³é”®ä¿®æ­£â€”â€”å…¬å…±å‰ç¼€é•¿åº¦ â‰¤ æœ€çŸ­è·¯å¾„çš„dir_len - 1
        min_dir_len = min(p["dir_len"] for p in group_paths)
        max_allowed_len = min_dir_len - 1  # ç¦æ­¢å…¬å…±å‰ç¼€åŒ…å«æœ€çŸ­è·¯å¾„çš„æœ€åä¸€å±‚
        base_dir = group_paths[0]["dir_list"]
        common_len = max_allowed_len  # åˆå§‹åŒ–ä¸ºæœ€å¤§å…è®¸é•¿åº¦
        # æ¯”è¾ƒæ‰€æœ‰è·¯å¾„ï¼Œæ‰¾åˆ°æœ€é•¿å…¬å…±å‰ç¼€ï¼ˆä¸è¶…è¿‡max_allowed_lenï¼‰
        for p in group_paths[1:]:
            curr_dir = p["dir_list"]
            curr_common = 0
            while curr_common < common_len and curr_dir[curr_common] == base_dir[curr_common]:
                curr_common += 1
            if curr_common < common_len:
                common_len = curr_common
            if common_len == 0:
                break
        return common_len

    # å·¥å…·å‡½æ•°2ï¼šç”Ÿæˆçˆ¶ç›®å½•å­—ç¬¦ä¸²å’Œå­è·¯å¾„å­—ç¬¦ä¸²
    def get_parent_subpath(path, common_len):
        dir_list = path["dir_list"]
        # çˆ¶ç›®å½•ï¼šå…¬å…±å‰ç¼€éƒ¨åˆ†
        parent_dir = dir_list[:common_len] if common_len > 0 else []
        parent_str = " / ".join(parent_dir) if parent_dir else ("æ ¹ç›®å½•" if path["is_root"] else "")
        # å­è·¯å¾„ï¼šå…¬å…±å‰ç¼€ä¹‹åçš„éƒ¨åˆ†ï¼ˆè‹¥ä¸ºç©ºï¼Œæ˜¾ç¤ºæœ€å1å±‚ç›®å½•ï¼‰
        sub_dir = dir_list[common_len:] if common_len < path["dir_len"] else [dir_list[-1]]
        sub_path_str = " / ".join(sub_dir)
        return parent_str, sub_path_str

    # æ­¥éª¤2ï¼šæŒ‰â€œç¼–å·è¿ç»­+group_keyç›¸åŒâ€åˆ’å¤§ç»„ï¼ˆæ ¸å¿ƒåˆ†ç»„é€»è¾‘ï¼‰
    groups = []
    if path_info_list:
        current_group = [path_info_list[0]]  # åˆå§‹åŒ–å½“å‰ç»„ï¼ˆç¬¬ä¸€ä¸ªè·¯å¾„ï¼‰
        for path in path_info_list[1:]:
            prev_path = current_group[-1]
            # åˆ¤æ–­ï¼šå½“å‰è·¯å¾„ä¸å‰ä¸€ä¸ªè·¯å¾„â€œç¼–å·è¿ç»­ï¼ˆå¿…ç„¶æ»¡è¶³ï¼ŒæŒ‰é¡ºåºéå†ï¼‰ä¸”group_keyç›¸åŒâ€
            if path["group_key"] == prev_path["group_key"]:
                current_group.append(path)
            else:
                # ä¸åŒgroup_keyï¼Œä¿å­˜å½“å‰ç»„ï¼Œæ–°å»ºç»„
                groups.append(current_group)
                current_group = [path]
        groups.append(current_group)  # åŠ å…¥æœ€åä¸€ä¸ªç»„

    # æ­¥éª¤3ï¼šå¤„ç†æ¯ä¸ªå¤§ç»„ï¼Œåˆå¹¶ç»„å†…å…¬å…±å‰ç¼€
    processed_groups = []
    for group in groups:
        common_len = get_group_common_prefix(group)  # ç»„å†…å…¬å…±å‰ç¼€é•¿åº¦
        group_parent = ""  # ç»„çš„ç»Ÿä¸€çˆ¶ç›®å½•ï¼ˆå–ç¬¬ä¸€æ¡è·¯å¾„çš„çˆ¶ç›®å½•ï¼Œç»„å†…æ‰€æœ‰è·¯å¾„çˆ¶ç›®å½•ç›¸åŒï¼‰
        group_paths = []
        
        for path in group:
            parent_str, sub_path_str = get_parent_subpath(path, common_len)
            # ç»Ÿä¸€ç»„çš„çˆ¶ç›®å½•ï¼ˆç»„å†…æ‰€æœ‰è·¯å¾„çˆ¶ç›®å½•ä¸€è‡´ï¼Œå–ç¬¬ä¸€æ¡çš„å³å¯ï¼‰
            if not group_parent:
                group_parent = parent_str
            # æ”¶é›†ç»„å†…è·¯å¾„ï¼ˆå«åŸå§‹ç¼–å·å’Œå­è·¯å¾„ï¼‰
            group_paths.append({
                "orig_seq": path["orig_seq"],
                "sub_path": sub_path_str
            })
        
        processed_groups.append({
            "parent_str": group_parent,
            "paths": group_paths  # ç»„å†…è·¯å¾„æŒ‰åŸå§‹ç¼–å·é¡ºåº
        })

    # æ­¥éª¤4ï¼šæŒ‰åŸå§‹ç¼–å·1-20æ‹¼æ¥æœ€ç»ˆæ¶ˆæ¯ï¼ˆç¡®ä¿é¡ºåºä¸å˜ï¼‰
    msg = "æ‰¾åˆ°ä»¥ä¸‹åŒ¹é…çš„æ–‡ä»¶å¤¹ï¼Œè¯·è¾“å…¥åºå·é€‰æ‹©ï¼š\n\n"
    # ç”¨å­—å…¸æš‚å­˜æ‰€æœ‰è·¯å¾„ï¼ˆkey=åŸå§‹ç¼–å·ï¼Œvalue=ï¼ˆçˆ¶ç›®å½•ï¼Œå­è·¯å¾„ï¼‰ï¼‰
    seq_path_dict = {}
    for group in processed_groups:
        parent = group["parent_str"]
        for path in group["paths"]:
            seq_path_dict[path["orig_seq"]] = (parent, path["sub_path"])

    # æŒ‰ç¼–å·1-20ä¾æ¬¡éå†ï¼Œæ˜¾ç¤ºç»“æœ
    last_parent = None  # é¿å…é‡å¤æ˜¾ç¤ºçˆ¶ç›®å½•
    for orig_seq in range(1, len(seq_path_dict) + 1):
        parent, sub_path = seq_path_dict[orig_seq]
        
        # çˆ¶ç›®å½•å˜åŒ–æ—¶ï¼Œæ˜¾ç¤ºæ–°çˆ¶ç›®å½•
        if parent != last_parent:
            msg += f"ğŸ“ {parent}\n"
            last_parent = parent
        
        # æ˜¾ç¤ºç¼–å·å’Œå­è·¯å¾„
        msg += f"      {orig_seq}ï¼š{sub_path}\n"

        # ç»„é—´ç©ºè¡Œï¼ˆåˆ¤æ–­ä¸‹ä¸€ä¸ªç¼–å·çš„çˆ¶ç›®å½•æ˜¯å¦å˜åŒ–ï¼‰
        next_seq = orig_seq + 1
        if next_seq in seq_path_dict:
            next_parent = seq_path_dict[next_seq][0]
            if next_parent != parent:
                msg += "\n"

    msg += "\nè¯·è¾“å…¥åºå·ï¼ˆä¾‹ï¼š1ï¼‰é€‰æ‹©ï¼Œå¤šé€‰ç”¨ç©ºæ ¼åˆ†éš”ï¼ˆä¾‹ï¼š1 2 3ï¼‰"
    return msg




def perform_search(client, keyword, user_id, chat_id):
    try:
        import asyncio
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        results = loop.run_until_complete(search_123_files(client, keyword))
        if not results:
            reply_thread_pool.submit(send_message_with_id, chat_id, "æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶å¤¹")
            return
        user_state_manager.set_state(user_id, "SELECTING_FILE", json.dumps(results))
        # æ„å»ºç¬¦åˆè¦æ±‚çš„ç»“æœæ¶ˆæ¯
        #message = "æ‰¾åˆ°ä»¥ä¸‹åŒ¹é…çš„æ–‡ä»¶å¤¹ï¼Œè¯·è¾“å…¥åºå·é€‰æ‹©ï¼š\n"  # ç§»é™¤å¤šä½™ç©ºè¡Œ
        #for i, item in enumerate(results, 1):
            # ä»…æ˜¾ç¤ºåºå·å’Œå®Œæ•´è·¯å¾„ï¼Œæ— å…¶ä»–ä¿¡æ¯
            #message += f"{i}. {item['path']}\n"  # è·¯å¾„ç›´æ¥å–è‡ªitem['path']
        #message += "\nè¯·è¾“å…¥åºå·ï¼ˆä¾‹å¦‚ï¼š1ï¼‰æ¥é€‰æ‹©éœ€è¦åˆ†äº«çš„æ–‡ä»¶å¤¹\næ”¯æŒå¤šé€‰ï¼Œä»¥ç©ºæ ¼åˆ†éš”ï¼ˆä¾‹å¦‚1 2 3 4 5ï¼‰"
        #logger.info(message)
        #print(results)
        folder_message = build_folder_message(results)
        logger.info("æœç´¢ç»“æœåˆå¹¶å®Œæˆ")
        reply_thread_pool.submit(send_message_with_id, chat_id, folder_message)
    except Exception as e:
        reply_thread_pool.submit(send_message_with_id, chat_id, f"æœç´¢æ–‡ä»¶å¤¹å¤±è´¥: {str(e)}")
        logger.error(f"æœç´¢æ–‡ä»¶å¤¹å¤±è´¥: {str(e)}")
from add_mag import submit_magnet_video_download
def add_magnet_links(client:P123Client, text, upload_dir=None, message=None):
    """è¯†åˆ«æ–‡æœ¬ä¸­çš„å¤šä¸ªç£åŠ›é“¾æ¥å¹¶æ·»åŠ åˆ°ç¦»çº¿ä¸‹è½½

    :param client: P123Clientå®ä¾‹
    :param text: åŒ…å«ç£åŠ›é“¾æ¥çš„æ–‡æœ¬
    :param upload_dir: ä¿å­˜åˆ°ç›®å½•çš„id
    :return: æ¥å£å“åº”ä¿¡æ¯ï¼ŒåŒ…å«æˆåŠŸæäº¤çš„é“¾æ¥æ•°é‡
    """
    import re
    # ç²¾ç¡®åŒ¹é…32/40ä½å“ˆå¸Œï¼Œé¿å…åé¢å¤šå­—ç¬¦
    magnet_pattern = r'magnet:\?xt=urn:btih:(?:[A-Fa-f0-9]{40}(?![A-Fa-f0-9])|[A-Za-z0-9]{32}(?![A-Za-z0-9]))(?:&.*?)?'
    # æå–æ‰€æœ‰ç£åŠ›é“¾æ¥
    magnet_links = re.findall(magnet_pattern, text)
    magnet_links = list(set(magnet_links))
    if not magnet_links:
        return {'status': 'error', 'message': 'æœªæ‰¾åˆ°ç£åŠ›é“¾æ¥', 'added_count': 0}
    logger.debug(f"æ‰¾åˆ°ç£åŠ›é“¾æ¥:{magnet_links}")
    if message:
        reply_thread_pool.submit(send_reply, message, f"æ‰¾åˆ°{len(magnet_links)}æ¡ç£åŠ›é“¾\n{magnet_links}\næ­£åœ¨æ·»åŠ ï¼Œè¯·è€å¿ƒç­‰å¾…")
    added_count = 0
    responses = []
    try:
        # ä¾æ¬¡æäº¤æ¯ä¸ªç£åŠ›é“¾æ¥
        for link in magnet_links:
            response = submit_magnet_video_download(link, client.token, upload_dir)
            ##response = client.offline_add(
            #    url=link,
            #    upload_dir=upload_dir,
             #   async_=False
            #)
            time.sleep(0.5)
            # ä¿å­˜é“¾æ¥å’Œå“åº”çš„å¯¹åº”å…³ç³»
            responses.append({'link': link, 'response': response})
            added_count += 1
        return {'status': 'success', 'data': responses, 'added_count': added_count}
    except Exception as e:
        return {'status': 'error', 'message': f'æ·»åŠ ç£åŠ›é“¾æ¥å¤±è´¥: {str(e)}', 'added_count': added_count}
import base64
import binascii
import re

def robust_normalize_md5(input_str):
    """
    è‡ªåŠ¨è¯†åˆ«MD5æ ¼å¼å¹¶è½¬æ¢ä¸ºåå…­è¿›åˆ¶æ ¼å¼ï¼Œå¼‚å¸¸æ—¶è¿”å›åŸå§‹è¾“å…¥
    
    å‚æ•°:
        input_str: å¾…å¤„ç†çš„è¾“å…¥ï¼ˆå¯ä»¥æ˜¯ä»»ä½•ç±»å‹ï¼‰
    
    è¿”å›:
        è½¬æ¢åçš„åå…­è¿›åˆ¶MD5ï¼ˆå°å†™ï¼‰ï¼Œæˆ–åŸå§‹è¾“å…¥ï¼ˆå¤„ç†å¤±è´¥æ—¶ï¼‰
    """
    # å…ˆæ£€æŸ¥æ˜¯å¦ä¸ºå­—ç¬¦ä¸²ç±»å‹ï¼Œéå­—ç¬¦ä¸²ç›´æ¥è¿”å›åŸå§‹å€¼
    if not isinstance(input_str, str):
        return input_str
    
    # å¤„ç†ç©ºå­—ç¬¦ä¸²
    if not input_str:
        return input_str
    
    # å»é™¤é¦–å°¾ç©ºæ ¼
    processed_str = input_str.strip()
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºåå…­è¿›åˆ¶MD5ï¼ˆ32ä½ï¼Œä»…å«0-9ã€a-fã€A-Fï¼‰
    hex_pattern = re.compile(r'^[0-9a-fA-F]{32}$')
    if hex_pattern.match(processed_str):
        return processed_str.lower()
    
    # å°è¯•Base64è§£ç å¤„ç†
    try:
        # å°è¯•Base64è§£ç ï¼ˆå¤„ç†æ ‡å‡†Base64å’ŒURLå®‰å…¨çš„Base64ï¼‰
        binary_data = base64.b64decode(processed_str, validate=True)
        
        # éªŒè¯MD5å›ºå®šé•¿åº¦ï¼ˆ16å­—èŠ‚ï¼‰
        if len(binary_data) == 16:
            # è½¬æ¢ä¸ºåå…­è¿›åˆ¶å­—ç¬¦ä¸²ï¼ˆå°å†™ï¼‰
            return binascii.hexlify(binary_data).decode('utf-8').lower()
    
    # æ•æ‰Base64è§£ç ç›¸å…³å¼‚å¸¸
    except binascii.Error:
        pass
    # æ•æ‰å…¶ä»–å¯èƒ½çš„å¼‚å¸¸
    except Exception:
        pass
    
    # æ‰€æœ‰å¤„ç†å¤±è´¥ï¼Œè¿”å›åŸå§‹è¾“å…¥
    return input_str

def parse_share_link(message, share_link, up_load_pid=UPLOAD_JSON_TARGET_PID, send_messages=True):
    """è§£æç§’ä¼ é“¾æ¥"""
    if '#' and '$' in share_link:
        None
    else:
        return False
    logger.info("è§£æç§’ä¼ é“¾æ¥...")
    common_base_path = ""
    is_common_path_format = False
    is_v2_etag_format = False
    LEGACY_FOLDER_LINK_PREFIX_V1 = "123FSLinkV1$"
    LEGACY_FOLDER_LINK_PREFIX_V2 = "123FSLinkV2$"
    COMMON_PATH_LINK_PREFIX_V1 = "123FLCPV1$"
    COMMON_PATH_LINK_PREFIX_V2 = "123FLCPV2$"
    COMMON_PATH_DELIMITER = "%"
    
    if share_link.startswith(COMMON_PATH_LINK_PREFIX_V2):
        is_common_path_format = True
        is_v2_etag_format = True
        share_link = share_link[len(COMMON_PATH_LINK_PREFIX_V2):]
    elif share_link.startswith(COMMON_PATH_LINK_PREFIX_V1):
        is_common_path_format = True
        share_link = share_link[len(COMMON_PATH_LINK_PREFIX_V1):]
    elif share_link.startswith(LEGACY_FOLDER_LINK_PREFIX_V2):
        is_v2_etag_format = True
        share_link = share_link[len(LEGACY_FOLDER_LINK_PREFIX_V2):]
    elif share_link.startswith(LEGACY_FOLDER_LINK_PREFIX_V1):
        share_link = share_link[len(LEGACY_FOLDER_LINK_PREFIX_V1):]
    if is_common_path_format:
        delimiter_pos = share_link.find(COMMON_PATH_DELIMITER)
        if delimiter_pos > -1:
            common_base_path = share_link[:delimiter_pos]
            share_link = share_link[delimiter_pos + 1:]
    files = []
    for s_link in share_link.split('$'):
        if not s_link:
            continue
        parts = s_link.split('#')
        if len(parts) < 3:
            continue
        etag = parts[0]
        size = parts[1]
        file_path = '#'.join(parts[2:])
        if is_common_path_format and common_base_path:
            file_path = common_base_path + file_path
        files.append({
            "etag": etag,
            "size": int(size),
            "file_name": file_path,
            "is_v2_etag": is_v2_etag_format
        })
    
    logger.info(f"è§£æåˆ° {len(files)} ä¸ªæ–‡ä»¶")
    
    if not files:
        # ä½¿ç”¨çº¿ç¨‹æ± å‘é€å›å¤
        #if send_messages:
            #reply_thread_pool.submit(send_reply, message, "æœªæ‰¾åˆ°å¯è½¬å­˜çš„æ–‡ä»¶ã€‚")
        return False
    status = True
    # ä½¿ç”¨çº¿ç¨‹æ± å‘é€å›å¤
    if send_messages:
        reply_thread_pool.submit(send_reply_delete, message, f"å¼€å§‹è½¬å­˜ {len(files)} ä¸ªæ–‡ä»¶...")
    
    try:
        # å¼€å§‹è®¡æ—¶
        start_time = time.time()
        
        # åˆå§‹åŒ–123å®¢æˆ·ç«¯
        client = init_123_client()
        
        # è½¬å­˜æ–‡ä»¶
        results = []
        total_files = len(files)
        message_batch = []  # ç”¨äºå­˜å‚¨æ¯æ‰¹æ¶ˆæ¯
        batch_size = 0      # æ‰¹æ¬¡å¤§å°è®¡æ•°å™¨
        total_size = 0      # ç´¯è®¡æˆåŠŸè½¬å­˜æ–‡ä»¶ä½“ç§¯(å­—èŠ‚)
        skip_count = 0      # è·³è¿‡çš„é‡å¤æ–‡ä»¶æ•°é‡
        last_etag = None    # ä¸Šä¸€ä¸ªæˆåŠŸè½¬å­˜æ–‡ä»¶çš„etag
        
        # åˆ›å»ºæ–‡ä»¶å¤¹ç¼“å­˜
        folder_cache = {}
        # ä½¿ç”¨UPLOAD_TARGET_PIDä½œä¸ºæ ¹ç›®å½•
        target_dir_id = up_load_pid
        
        for i, file_info in enumerate(files):
            file_path = file_info.get('file_name', '')
            etag = file_info.get('etag', '')
            size = int(file_info.get('size', 0))
            is_v2_etag = file_info.get('is_v2_etag', False)
            
            if not all([file_path, etag, size]):
                results.append({
                    "success": False,
                    "file_name": file_path or "æœªçŸ¥æ–‡ä»¶",
                    "error": "æ–‡ä»¶ä¿¡æ¯ä¸å®Œæ•´"
                })
                continue
            
            try:
                # å¤„ç†æ–‡ä»¶è·¯å¾„
                path_parts = file_path.split('/')
                file_name = path_parts.pop()
                parent_id = target_dir_id
                
                # åˆ›å»ºç›®å½•ç»“æ„
                current_path = ""
                for part in path_parts:
                    if not part:
                        continue
                    
                    current_path = f"{current_path}/{part}" if current_path else part
                    cache_key = f"{parent_id}/{current_path}"
                    
                    # æ£€æŸ¥ç¼“å­˜
                    if cache_key in folder_cache:
                        parent_id = folder_cache[cache_key]
                        continue
                    
                    # åˆ›å»ºæ–°æ–‡ä»¶å¤¹ï¼ˆå¸¦é‡è¯•ï¼‰
                    retry_count = 3
                    folder = None
                    while retry_count > 0:
                        try:
                            folder = client.fs_mkdir(part, parent_id=parent_id, duplicate=1)
                            time.sleep(0.2)
                            check_response(folder)
                            break
                        except Exception as e:
                            retry_count -= 1
                            logger.warning(f"åˆ›å»ºæ–‡ä»¶å¤¹ {part} å¤±è´¥ (å‰©ä½™é‡è¯•: {retry_count}): {str(e)}")
                            time.sleep(31)
                    
                    if not folder:
                        logger.warning(f"åˆ›å»ºæ–‡ä»¶å¤¹å¤±è´¥: {part}ï¼Œå°†ä½¿ç”¨å½“å‰ç›®å½•")
                    else:
                        folder_id = folder["data"]["Info"]["FileId"]
                        folder_cache[cache_key] = folder_id
                        parent_id = folder_id
                
                # å¤„ç†ETag
                if is_v2_etag:
                    # å®ç°Base62 ETagè½¬Hex
                    etag = optimized_etag_to_hex(etag, True)
                
                # ç§’ä¼ æ–‡ä»¶ï¼ˆå¸¦é‡è¯•ï¼‰
                retry_count = 3
                rapid_resp = None
                while retry_count > 0:
                    # æ£€æŸ¥etagæ˜¯å¦ä¸ä¸Šä¸€ä¸ªæˆåŠŸè½¬å­˜çš„æ–‡ä»¶ç›¸åŒ
                    if last_etag == etag:
                        skip_count += 1
                        logger.info(f"è·³è¿‡é‡å¤æ–‡ä»¶: {file_path}")
                        rapid_resp = {"data": {"Reuse": True, "Skip": True}, "code": 0}  # æ ‡è®°ä¸ºè·³è¿‡
                        break
                    
                    try:
                        rapid_resp = client.upload_file_fast(
                            file_name=file_name,
                            parent_id=parent_id,
                            file_md5=robust_normalize_md5(etag),
                            file_size=size,
                            duplicate=1
                        )
                        check_response(rapid_resp)
                        break
                    except Exception as e:
                        retry_count -= 1
                        logger.warning(f"è½¬å­˜æ–‡ä»¶ {file_name} å¤±è´¥ (å‰©ä½™é‡è¯•: {retry_count}): {str(e)}")
                        if rapid_resp and ("åŒåæ–‡ä»¶" in rapid_resp.get("message", {})):
                            if send_messages:
                                reply_thread_pool.submit(send_reply, message, rapid_resp.get("message", {}))
                            break
                        if rapid_resp and ("Etag" in rapid_resp.get("message", {})):
                            break
                        time.sleep(31)
                
                #if rapid_resp and rapid_resp.get("data", {}):
                if rapid_resp is None:
                    # å¤„ç†æ‰€æœ‰é‡è¯•å¤±è´¥ä¸” rapid_resp ä¸º None çš„åœºæ™¯
                    error_msg = "ç§’ä¼ å¤±è´¥ï¼šæ¥å£è¿”å›ç©ºå€¼ä¸”é‡è¯•è€—å°½"
                    results.append({
                        "success": False,
                        "file_name": file_path,
                        "error": error_msg
                    })
                    dir_path, file_name = os.path.split(file_path)
                    msg = {
                        'status': 'âŒ',
                        'dir': dir_path,
                        'file': f"{file_name} ({error_msg})"
                    }
                    message_batch.append(msg)
                    batch_size += 1
                    logger.error(f"{msg['status']}:{msg['dir']}/{msg['file']}")
                elif rapid_resp.get("code") == 0 and rapid_resp.get("data", {}) and rapid_resp.get("data", {}).get("Reuse", False):
                    # æ£€æŸ¥æ˜¯å¦æ˜¯è·³è¿‡çš„æ–‡ä»¶
                    if rapid_resp.get("data", {}).get("Skip"):
                        # è§£æè·¯å¾„ç»“æ„
                        dir_path, file_name = os.path.split(file_path)
                        msg = {
                            'status': 'ğŸ”„',
                            'dir': dir_path,
                            'file': f"{file_name} (é‡å¤è·³è¿‡)"
                        }
                        message_batch.append(msg)
                        batch_size += 1
                        logger.info(f"{msg['status']}:{msg['dir']}/{msg['file']}")
                    else:
                        # æ›´æ–°ä¸Šä¸€ä¸ªæˆåŠŸè½¬å­˜æ–‡ä»¶çš„etag
                        last_etag = etag
                        results.append({
                            "success": True,
                            "file_name": file_path,
                            "file_id": rapid_resp.get("data", {}).get("FileId", ""),
                            "size": size
                        })
                        total_size += size
                        # è§£æè·¯å¾„ç»“æ„
                        dir_path, file_name = os.path.split(file_path)
                        msg = {
                            'status': 'âœ…',
                            'dir': dir_path,
                            'file': file_name
                        }
                        message_batch.append(msg)
                        batch_size += 1
                        logger.info(f"{msg['status']}:{msg['dir']}/{msg['file']}")
                else:
                    results.append({
                        "success": False,
                        "file_name": file_path,
                        "error": "æ­¤æ–‡ä»¶åœ¨123æœåŠ¡å™¨ä¸å­˜åœ¨ï¼Œæ— æ³•ç§’ä¼ " if rapid_resp.get("data", {}) and (rapid_resp.get("data", {}).get("Reuse", True) == False) else rapid_resp.get("message", "æœªçŸ¥é”™è¯¯")
                    })
                    # è§£æè·¯å¾„ç»“æ„
                    dir_path, file_name = os.path.split(file_path)
                    error_msg = "æ­¤æ–‡ä»¶åœ¨123æœåŠ¡å™¨ä¸å­˜åœ¨ï¼Œæ— æ³•ç§’ä¼ " if rapid_resp.get("data", {}) and (rapid_resp.get("data", {}).get("Reuse", True) == False) else rapid_resp.get("message", "æœªçŸ¥é”™è¯¯")
                    msg = {
                        'status': 'âŒ',
                        'dir': dir_path,
                        'file': f"{file_name} ({error_msg})"
                    }
                    message_batch.append(msg)
                    batch_size += 1
                    logger.warning(f"{msg['status']}:{msg['dir']}/{msg['file']}")
                
                # æ¯10æ¡æ¶ˆæ¯å‘é€ä¸€æ¬¡
                if batch_size % 10 == 0:
                    # ç”Ÿæˆæ ‘çŠ¶ç»“æ„æ¶ˆæ¯
                    tree_messages = defaultdict(lambda: {'âœ…': [], 'âŒ': [], 'ğŸ”„': []})
                    for entry in message_batch:
                        tree_messages[entry['dir']][entry['status']].append(entry['file'])
                    
                    batch_msg = []
                    for dir_path, status_files in tree_messages.items():
                        for status, files in status_files.items():
                            if files:
                                batch_msg.append(f"--- {status} {dir_path}")
                                for i, file in enumerate(files):
                                    prefix = '      â””â”€â”€' if i == len(files)-1 else '      â”œâ”€â”€'
                                    batch_msg.append(f"{prefix} {file}")
                    batch_msg = "\n".join(batch_msg)
                    if send_messages:
                        reply_thread_pool.submit(send_reply_delete, message, f"ğŸ“Š {batch_size}/{total_files} ({int(batch_size/total_files*100)}%) ä¸ªæ–‡ä»¶å·²å¤„ç†\n\n{batch_msg}")
                    message_batch = []
                time.sleep(1/get_int_env("ENV_FILE_PER_SECOND", 5))  # é¿å…é™æµ
            
            except Exception as e:
                results.append({
                    "success": False,
                    "file_name": file_path,
                    "error": str(e)
                })
                # è§£æè·¯å¾„ç»“æ„
                dir_path, file_name = os.path.split(file_path)
                msg = {
                    'status': 'âŒ',
                    'dir': dir_path,
                    'file': f"{file_name} ({str(e)})"
                }
                message_batch.append(msg)
                batch_size += 1
                logger.warning(f"{msg['status']}:{msg['dir']}/{msg['file']}")
                time.sleep(3)
        
        # å‘é€å‰©ä½™çš„æ¶ˆæ¯
        if message_batch:
            # ç”Ÿæˆæ ‘çŠ¶ç»“æ„æ¶ˆæ¯
            tree_messages = defaultdict(lambda: {'âœ…': [], 'âŒ': [], 'ğŸ”„': []})
            for entry in message_batch:
                tree_messages[entry['dir']][entry['status']].append(entry['file'])
            
            batch_msg = []
            for dir_path, status_files in tree_messages.items():
                for status, files in status_files.items():
                    if files:
                        batch_msg.append(f"--- {status} {dir_path}")
                        for i, file in enumerate(files):
                            prefix = '      â””â”€â”€' if i == len(files)-1 else '      â”œâ”€â”€'
                            batch_msg.append(f"{prefix} {file}")
            batch_msg = "\n".join(batch_msg)
            if send_messages:
                reply_thread_pool.submit(send_reply_delete, message, f"ğŸ“Š {batch_size}/{total_files} (100%) ä¸ªæ–‡ä»¶å·²å¤„ç†\n\n{batch_msg}")
        
        # è®¡ç®—è€—æ—¶
        end_time = time.time()
        elapsed_time = round(end_time - start_time, 2)
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results if r['success'])
        fail_count = len(results) - success_count
        
        # å‘é€æœ€ç»ˆç»“æœ
        result_msg = f"âœ… ç§’ä¼ é“¾æ¥è½¬å­˜å®Œæˆï¼\nâœ…æˆåŠŸ: {success_count}ä¸ª\nâŒå¤±è´¥: {fail_count}ä¸ª\nğŸ”„è·³è¿‡é‡å¤æ–‡ä»¶: {skip_count}ä¸ª\nğŸ“å…± {total_files} ä¸ªæ–‡ä»¶\n"
        # å°†å­—èŠ‚è½¬æ¢ä¸ºGBå¹¶ä¿ç•™ä¸¤ä½å°æ•°
        result_msg += f"ğŸ“ŠæˆåŠŸè½¬å­˜ä½“ç§¯: {total_size / (1024 ** 3):.2f}GB\nğŸ“Šå¹³å‡æ–‡ä»¶å¤§å°: {total_size / (1024 ** 3)/total_files:.2f}GB\nâ±ï¸è€—æ—¶: {elapsed_time} ç§’"
        result_msg_error = ""
        # æ·»åŠ å¤±è´¥æ–‡ä»¶å’Œå¤±è´¥åŸå› 
        if fail_count > 0:
            # æ”¶é›†æ‰€æœ‰å¤±è´¥çš„æ–‡ä»¶å’ŒåŸå› 
            fail_files = [(r['file_name'], r['error']) for r in results if not r['success']]
            
            # é™åˆ¶æ¶ˆæ¯é•¿åº¦ï¼ŒTelegramæ¶ˆæ¯æœ‰é•¿åº¦é™åˆ¶
            max_fail_files = 10  # æœ€å¤šæ˜¾ç¤º10ä¸ªå¤±è´¥æ–‡ä»¶
            show_fail_files = fail_files[:max_fail_files]
            
            result_msg_error = "âŒå¤±è´¥æ–‡ä»¶åˆ—è¡¨ï¼š"
            for file_name, error in show_fail_files:
                # æˆªæ–­è¿‡é•¿çš„æ–‡ä»¶åå’Œé”™è¯¯ä¿¡æ¯
                display_name = file_name
                display_error = error
                result_msg_error += f"\n- {display_name}: {display_error}"
            
            # å¦‚æœæœ‰æ›´å¤šå¤±è´¥æ–‡ä»¶ï¼Œæç¤ºç”¨æˆ·
            if len(fail_files) > max_fail_files:
                result_msg_error += f"\n\n... è¿˜æœ‰ {len(fail_files) - max_fail_files} ä¸ªå¤±è´¥æ–‡ä»¶æœªæ˜¾ç¤º ..."
        
        if send_messages:
            reply_thread_pool.submit(send_reply, message, result_msg)
            if result_msg_error:
                reply_thread_pool.submit(send_reply, message, result_msg_error)
                status = False
        else:
            if result_msg_error:
                reply_thread_pool.submit(send_message, result_msg_error)
                status = False
        #print(result_msg)
        
    except Exception as e:
        logger.error(f"å¤„ç†ç§’ä¼ é“¾æ¥å¼‚å¸¸: {str(e)}")
        if send_messages:
            reply_thread_pool.submit(send_reply, message, f"å¤„ç†ç§’ä¼ é“¾æ¥æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        else:
            reply_thread_pool.submit(send_message, f"å¤„ç†ç§’ä¼ é“¾æ¥æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        status = False
    
    return status

def extract_123_links_from_full_text(message_str):
    """
    æå–ç¬¦åˆæ¡ä»¶çš„123ç³»åˆ—ç§’ä¼ é“¾æ¥
    ç‰¹å¾ï¼šä»¥123FSLinkV1/2ã€123FLCPV1/2å¼€å¤´ï¼Œä»¥æ–‡æœ¬å½¢å¼\nï¼ˆå­—ç¬¦ä¸²"\\n"ï¼‰æˆ–ğŸ”ä¸ºç»“æŸæ ‡å¿—
          è‹¥æœªåŒ¹é…åˆ°ç»“æŸæ ‡å¿—ï¼Œåˆ™è‡ªåŠ¨åŒ¹é…åˆ°æ–‡æœ¬æœ«å°¾
    :param message_str: å®Œæ•´çš„åŸå§‹å­—ç¬¦ä¸²
    :return: åŒ¹é…åˆ°çš„é“¾æ¥åˆ—è¡¨ï¼ˆå»é‡å¹¶ä¿ç•™åŸå§‹é¡ºåºï¼‰
    """
    # æ„å»ºæ­£åˆ™ï¼š
    # 1. åŒ¹é…æŒ‡å®šå¼€å¤´ (123FSLinkV1/2 æˆ– 123FLCPV1/2)
    # 2. .*? éè´ªå©ªåŒ¹é…ä»»æ„å­—ç¬¦ï¼ˆåŒ…æ‹¬å®é™…æ¢è¡Œï¼Œå› å¯ç”¨DOTALLï¼‰
    # 3. (?=\\n|ğŸ”|$) æ­£å‘é¢„æŸ¥ï¼šåŒ¹é…åˆ°æ–‡æœ¬"\\n"ã€"ğŸ”"æˆ–æ–‡æœ¬æœ«å°¾æ—¶åœæ­¢ï¼ˆä¸åŒ…å«ç»“æŸæ ‡å¿—æœ¬èº«ï¼‰
    # æ³¨æ„ï¼šæ­£åˆ™ä¸­ç”¨\\nè¡¨ç¤ºæ–‡æœ¬ä¸­çš„"\n"ï¼ˆéœ€è½¬ä¹‰åæ–œæ ï¼‰
    link_pattern = re.compile(
        r'(123FSLinkV[12]|123FLCPV[12]).*?(?=\\n|\'}|\',|$)',
        re.DOTALL  # è®©.åŒ¹é…å®é™…æ¢è¡Œç¬¦ï¼ˆè‹¥æ–‡æœ¬ä¸­å­˜åœ¨ï¼‰
    )

    # æå–æ‰€æœ‰åŒ¹é…çš„é“¾æ¥
    matched_links = [match.group(0) for match in link_pattern.finditer(message_str)]
    
    # å»é‡å¹¶ä¿ç•™åŸå§‹é¡ºåº
    return list(dict.fromkeys(matched_links))

def extract_kuake_target_url(text):
    # åŒ¹é…æ ‡å‡†å¤¸å…‹é“¾æ¥ï¼ˆhttp/httpså¼€å¤´ï¼Œæå–æ ¸å¿ƒshare_idï¼‰
    link_pattern = r'https?://pan\.quark\.cn/s/([\w-]+)(?:[#?].*)?'
    # åŒ¹é…é“¾æ¥è‡ªå¸¦çš„pwdå‚æ•°
    pwd_in_link_pattern = r'[?&]pwd=(\w+)'
    # åŒ¹é…æ–‡æœ¬ä¸­çš„æå–ç ï¼ˆå…¼å®¹å¤šç§æ ¼å¼ï¼‰
    pwd_text_pattern = r'æå–ç [ï¼š:]?\s*(\w+)'

    # å…³é”®ä¼˜åŒ–1ï¼šç”¨é›†åˆè®°å½•å·²å¤„ç†çš„share_idï¼Œé¿å…é‡å¤æ·»åŠ åŒä¸€é“¾æ¥
    processed_share_ids = set()
    link_info_list = []
    
    for match in re.finditer(link_pattern, text, re.IGNORECASE):
        share_id = match.group(1)
        if not share_id or share_id in processed_share_ids:  # é‡å¤share_idç›´æ¥è·³è¿‡
            continue
        
        original_link = match.group(0)
        built_in_pwd = re.search(pwd_in_link_pattern, original_link).group(1) if re.search(pwd_in_link_pattern, original_link) else None
        
        link_info_list.append({"share_id": share_id.strip(), "built_in_pwd": built_in_pwd})
        processed_share_ids.add(share_id)  # æ ‡è®°ä¸ºå·²å¤„ç†

    # æå–æ–‡æœ¬æå–ç ï¼ˆå»é‡ä¿åºï¼‰
    passwords = list(dict.fromkeys(re.findall(pwd_text_pattern, text, re.IGNORECASE)))

    # ç”Ÿæˆæ ‡å‡†åŒ–é“¾æ¥
    processed_links = []
    for idx, info in enumerate(link_info_list):
        base_url = f"https://pan.quark.cn/s/{info['share_id']}"
        # å…³é”®ä¼˜åŒ–2ï¼šç¡®ä¿pwdåŒ¹é…é€»è¾‘ä¸é”™ä½ï¼ˆä¼˜å…ˆè‡ªå¸¦pwdï¼Œæ— åˆ™æŒ‰ç´¢å¼•å–æ–‡æœ¬pwdï¼‰
        final_pwd = info['built_in_pwd']
        if not final_pwd and idx < len(passwords):
            final_pwd = passwords[idx]
        
        final_url = f"{base_url}?pwd={final_pwd}" if final_pwd else base_url
        processed_links.append(final_url)

    # æœ€ç»ˆå»é‡ï¼ˆä¿åºï¼‰
    return list(dict.fromkeys(processed_links))

from quark_export_share import export_share_info
from share import TMDBHelper
tmdb = TMDBHelper()
# åˆ›å»ºé”å¯¹è±¡ç¡®ä¿æ–‡ä»¶ä¾æ¬¡è½¬å­˜
link_process_lock = threading.Lock()
@bot.message_handler(content_types=['text', 'photo'])
def handle_general_message(message):
    logger.info("è¿›å…¥handle_general_message")
    user_id = message.from_user.id
    if user_id != TG_ADMIN_USER_ID:
        reply_thread_pool.submit(send_reply, message, "æ‚¨æ²¡æœ‰æƒé™ä½¿ç”¨æ­¤æœºå™¨äººã€‚")
        return
    
    with link_process_lock:
        text = f"{message}"
        client = init_123_client()             
        # æ‰§è¡ŒåŒ¹é…
        full_links = extract_123_links_from_full_text(text)
        if full_links:
            for link in full_links:
                parse_share_link(message, link)
            user_state_manager.clear_state(user_id)
            return
        # è°ƒç”¨å‡½æ•°å¹¶è·å–è¿”å›å€¼
        result = add_magnet_links(client,text,get_int_env("ENV_123_MAGNET_UPLOAD_PID", 0),message)

        # æ ¹æ®è¿”å›å€¼çŠ¶æ€æ‰§è¡Œä¸åŒçš„print
        if result['status'] == 'success':
            success_count = 0
            fail_count = 0
            fail_messages = []
            
            # æ£€æŸ¥æ¯ä¸ªé“¾æ¥çš„æ·»åŠ ç»“æœ
            for item in result['data']:
                link = item['link']
                response = item['response']
                if isinstance(response, dict) and response.get('code') == 0:
                    success_count += 1
                else:
                    fail_count += 1
                    # æˆªå–é“¾æ¥çš„å‰40ä¸ªå­—ç¬¦ä½œä¸ºæ ‡è¯†
                    link_identifier = link
                    msg = f"\n{link_identifier}: {response.get('message', 'æœªçŸ¥é”™è¯¯')}" if isinstance(response, dict) else f"{link_identifier}: {str(response)}"
                    fail_messages.append(msg)
            
            # æ‰“å°ç»“æœ
            logger.info(f"123ç£åŠ›é“¾æ¥æ·»åŠ ç»“æœ: æˆåŠŸ{success_count}ä¸ª, å¤±è´¥{fail_count}ä¸ª")
            if fail_count > 0:
                logger.error(f"å¤±è´¥è¯¦æƒ…:{', '.join(fail_messages)}")
                reply_thread_pool.submit(send_reply, message, f"123ç£åŠ›é“¾æ¥æ·»åŠ éƒ¨åˆ†å¤±è´¥: æˆåŠŸ{success_count}ä¸ª, å¤±è´¥{fail_count}ä¸ª\nå¤±è´¥è¯¦æƒ…: {', '.join(fail_messages)}")
            else:
                reply_thread_pool.submit(send_reply, message, f"123ç£åŠ›é“¾æ¥æ·»åŠ æˆåŠŸ: å…±æ·»åŠ äº†{success_count}ä¸ªé“¾æ¥")
            user_state_manager.clear_state(user_id)
            return
        else:
            if result['message'] == 'æœªæ‰¾åˆ°ç£åŠ›é“¾æ¥':
                #logger.info("æœªæ‰¾åˆ°ä»»ä½•ç£åŠ›é“¾æ¥")
                None
            else:
                logger.error(f"123ç£åŠ›é“¾æ¥æ·»åŠ å¤±è´¥: {result['message']}")
                reply_thread_pool.submit(send_reply_delete, message, f"123ç£åŠ›é“¾æ¥æ·»åŠ å¤±è´¥: {result['message']}")
                user_state_manager.clear_state(user_id)
                return
        if "æå–ç " in text and "www.123" in text:
            reply_thread_pool.submit(send_reply, message, f"ä»…æ”¯æŒå½¢å¦‚ https://www.123pan.com/s/abcde-fghi?pwd=ABCD çš„æå–ç æ ¼å¼")
            return
        target_urls = extract_target_url(text)
        if target_urls:
            reply_thread_pool.submit(send_reply_delete, message, f"å‘ç°{len(target_urls)}ä¸ª123åˆ†äº«é“¾æ¥ï¼Œå¼€å§‹è½¬å­˜...")
            success_count = 0
            fail_count = 0
            for url in target_urls:
                try:
                    result = transfer_shared_link_optimize(client, url, UPLOAD_LINK_TARGET_PID)
                    if result:
                        success_count += 1
                        logger.info(f"è½¬å­˜æˆåŠŸ: {url}")
                    else:
                        fail_count += 1
                        logger.error(f"è½¬å­˜å¤±è´¥: {url}")
                except Exception as e:
                    fail_count += 1
                    logger.error(f"è½¬å­˜å¼‚å¸¸: {url}, é”™è¯¯: {str(e)}")
                    
            #time.sleep(3)
            reply_thread_pool.submit(send_reply, message, f"è½¬å­˜å®Œæˆï¼šæˆåŠŸ{success_count}ä¸ªï¼Œå¤±è´¥{fail_count}ä¸ª")
            user_state_manager.clear_state(user_id)
            return
        
        target_urls = extract_kuake_target_url(text)
        if target_urls:
            if not os.getenv("ENV_KUAKE_COOKIE", ""):
                logger.error(f"è¯·å¡«å†™å¤¸å…‹COOKIE")
                reply_thread_pool.submit(send_reply, message, f"è¯·å¡«å†™å¤¸å…‹COOKIE")
                return
            reply_thread_pool.submit(send_reply, message, f"å‘ç°{len(target_urls)}ä¸ªå¤¸å…‹åˆ†äº«é“¾æ¥ï¼Œå¼€å§‹å°è¯•ç§’ä¼ åˆ°123...")
            success_count = 0   
            fail_count = 0
            for url in target_urls:
                try:
                    json_data = export_share_info(url,os.getenv("ENV_KUAKE_COOKIE", ""))
                    if json_data:
                        save_json_file_quark(message,json_data)
                        #parse_share_link(message, kuake_link, get_int_env("ENV_123_KUAKE_UPLOAD_PID", 0))                
                    else:
                        logger.error(f"å¤¸å…‹åˆ†äº«è½¬å­˜123å‡ºé”™")
                        reply_thread_pool.submit(send_reply, message, f"å¤¸å…‹åˆ†äº«è½¬å­˜123å‡ºé”™")
                except Exception as e:
                    fail_count += 1
                    logger.error(f"è½¬å­˜å¼‚å¸¸: {url}, é”™è¯¯: {str(e)}")
            #time.sleep(3)
            #reply_thread_pool.submit(send_reply, message, f"è½¬å­˜å®Œæˆï¼šæˆåŠŸ{success_count}ä¸ªï¼Œå¤±è´¥{fail_count}ä¸ª")
            user_state_manager.clear_state(user_id)
            return

        from bot189 import extract_target_url as  extract_target_url_189
        from bot189 import save_189_link
        target_urls = extract_target_url_189(text)
        if target_urls:
            reply_thread_pool.submit(send_reply_delete, message, f"å‘ç°{len(target_urls)}ä¸ªå¤©ç¿¼äº‘ç›˜åˆ†äº«é“¾æ¥ï¼Œå¼€å§‹è½¬å­˜...")
            success_count = 0
            fail_count = 0
            for url in target_urls:
                try:                    
                    result = save_189_link(client189, url, os.getenv("ENV_189_LINK_UPLOAD_PID","-11"))
                    if result:
                        success_count += 1
                        logger.info(f"è½¬å­˜æˆåŠŸ: {url}")
                    else:
                        fail_count += 1
                        logger.error(f"è½¬å­˜å¤±è´¥: {url}")
                except Exception as e:
                    fail_count += 1
                    logger.error(f"è½¬å­˜å¼‚å¸¸: {url}, é”™è¯¯: {str(e)}")
            #time.sleep(3)
            reply_thread_pool.submit(send_reply, message, f"è½¬å­˜å®Œæˆï¼šæˆåŠŸ{success_count}ä¸ªï¼Œå¤±è´¥{fail_count}ä¸ª")
            user_state_manager.clear_state(user_id)
            return
        from bot115 import extract_target_url as  extract_target_url_115
        from bot115 import transfer_shared_link as  transfer_shared_link_115
        from bot115 import init_115_client
        target_urls = extract_target_url_115(text)
        if target_urls:
            reply_thread_pool.submit(send_reply_delete, message, f"å‘ç°{len(target_urls)}ä¸ª115åˆ†äº«é“¾æ¥ï¼Œå¼€å§‹è½¬å­˜...")
            client = init_115_client()
            success_count = 0
            fail_count = 0
            for url in target_urls:
                try:
                    result = transfer_shared_link_115(client, url, os.getenv("ENV_115_LINK_UPLOAD_PID","0"))
                    if result:
                        success_count += 1
                        logger.info(f"è½¬å­˜æˆåŠŸ: {url}")
                    else:
                        fail_count += 1
                        logger.error(f"è½¬å­˜å¤±è´¥: {url}")
                except Exception as e:
                    fail_count += 1
                    logger.error(f"è½¬å­˜å¼‚å¸¸: {url}, é”™è¯¯: {str(e)}")
            reply_thread_pool.submit(send_reply, message, f"è½¬å­˜å®Œæˆï¼šæˆåŠŸ{success_count}ä¸ªï¼Œå¤±è´¥{fail_count}ä¸ª")
            user_state_manager.clear_state(user_id)
            return
        if message.content_type == 'photo':
            user_state_manager.clear_state(user_id)
            reply_thread_pool.submit(send_reply, message, f"è¯¥æ¡æ¶ˆæ¯æœªæ‰¾åˆ°åˆ†äº«é“¾æ¥ã€ç§’ä¼ é“¾æ¥ã€ç§’ä¼ JSONã€ç£åŠ›é“¾ç­‰æœ‰æ•ˆå†…å®¹")
            return
    
    state, data = user_state_manager.get_state(user_id)
    if state == "SELECTING_FILE":
        try:
            raw_text = message.text.strip()
            text = raw_text.replace('ã€€', ' ').strip()
            full_width = 'ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™'
            half_width = '0123456789'
            trans_table = str.maketrans(full_width, half_width)
            text = text.translate(trans_table)
            try:
                # æ”¯æŒç©ºæ ¼åˆ†éš”çš„å¤šä¸ªæ•°å­—ï¼Œå¦‚ "1 2 3 5"
                selections = [int(num) - 1 for num in text.split()]
                if not selections:
                    raise ValueError("è¯·è‡³å°‘è¾“å…¥ä¸€ä¸ªæœ‰æ•ˆçš„åºå·")
                # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤çš„åºå·
                if len(selections) != len(set(selections)):
                    raise ValueError("åºå·ä¸èƒ½é‡å¤")
            except ValueError as e:
                if "invalid literal" in str(e):
                    raise ValueError("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—åºå·ï¼ˆä¾‹å¦‚ï¼š1 2 3 4ï¼‰ï¼Œä¸è¦åŒ…å«å­—æ¯æˆ–ç¬¦å·")
                else:
                    raise e

            results = json.loads(data)
            if not results:
                reply_thread_pool.submit(send_reply, message, "æœç´¢ç»“æœå·²å¤±æ•ˆï¼Œè¯·é‡æ–°æœç´¢")
                user_state_manager.clear_state(user_id)
                return
            
            # éªŒè¯æ‰€æœ‰é€‰æ‹©æ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
            for idx in selections:
                if not (0 <= idx < len(results)):
                    raise ValueError(f"åºå· {idx+1} è¶…å‡ºèŒƒå›´ï¼Œè¯·é‡æ–°è¾“å…¥")
            
            # åˆå§‹åŒ–å®¢æˆ·ç«¯ï¼ˆåªéœ€åˆå§‹åŒ–ä¸€æ¬¡ï¼‰
            client = init_123_client()
            
            # éå†æ‰€æœ‰é€‰æ‹©çš„æ–‡ä»¶å¤¹
            for selection in selections:
                selected_item = results[selection]
                file_id = selected_item['id']
                folder_name = selected_item['name']
                logger.info(f"é€‰ä¸­æ–‡ä»¶å¤¹ID: {file_id}, åç§°: {folder_name}")
                # åªä¸ºç¬¬ä¸€ä¸ªæ–‡ä»¶å¤¹å‘é€åˆ›å»ºåˆ†äº«é“¾æ¥çš„æ¶ˆæ¯ï¼Œé¿å…é‡å¤
                if selection == selections[0]:
                    reply_thread_pool.submit(send_reply, message, f"æ­£åœ¨ä¸º {len(selections)} ä¸ªæ–‡ä»¶å¤¹åˆ›å»ºåˆ†äº«é“¾æ¥...")
                if get_int_env("ENV_MAKE_NEW_LINK", 1):
                    existing_share = get_existing_shares(client, folder_name)
                else:
                    existing_share = None
                if existing_share:
                    # å°è¯•è·å–TMDBå…ƒæ•°æ®
                    file_name=get_first_video_file(client, file_id)
                    metadata = tmdb.get_metadata_optimize(folder_name, file_name)
                    share_data = {
                        "share_url": f"{existing_share['url']}{'?pwd=' + existing_share['password'] if existing_share['password'] else ''}",
                        "folder_name": folder_name,
                        "file_id": file_id  # é€‰ä¸­çš„æ–‡ä»¶å¤¹IDï¼Œç”¨äºåç»­æŸ¥è¯¢æ–‡ä»¶
                    }

                    if not metadata:
                        logger.warning(f"æœªè·å–åˆ°TMDBå…ƒæ•°æ®: {folder_name}/{file_name}")
                        reply_thread_pool.submit(send_message_with_id, message.chat.id, f"æœªè·å–åˆ°TMDBå…ƒæ•°æ®ï¼Œä¸äºˆåˆ†äº«ï¼Œè¯·è§„èŒƒæ–‡ä»¶å¤¹å: {folder_name}/{file_name}")
                        user_state_manager.clear_state(user_id)
                        return

                    # ä»…å½“metadataå­˜åœ¨ä¸”titleåœ¨folder_nameä¸­æ—¶æ‰æ‰§è¡Œ
                    if metadata:
                        # ä½¿ç”¨å°è£…å‡½æ•°æ„å»ºæ¶ˆæ¯
                        share_message, share_message2, poster_url, files = build_share_message(metadata, client, file_id, folder_name, file_name, existing_share)

                        # å‘é€å›¾ç‰‡å’Œæ¶ˆæ¯
                        try:
                            bot.send_photo(message.chat.id, poster_url, caption=share_message, parse_mode='HTML')
                            if TOKENSHARE:
                                botshare.send_photo(TARGET_CHAT_ID_SHARE, poster_url, caption=share_message, parse_mode='HTML')
                        except Exception as e:
                            logger.error(f"å‘é€å›¾ç‰‡å¤±è´¥: {str(e)}")
                            reply_thread_pool.submit(send_message_with_id, message.chat.id, share_message)
                    else:
                        files = get_directory_files(client, file_id, folder_name)
                        # ä½¿ç”¨åŸæ¥çš„æ¶ˆæ¯æ ¼å¼
                        share_message = f"âœ… å·²å­˜åœ¨åˆ†äº«é“¾æ¥ï¼š\n{folder_name}\n"
                        share_message += f"é“¾æ¥ï¼š{existing_share['url']}{'?pwd=' + existing_share['password'] if existing_share['password'] else ''}\n"
                        if existing_share['password']:
                            share_message += f"æå–ç ï¼š{existing_share['password']}\n"
                        share_message += f"è¿‡æœŸæ—¶é—´ï¼š{existing_share['expiry']}"
                        reply_thread_pool.submit(send_message_with_id, message.chat.id, share_message)

                    if AUTO_MAKE_JSON:
                        try:
                            # è·å–æ–‡ä»¶å¤¹å†…æ–‡ä»¶åˆ—è¡¨
                            #files = get_directory_files(client, file_id, folder_name)
                            if not files:
                                logger.warning(f"æ–‡ä»¶å¤¹ä¸ºç©º: {folder_name}")
                            else:
                                # åˆ›å»ºJSONç»“æ„
                                # è®¡ç®—æ€»æ–‡ä»¶æ•°å’Œæ€»ä½“ç§¯
                                total_files_count = len(files)
                                total_size = sum(file_info["size"] for file_info in files)

                                json_data = {
                                    "commonPath": f"{folder_name}/",
                                    "usesBase62EtagsInExport": False,
                                    "totalFilesCount": total_files_count,
                                    "totalSize": total_size,
                                    "files": [
                                        {
                                            "path": file_info["path"],
                                            "etag": file_info["etag"],
                                            "size": file_info["size"]
                                        }
                                        for file_info in files
                                    ]
                                }
                                # ä¿å­˜JSONæ–‡ä»¶
                                json_file_path = f"{folder_name}.json"
                                with open(json_file_path, 'w', encoding='utf-8') as f:
                                    json.dump(json_data, f, ensure_ascii=False, indent=2)
                                # å‘é€JSONæ–‡ä»¶
                                # å°†å­—èŠ‚è½¬æ¢ä¸ºGB (1GB = 1024^3 B)
                                total_size_gb = total_size / (1024 ** 3)
                                size_str = f"{total_size_gb:.2f}GB"
                                with open(json_file_path, 'rb') as f:
                                    # è®¡ç®—å¹³å‡æ–‡ä»¶å¤§å°
                                    avg_size = total_size / total_files_count if total_files_count > 0 else 0
                                    avg_size_gb = avg_size / (1024 ** 3)
                                    avg_size_str = f"{avg_size_gb:.2f}GB" if avg_size_gb >= 0.01 else f"{avg_size / (1024 ** 2):.2f}MB"
                                    if metadata:
                                        bot.send_document(message.chat.id, f, caption=share_message2, parse_mode='HTML')
                                        if TOKENSHARE:
                                            f.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆåˆ°å¼€å¤´
                                            botshare.send_document(TARGET_CHAT_ID_SHARE, f, caption=share_message2, parse_mode='HTML')
                                    else:
                                        bot.send_document(message.chat.id, f, caption=f"ğŸ“ {folder_name}\nğŸ“æ–‡ä»¶æ•°: {total_files_count}ä¸ª\nğŸ“¦æ€»ä½“ç§¯: {size_str}\nğŸ“Šå¹³å‡æ–‡ä»¶å¤§å°: {avg_size_str}")
                                # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                                os.remove(json_file_path)
                        except Exception as e:
                            logger.error(f"ç”Ÿæˆæˆ–å‘é€JSONæ–‡ä»¶å¤±è´¥: {str(e)}")
                            reply_thread_pool.submit(send_message_with_id, message.chat.id, f"ç”Ÿæˆæ–‡ä»¶åˆ—è¡¨å¤±è´¥ï¼Œè¯·é‡è¯•")
                    
                    
                    if os.getenv("ENV_123PANFX_COOKIE","") and len(selections)==1:
                        user_state_manager.set_state(user_id, "ASK_POST", json.dumps(share_data))
                        ask_msg = "æ˜¯å¦éœ€è¦å°†è¯¥å†…å®¹å‘å¸ƒåˆ°è®ºå›ï¼Ÿ\n1. æ”¾å¼ƒå‘å¸–\n2. å‘é€åˆ°ç”µå½±æ¿å—\n3. å‘é€åˆ°ç”µè§†å‰§æ¿å—\n4. å‘é€åˆ°åŠ¨æ¼«æ¿å—"
                        reply_thread_pool.submit(send_message_with_id, message.chat.id, ask_msg)
                    #else:
                        #bot.send_message(message.chat.id, "tgto123ï¼šå¦‚éœ€è‡ªåŠ¨å‘è´´åŠŸèƒ½ï¼Œè¯·é…ç½®123panfx.comçš„Cookie")
                    #user_state_manager.clear_state(user_id)
                    #return
                else:
                    # å°è¯•è·å–TMDBå…ƒæ•°æ®
                    file_name = get_first_video_file(client,file_id)
                    metadata = tmdb.get_metadata_optimize(folder_name, file_name)
                    porn_result = None

                    if not metadata:
                        logger.warning(f"æœªè·å–åˆ°TMDBå…ƒæ•°æ®: {folder_name}/{file_name}")
                        reply_thread_pool.submit(send_message_with_id, message.chat.id, f"æœªè·å–åˆ°TMDBå…ƒæ•°æ®ï¼Œä¸äºˆåˆ†äº«ï¼Œè¯·è§„èŒƒæ–‡ä»¶å¤¹å: {folder_name}/{file_name}")
                        user_state_manager.clear_state(user_id)
                        return
                    # æ£€æŸ¥å†…å®¹æ˜¯å¦æ¶‰åŠè‰²æƒ…
                    if os.getenv("AI_API_KEY", ""):
                        porn_result = check_porn_content(folder_name+"/"+file_name+"ï¼š"+metadata.get('plot'))
                    else:
                        porn_result = check_porn_content(
                                        content=folder_name+"/"+file_name+"ï¼š"+metadata.get('plot'),
                                        api_url="https://api.edgefn.net",
                                        api_key="sk-Mk6CjIVzoCcg2VnK8c5a85Ef49Ca43F1Ba9b9a13E98f30A9",
                                        model_name="DeepSeek-R1-0528-Qwen3-8B",
                                        max_tokens=15000
                                    )
                    
                    # æ ¹æ®æ£€æµ‹ç»“æœå†³å®šåç»­æ“ä½œ
                    if porn_result and porn_result['is_pornographic']:
                        logger.warning(f"æ£€æµ‹åˆ°è‰²æƒ…å†…å®¹ï¼Œå·²æ‹’ç»åˆ†äº«: {folder_name}")
                        reply_thread_pool.submit(send_message_with_id, message.chat.id, f"å½±è§†ä»‹ç»ä¸­æ£€æµ‹åˆ°æ¶‰åŠè‰²æƒ…å†…å®¹ï¼Œæ‹’ç»åˆ†äº«ï¼Œåˆ¤æ–­ä¾æ®ï¼š{porn_result['reason']}")
                        user_state_manager.clear_state(user_id)
                        return
                    
                    # éè‰²æƒ…å†…å®¹ï¼Œç»§ç»­åˆ›å»ºåˆ†äº«é“¾æ¥
                    share_info = create_share_link(client, file_id)
                    share_data = {
                        "share_url": share_info["url"],
                        "folder_name": folder_name,
                        "file_id": file_id  # é€‰ä¸­çš„æ–‡ä»¶å¤¹IDï¼Œç”¨äºåç»­æŸ¥è¯¢æ–‡ä»¶
                    }

                    # ä»…å½“metadataå­˜åœ¨ä¸”titleåœ¨folder_nameä¸­æ—¶æ‰æ‰§è¡Œ
                    if metadata:
                        # ä½¿ç”¨å°è£…å‡½æ•°æ„å»ºæ¶ˆæ¯
                        share_message, share_message2, poster_url, files = build_share_message(metadata, client, file_id, folder_name, file_name, share_info)

                        # å‘é€å›¾ç‰‡å’Œæ¶ˆæ¯
                        try:
                            bot.send_photo(message.chat.id, poster_url, caption=share_message, parse_mode='HTML')
                            if TOKENSHARE:
                                botshare.send_photo(TARGET_CHAT_ID_SHARE, poster_url, caption=share_message, parse_mode='HTML')
                        except Exception as e:
                            logger.error(f"å‘é€å›¾ç‰‡å¤±è´¥: {str(e)}")
                            reply_thread_pool.submit(send_message_with_id, message.chat.id, share_message)
                    else:
                        files = get_directory_files(client, file_id, folder_name)
                        # ä½¿ç”¨åŸæ¥çš„æ¶ˆæ¯æ ¼å¼
                        share_message = f"âœ… åˆ†äº«é“¾æ¥å·²åˆ›å»ºï¼š\n{folder_name}\n"
                        share_message += f"é“¾æ¥ï¼š{share_info['url']}\n"
                        if share_info['password']:
                            share_message += f"æå–ç ï¼š{share_info['password']}\n"
                        share_message += f"è¿‡æœŸæ—¶é—´ï¼š{share_info['expiry']}"
                        reply_thread_pool.submit(send_message_with_id, message.chat.id, share_message)
                    if AUTO_MAKE_JSON:
                        # ç”ŸæˆJSONæ–‡ä»¶
                        try:
                            # è·å–æ–‡ä»¶å¤¹å†…æ–‡ä»¶åˆ—è¡¨
                            #files = get_directory_files(client, file_id, folder_name)
                            if not files:
                                logger.warning(f"æ–‡ä»¶å¤¹ä¸ºç©º: {folder_name}")
                            else:                                                               # è®¡ç®—æ€»æ–‡ä»¶æ•°å’Œæ€»ä½“ç§¯
                                total_files_count = len(files)
                                total_size = sum(file_info["size"] for file_info in files)
                                # åˆ›å»ºJSONç»“æ„
                                json_data = {
                                    "commonPath": f"{folder_name}/",
                                    "usesBase62EtagsInExport": False,
                                    "totalFilesCount": total_files_count,
                                    "totalSize": total_size,
                                    "files": [
                                        {
                                            "path": file_info["path"],
                                            "etag": file_info["etag"],
                                            "size": file_info["size"]
                                        }
                                        for file_info in files
                                    ]
                                }
                                # ä¿å­˜JSONæ–‡ä»¶
                                json_file_path = f"{folder_name}.json"
                                with open(json_file_path, 'w', encoding='utf-8') as f:
                                    json.dump(json_data, f, ensure_ascii=False, indent=2)
                                # å‘é€JSONæ–‡ä»¶

                                # å°†å­—èŠ‚è½¬æ¢ä¸ºGBå¹¶ä¿ç•™ä¸¤ä½å°æ•°
                                total_size_gb = total_size / (1024 ** 3)
                                size_str = f"{total_size_gb:.2f}GB"
                                
                                with open(json_file_path, 'rb') as f:
                                    # è®¡ç®—å¹³å‡æ–‡ä»¶å¤§å°
                                    avg_size = total_size / total_files_count if total_files_count > 0 else 0
                                    avg_size_gb = avg_size / (1024 ** 3)
                                    avg_size_str = f"{avg_size_gb:.2f}GB" if avg_size_gb >= 0.01 else f"{avg_size / (1024 ** 2):.2f}MB"
                                    if metadata:
                                        bot.send_document(message.chat.id, f, caption=share_message2, parse_mode='HTML')
                                        if TOKENSHARE:
                                            f.seek(0)
                                            botshare.send_document(TARGET_CHAT_ID_SHARE, f, caption=share_message2, parse_mode='HTML')
                                    else:
                                        bot.send_document(message.chat.id, f, caption=f"ğŸ“ {folder_name}\nğŸ“æ–‡ä»¶æ•°: {total_files_count}ä¸ª\nğŸ“¦æ€»ä½“ç§¯: {size_str}\nğŸ“Šå¹³å‡æ–‡ä»¶å¤§å°: {avg_size_str}")
                                # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                                os.remove(json_file_path)
                        except Exception as e:
                            logger.error(f"ç”Ÿæˆæˆ–å‘é€JSONæ–‡ä»¶å¤±è´¥: {str(e)}")
                            reply_thread_pool.submit(send_message_with_id, message.chat.id, f"ç”Ÿæˆæ–‡ä»¶åˆ—è¡¨å¤±è´¥ï¼Œè¯·é‡è¯•")
                    if os.getenv("ENV_123PANFX_COOKIE","") and len(selections)==1:
                        user_state_manager.set_state(user_id, "ASK_POST", json.dumps(share_data))
                        ask_msg = "æ˜¯å¦éœ€è¦å°†è¯¥å†…å®¹å‘å¸ƒåˆ°è®ºå›ï¼Ÿ\n1. æ”¾å¼ƒå‘å¸–\n2. å‘é€åˆ°ç”µå½±æ¿å—\n3. å‘é€åˆ°ç”µè§†å‰§æ¿å—\n4. å‘é€åˆ°åŠ¨æ¼«æ¿å—"
                        reply_thread_pool.submit(send_message_with_id, message.chat.id, ask_msg)
                    #else:
                        #bot.send_message(message.chat.id, "tgto123ï¼šå¦‚éœ€è‡ªåŠ¨å‘è´´åŠŸèƒ½ï¼Œè¯·é…ç½®123panfx.comçš„Cookie")
                    #user_state_manager.clear_state(user_id)

            #else:
                #raise ValueError(f"åºå·è¶…å‡ºèŒƒå›´ï¼Œè¯·è¾“å…¥ 1-{len(results)} ä¹‹é—´çš„æ•°å­—")
        except ValueError as e:
            reply_thread_pool.submit(send_reply, message, str(e))
        except Exception as e:
            reply_thread_pool.submit(send_reply, message, f"åˆ›å»ºåˆ†äº«é“¾æ¥å¤±è´¥: è¯·æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦ä¸ºç©ºï¼Œ{str(e)}")
            logger.error(f"åˆ›å»ºåˆ†äº«é“¾æ¥å¤±è´¥: {str(e)}")
    elif state == "ASK_POST":
        try:
            selection = message.text.strip()
            if selection not in ["1", "2", "3", "4"]:
                raise ValueError("è¯·è¾“å…¥1ã€2ã€3æˆ–4é€‰æ‹©æ“ä½œ")
            #global json
            # è§£æä¿å­˜çš„åˆ†äº«æ•°æ®
            share_data = json.loads(data)
            share_url = share_data["share_url"]
            folder_name = share_data["folder_name"]
            file_id = share_data["file_id"]

            if selection == "1":
                # æ”¾å¼ƒå‘å¸–
                reply_thread_pool.submit(send_reply, message, "å·²å–æ¶ˆå‘å¸–")
                user_state_manager.clear_state(user_id)
            else:
                # ç¡®å®šåª’ä½“ç±»å‹ï¼ˆ2=ç”µå½±ï¼Œ3=ç”µè§†å‰§ï¼‰
                # æ ¹æ®é€‰æ‹©ç¡®å®šåª’ä½“ç±»å‹ï¼š2->ç”µå½±ï¼Œ3->åŠ¨ç”»ï¼Œå…¶ä»–->ç”µè§†å‰§
                if selection == "2":
                    media_type = "movie"  # é€‰æ‹©2ï¼šç”µå½±
                elif selection == "3":
                    media_type = "tv"  # é€‰æ‹©3ï¼šç”µè§†å‰§
                elif selection == "4":
                    media_type = "anime"  # é€‰æ‹©4ï¼šåŠ¨æ¼«
                else:
                    media_type = None  # é€‰æ‹©1ï¼šæ”¾å¼ƒï¼ˆæ— éœ€å¤„ç†ï¼‰

                # è·å–ç¬¬ä¸€ä¸ªè§†é¢‘æ–‡ä»¶åç§°
                reply_thread_pool.submit(send_reply, message, "æ­£åœ¨æŸ¥æ‰¾è§†é¢‘æ–‡ä»¶ä»¥ç¡®å®šå½±è§†çš„åˆ†è¾¨ç‡åŠéŸ³é¢‘ç­‰ä¿¡æ¯...")
                client = init_123_client()
                file_name = get_first_video_file(client, file_id)
                if not file_name:
                    reply_thread_pool.submit(send_reply, message, "æœªæ‰¾åˆ°è§†é¢‘æ–‡ä»¶ï¼Œæ— æ³•å‘å¸–")
                    user_state_manager.clear_state(user_id)
                    return

                # è°ƒç”¨share.pyä¸­çš„post_to_forumå‘å¸ƒ
                from share import post_to_forum
                reply_thread_pool.submit(send_reply, message, "æ­£åœ¨å‘å¸ƒåˆ°è®ºå›...")
                success, forum_url = post_to_forum(
                    share_url=share_url,
                    folder_name=folder_name,
                    file_name=file_name,
                    media_type=media_type
                )

                # åé¦ˆç»“æœ
                if success:
                    reply_thread_pool.submit(send_reply, message, f"å‘å¸–æˆåŠŸï¼\n{folder_name}\nç¤¾åŒºé“¾æ¥ï¼š{forum_url}\n123èµ„æºç¤¾åŒºå› æ‚¨çš„åˆ†äº«è€Œæ›´ç¾å¥½â¤ï¸")
                else:
                    reply_thread_pool.submit(send_reply, message, f"å‘å¸–å¤±è´¥ï¼Œ{forum_url}, è¯·é‡è¯•")
                user_state_manager.clear_state(user_id)

        except ValueError as e:
            reply_thread_pool.submit(send_reply, message, str(e))
        except Exception as e:
            reply_thread_pool.submit(send_reply, message, f"æ“ä½œå¤±è´¥: {str(e)}")
            logger.error(f"å¤„ç†å‘å¸–é€‰æ‹©é”™è¯¯: {e}")
    else:
        reply_thread_pool.submit(send_reply, message, "æœªè¯†åˆ«çš„å‘½ä»¤")



# æ–°å¢å‡½æ•°ï¼šæŸ¥è¯¢å·²å­˜åœ¨çš„æœªå¤±æ•ˆåˆ†äº«é“¾æ¥
def get_existing_shares(client: P123Client, folder_name: str) -> dict:
    """æŸ¥è¯¢å·²å­˜åœ¨çš„æœªå¤±æ•ˆåˆ†äº«é“¾æ¥"""
    shares = []
    last_share_id = 0
    try:
        while True:
            # è°ƒç”¨åˆ†äº«åˆ—è¡¨API
            response = requests.get(
                f"https://open-api.123pan.com/api/v1/share/list?limit=100&lastShareId={last_share_id}",
                headers={
                    'Authorization': f'Bearer {client.token}',
                    'Platform': 'open_platform'
                },
                timeout=TIMEOUT
            )
            data = response.json()

            if data.get('code') != 0:
                logger.error(f"è·å–åˆ†äº«åˆ—è¡¨å¤±è´¥: {data.get('message')}")
                break

            # æå–å½“å‰é¡µåˆ†äº«æ•°æ®
            share_list = data.get('data', {}).get('shareList', [])
            shares.extend(share_list)

            # å¤„ç†åˆ†é¡µ
            last_share_id = data.get('data', {}).get('lastShareId', -1)
            if last_share_id == -1:
                break  # å·²åˆ°æœ€åä¸€é¡µ

        # ç­›é€‰å‡ºåç§°åŒ¹é…ä¸”æœªå¤±æ•ˆçš„åˆ†äº«
        for share in shares:
            if (share.get('shareName') == folder_name and
                    share.get('expired') == 0 and  # expired=0è¡¨ç¤ºæœªå¤±æ•ˆ
                    share.get('expiration', '') > '2050-06-30 00:00:00'):  # è¿‡æœŸæ—¶é—´å¤§äº2050-06-30 00:00:00
                return {
                    "url": f"https://www.123pan.com/s/{share.get('shareKey')}",
                    "password": share.get('sharePwd'),
                    "expiry": "æ°¸ä¹…æœ‰æ•ˆ"
                }

        # æœªæ‰¾åˆ°åŒ¹é…çš„æœ‰æ•ˆåˆ†äº«
        return None

    except Exception as e:
        logger.error(f"æŸ¥è¯¢å·²å­˜åœ¨åˆ†äº«å¤±è´¥: {str(e)}")
        return None


@bot.message_handler(content_types=['document'], func=lambda message: message.document.mime_type == 'application/json' or message.document.file_name.endswith('.json'))
def process_json_file(message):
    with link_process_lock:  # è·å–é”ï¼Œç¡®ä¿å¤šä¸ªè¯·æ±‚ä¾æ¬¡å¤„ç†
        user_id = message.from_user.id
        if user_id != TG_ADMIN_USER_ID:
            # ä½¿ç”¨çº¿ç¨‹æ± å‘é€å›å¤
            reply_thread_pool.submit(send_reply, message, "æ‚¨æ²¡æœ‰æƒé™ä½¿ç”¨æ­¤åŠŸèƒ½ã€‚")
            return
        logger.info("è¿›å…¥è½¬å­˜json\n")
        try:
            # å¼€å§‹è®¡æ—¶
            start_time = time.time()
            
            file_retry_count = 0
            # è·å–æ–‡ä»¶ID
            while file_retry_count < 10:
                try:
                    file_id = message.document.file_id
                    file_info = bot.get_file(file_id)
                    file_path = file_info.file_path
                    break
                except Exception as e:
                    logger.error(f"ä»TGè·å–æ–‡ä»¶å¤±è´¥ï¼Œå°è¯•é‡è¯•: {str(e)}")
                    file_retry_count += 1
                    time.sleep(30)

            # ä¸‹è½½JSONæ–‡ä»¶
            json_url = f'https://api.telegram.org/file/bot{TG_BOT_TOKEN}/{file_path}'
            response = requests.get(json_url)
            json_data = response.json()

            # æå–commonPathã€filesã€totalFilesCountå’ŒtotalSize
            common_path = json_data.get('commonPath', '').strip()
            if common_path.endswith('/'):
                common_path = common_path[:-1]
            files = json_data.get('files', [])
            uses_v2_etag = json_data.get('usesBase62EtagsInExport', False)
            total_files_count = json_data.get('totalFilesCount', len(files))
            total_size_json = json_data.get('totalSize', 0)

            if not files:
                # ä½¿ç”¨çº¿ç¨‹æ± å‘é€å›å¤
                reply_thread_pool.submit(send_reply, message, "JSONæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶ä¿¡æ¯ã€‚")
                return

            # ä½¿ç”¨çº¿ç¨‹æ± å‘é€å›å¤
            reply_thread_pool.submit(send_reply_delete, message, f"å¼€å§‹è½¬å­˜JSONæ–‡ä»¶ä¸­çš„{len(files)}ä¸ªæ–‡ä»¶...")
            start_time = time.time()
            # åˆå§‹åŒ–123å®¢æˆ·ç«¯
            client = init_123_client()

            # è½¬å­˜æ–‡ä»¶
            results = []
            total_files = len(files)
            message_batch = []  # ç”¨äºå­˜å‚¨æ¯æ‰¹æ¶ˆæ¯(åŒ…æ‹¬æˆåŠŸå’Œå¤±è´¥)
            batch_size = 0      # æ‰¹æ¬¡å¤§å°è®¡æ•°å™¨
            total_size = 0      # ç´¯è®¡æˆåŠŸè½¬å­˜æ–‡ä»¶ä½“ç§¯(å­—èŠ‚)
            skip_count = 0      # è·³è¿‡çš„é‡å¤æ–‡ä»¶æ•°é‡
            last_etag = None    # ä¸Šä¸€ä¸ªæˆåŠŸè½¬å­˜æ–‡ä»¶çš„etag

            # åˆ›å»ºæ–‡ä»¶å¤¹ç¼“å­˜
            folder_cache = {}
            target_dir_name = common_path if common_path else 'JSONè½¬å­˜'
            # ä½¿ç”¨UPLOAD_TARGET_PIDä½œä¸ºæ ¹ç›®å½•
            target_dir_id = UPLOAD_JSON_TARGET_PID

            for i, file_info in enumerate(files):
                file_path = file_info.get('path', '')
                
                # æ„å»ºå®Œæ•´æ–‡ä»¶è·¯å¾„
                if common_path:
                    file_path = f"{common_path}/{file_path}"
                etag = file_info.get('etag', '')
                size = int(file_info.get('size', 0))

                if not all([file_path, etag, size]):
                    results.append({
                        "success": False,
                        "file_name": file_path or "æœªçŸ¥æ–‡ä»¶",
                        "error": "æ–‡ä»¶ä¿¡æ¯ä¸å®Œæ•´"
                    })
                    continue

                try:
                    # å¤„ç†æ–‡ä»¶è·¯å¾„
                    path_parts = file_path.split('/')
                    file_name = path_parts.pop()
                    parent_id = target_dir_id

                    # åˆ›å»ºç›®å½•ç»“æ„
                    current_path = ""
                    for part in path_parts:
                        if not part:
                            continue

                        current_path = f"{current_path}/{part}" if current_path else part
                        cache_key = f"{parent_id}/{current_path}"

                        # æ£€æŸ¥ç¼“å­˜
                        if cache_key in folder_cache:
                            parent_id = folder_cache[cache_key]
                            continue

                        # åˆ›å»ºæ–°æ–‡ä»¶å¤¹ï¼ˆå¸¦é‡è¯•ï¼‰
                        retry_count = 3
                        folder = None
                        while retry_count > 0:
                            try:
                                folder = client.fs_mkdir(part, parent_id=parent_id, duplicate=1)     
                                time.sleep(0.2)                  
                                check_response(folder)
                                break
                            except Exception as e:
                                retry_count -= 1
                                logger.warning(f"åˆ›å»ºæ–‡ä»¶å¤¹ {part} å¤±è´¥ (å‰©ä½™é‡è¯•: {retry_count}): {str(e)}")
                                time.sleep(31)

                        if not folder:
                            logger.warning(f"åˆ›å»ºæ–‡ä»¶å¤¹å¤±è´¥: {part}ï¼Œå°†ä½¿ç”¨å½“å‰ç›®å½•")
                        else:
                            folder_id = folder["data"]["Info"]["FileId"]
                            folder_cache[cache_key] = folder_id
                            parent_id = folder_id
                        #time.sleep(1/get_int_env("ENV_FILE_PER_SECOND", 5))  # é¿å…é™æµ

                    # å¤„ç†ETag
                    if uses_v2_etag:
                        # å®ç°Base62 ETagè½¬Hexï¼ˆå‚è€ƒ123pan_botä¸­çš„å®ç°ï¼‰
                        etag = optimized_etag_to_hex(etag, True)

                    # ç§’ä¼ æ–‡ä»¶ï¼ˆå¸¦é‡è¯•ï¼‰
                    retry_count = 3
                    rapid_resp = None
                    while retry_count > 0:
                        # æ£€æŸ¥etagæ˜¯å¦ä¸ä¸Šä¸€ä¸ªæˆåŠŸè½¬å­˜çš„æ–‡ä»¶ç›¸åŒ
                        if last_etag == etag:
                            skip_count += 1
                            logger.info(f"è·³è¿‡é‡å¤æ–‡ä»¶: {file_path}")
                            rapid_resp = {"data": {"Reuse": True, "Skip": True}, "code": 0}  # æ ‡è®°ä¸ºè·³è¿‡
                            break
                        
                        try:
                            rapid_resp = client.upload_file_fast(
                                file_name=file_name,
                                parent_id=parent_id,
                                file_md5=robust_normalize_md5(etag),
                                file_size=size,
                                duplicate=1
                            )
                            check_response(rapid_resp)
                            break
                        except Exception as e:
                            retry_count -= 1
                            logger.warning(f"è½¬å­˜æ–‡ä»¶ {file_name} å¤±è´¥ (å‰©ä½™é‡è¯•: {retry_count}): {str(e)}")
                            if rapid_resp and ("Etag" in rapid_resp.get("message", {})):
                                break                            
                            time.sleep(31)

                    if rapid_resp is None:
                        # å¤„ç†æ‰€æœ‰é‡è¯•å¤±è´¥ä¸” rapid_resp ä¸º None çš„åœºæ™¯
                        error_msg = "ç§’ä¼ å¤±è´¥ï¼šæ¥å£è¿”å›ç©ºå€¼ä¸”é‡è¯•è€—å°½"
                        results.append({
                            "success": False,
                            "file_name": file_path,
                            "error": error_msg
                        })
                        dir_path, file_name = os.path.split(file_path)
                        msg = {
                            'status': 'âŒ',
                            'dir': dir_path,
                            'file': f"{file_name} ({error_msg})"
                        }
                        message_batch.append(msg)
                        batch_size += 1
                        logger.error(f"{msg['status']}:{msg['dir']}/{msg['file']}")
                    elif rapid_resp.get("code") == 0 and rapid_resp.get("data", {}) and rapid_resp.get("data", {}).get("Reuse", False):
                        # æ£€æŸ¥æ˜¯å¦æ˜¯è·³è¿‡çš„æ–‡ä»¶
                        if rapid_resp.get("data", {}).get("Skip"):
                            # è§£æè·¯å¾„ç»“æ„
                            dir_path, file_name = os.path.split(file_path)
                            msg = {
                                'status': 'ğŸ”„',
                                'dir': dir_path,
                                'file': f"{file_name} (é‡å¤è·³è¿‡)"
                            }
                            message_batch.append(msg)
                            batch_size += 1
                            logger.info(f"{msg['status']}:{msg['dir']}/{msg['file']}")
                        else:
                            # æ›´æ–°ä¸Šä¸€ä¸ªæˆåŠŸè½¬å­˜æ–‡ä»¶çš„etag
                            last_etag = etag
                            results.append({
                                "success": True,
                                "file_name": file_path,
                                "file_id": rapid_resp.get("data", {}).get("FileId", ""),
                                "size": size
                            })
                            total_size += size
                            # è§£æè·¯å¾„ç»“æ„
                            dir_path, file_name = os.path.split(file_path)
                            msg = {
                                'status': 'âœ…',
                                'dir': dir_path,
                                'file': file_name
                            }
                            message_batch.append(msg)
                            batch_size += 1
                            logger.info(f"{msg['status']}:{msg['dir']}/{msg['file']}")

                    else:
                        results.append({
                            "success": False,
                            "file_name": file_path,
                            "error": "æ­¤æ–‡ä»¶åœ¨123æœåŠ¡å™¨ä¸å­˜åœ¨ï¼Œæ— æ³•ç§’ä¼ " if rapid_resp.get("data", {}) and (rapid_resp.get("data", {}).get("Reuse", True) == False) else rapid_resp.get("message", "æœªçŸ¥é”™è¯¯")
                        })
                        # è§£æè·¯å¾„ç»“æ„
                        dir_path, file_name = os.path.split(file_path)
                        msg = {
                            'status': 'âŒ',
                            'dir': dir_path,
                            'file': f"{file_name} ({"æ­¤æ–‡ä»¶åœ¨123æœåŠ¡å™¨ä¸å­˜åœ¨ï¼Œæ— æ³•ç§’ä¼ " if rapid_resp.get("data", {}) and (rapid_resp.get("data", {}).get("Reuse", True) == False) else rapid_resp.get("message", "æœªçŸ¥é”™è¯¯")})"
                        }
                        message_batch.append(msg)
                        batch_size += 1
                        logger.info(f"{msg['status']}:{msg['dir']}/{msg['file']}")
                                        
                    # æ¯10æ¡æ¶ˆæ¯å‘é€ä¸€æ¬¡
                    if batch_size % 10 == 0:
                        # ç”Ÿæˆæ ‘çŠ¶ç»“æ„æ¶ˆæ¯
                        tree_messages = defaultdict(lambda: {'âœ…': [], 'âŒ': [], 'ğŸ”„': []})
                        for entry in message_batch:
                            tree_messages[entry['dir']][entry['status']].append(entry['file'])
                        
                        batch_msg = []
                        for dir_path, status_files in tree_messages.items():
                            for status, files in status_files.items():
                                if files:
                                    batch_msg.append(f"--- {status} {dir_path}")
                                    for i, file in enumerate(files):
                                        prefix = '      â””â”€â”€' if i == len(files)-1 else '      â”œâ”€â”€'
                                        batch_msg.append(f"{prefix} {file}")
                        batch_msg = "\n".join(batch_msg)
                        reply_thread_pool.submit(send_reply_delete, message, f"ğŸ“Š {batch_size}/{total_files_count} ({int(batch_size/total_files_count*100)}%) ä¸ªæ–‡ä»¶å·²å¤„ç†\n\n{batch_msg}")
                        message_batch = []
                    time.sleep(1/get_int_env("ENV_FILE_PER_SECOND", 5))  # é¿å…é™æµ

                except Exception as e:
                        # è§£æè·¯å¾„ç»“æ„
                        dir_path, file_name = os.path.split(file_path)
                        msg = {
                            'status': 'âŒ',
                            'dir': dir_path,
                            'file': f"{file_name} ({str(e)})"
                        }
                        message_batch.append(msg)
                        batch_size += 1
                        logger.info(f"{msg['status']}:{msg['dir']}/{msg['file']}")
                        results.append({
                            "success": False,
                            "file_name": file_path,
                            "error": str(e)
                        })
                        # æ¯10æ¡æ¶ˆæ¯å‘é€ä¸€æ¬¡
                        if batch_size % 10 == 0:
                            # ç”Ÿæˆæ ‘çŠ¶ç»“æ„æ¶ˆæ¯
                            tree_messages = defaultdict(lambda: {'âœ…': [], 'âŒ': [], 'ğŸ”„': []})
                            for entry in message_batch:
                                tree_messages[entry['dir']][entry['status']].append(entry['file'])
                            
                            batch_msg = []
                            for dir_path, status_files in tree_messages.items():
                                for status, files in status_files.items():
                                    if files:
                                        batch_msg.append(f"--- {status} {dir_path}")
                                        for i, file in enumerate(files):
                                            prefix = '      â””â”€â”€' if i == len(files)-1 else '      â”œâ”€â”€'
                                            batch_msg.append(f"{prefix} {file}")
                            batch_msg = "\n".join(batch_msg)
                            reply_thread_pool.submit(send_reply_delete, message, f"ğŸ“Š {batch_size}/{total_files_count} ({int(batch_size/total_files_count*100)}%) ä¸ªæ–‡ä»¶å·²å¤„ç†\n\n{batch_msg}")
                            message_batch = []
                        time.sleep(1/get_int_env("ENV_FILE_PER_SECOND", 5))  # é¿å…é™æµ

            # å‘é€å‰©ä½™çš„æ¶ˆæ¯
            if message_batch:
                # ç”Ÿæˆæ ‘çŠ¶ç»“æ„æ¶ˆæ¯
                tree_messages = defaultdict(lambda: {'âœ…': [], 'âŒ': [], 'ğŸ”„': []})
                for entry in message_batch:
                    tree_messages[entry['dir']][entry['status']].append(entry['file'])
                
                batch_msg = []
                for dir_path, status_files in tree_messages.items():
                    for status, files in status_files.items():
                        if files:
                            batch_msg.append(f"--- {status} {dir_path}")
                            for i, file in enumerate(files):
                                prefix = '      â””â”€â”€' if i == len(files)-1 else '      â”œâ”€â”€'
                                batch_msg.append(f"{prefix} {file}")
                batch_msg = "\n".join(batch_msg)
                reply_thread_pool.submit(send_reply_delete, message, f"ğŸ“Š {batch_size}/{total_files_count} ({int(batch_size/total_files_count*100)}%) ä¸ªæ–‡ä»¶å·²å¤„ç†\n\n{batch_msg}")

            # ç»“æŸè®¡æ—¶å¹¶è®¡ç®—è€—æ—¶
            end_time = time.time()
            elapsed_time = end_time - start_time
            hours, remainder = divmod(int(elapsed_time), 3600)
            minutes, seconds = divmod(remainder, 60)
            time_str = f"{hours:02d}:{minutes:02d}:{seconds:02d}"

            # å‘é€è½¬å­˜ç»“æœ
            success_count = sum(1 for r in results if r['success'])
            fail_count = len(results) - success_count

            # å°†å­—èŠ‚è½¬æ¢ä¸ºGB (1GB = 1024^3 B)
            total_size_gb = total_size / (1024 ** 3)
            size_str = f"{total_size_gb:.2f}GB"

            # å¤„ç†JSONæ–‡ä»¶ä¸­çš„æ€»ä½“ç§¯
            total_size_json_gb = total_size_json / (1024 ** 3)
            total_size_json_str = f"{total_size_json_gb:.2f}GB"

            # è®¡ç®—å¹³å‡æ–‡ä»¶å¤§å°
            avg_size = total_size / success_count if success_count > 0 else 0
            avg_size_gb = avg_size / (1024 ** 3)
            avg_size_str = f"{avg_size_gb:.2f}GB" if avg_size_gb >= 0.01 else f"{avg_size / (1024 ** 2):.2f}MB"
            # æ·»åŠ è·³è¿‡çš„é‡å¤æ–‡ä»¶æ•°é‡æ˜¾ç¤º
            result_msg = f"âœ… JSONæ–‡ä»¶è½¬å­˜å®Œæˆï¼\nâœ…æˆåŠŸ: {success_count}ä¸ª\nâŒå¤±è´¥: {fail_count}ä¸ª\nğŸ”„è·³è¿‡åŒä¸€ç›®å½•ä¸‹çš„é‡å¤æ–‡ä»¶: {skip_count}ä¸ª\nğŸ“ŠæˆåŠŸè½¬å­˜ä½“ç§¯: {size_str}\nğŸ“Šå¹³å‡æ–‡ä»¶å¤§å°: {avg_size_str}\nğŸ“JSONæ–‡ä»¶ç†è®ºæ–‡ä»¶æ•°: {total_files_count}ä¸ª\nğŸ“¦JSONæ–‡ä»¶ç†è®ºæ€»ä½“ç§¯: {total_size_json_str}\nğŸ“ç›®æ ‡ç›®å½•: {target_dir_name}\nâ±ï¸è€—æ—¶: {time_str}"
            reply_thread_pool.submit(send_reply, message, f"{result_msg}")
            time.sleep(0.5)
            # æ·»åŠ å¤±è´¥æ–‡ä»¶è¯¦æƒ…
            if fail_count > 0:
                failed_files = []
                for result in results:
                    if not result["success"]:
                        # ç®€åŒ–æ–‡ä»¶åæ˜¾ç¤º
                        file_name = result["file_name"]
                        failed_files.append(f"â€¢ {file_name}ï¼ˆå¤±è´¥åŸå› ï¼š{result['error']}ï¼‰")
                # åˆ†æ‰¹å‘é€æ‰€æœ‰å¤±è´¥æ–‡ä»¶ï¼Œæ¯æ‰¹æœ€å¤š10ä¸ª
                batch_size = 10

                for idx in range(0, len(failed_files), batch_size):
                    batch = failed_files[idx:idx+batch_size]
                    batch_msg = "âŒ å¤±è´¥æ–‡ä»¶ (æ‰¹æ¬¡ {}/{}):\n".format((idx//batch_size)+1, (len(failed_files)+batch_size-1)//batch_size) + "\n".join(batch)
                    reply_thread_pool.submit(send_reply, message, batch_msg)
                    time.sleep(0.5)
        except Exception as e:
            logger.error(f"å¤„ç†JSONæ–‡ä»¶å¤±è´¥: {str(e)}")
            reply_thread_pool.submit(send_reply, message, f"âŒ å¤„ç†JSONæ–‡ä»¶å¤±è´¥:\n{str(e)}")

def save_json_file_quark(message,json_data):
    logger.info("è¿›å…¥123è½¬å­˜å¤¸å…‹")
    try:
        # å¼€å§‹è®¡æ—¶
        start_time = time.time()
        # æå–commonPathã€filesã€totalFilesCountå’ŒtotalSize
        common_path = json_data.get('commonPath', '').strip()
        if common_path.endswith('/'):
            common_path = common_path[:-1]
        files = json_data.get('files', [])
        uses_v2_etag = json_data.get('usesBase62EtagsInExport', False)
        total_files_count = json_data.get('totalFilesCount', len(files))
        total_size_json = json_data.get('totalSize', 0)

        if not files:
            # ä½¿ç”¨çº¿ç¨‹æ± å‘é€å›å¤
            reply_thread_pool.submit(send_reply, message, "å¤¸å…‹åˆ†äº«ä¸­æ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶ä¿¡æ¯ã€‚")
            return

        # ä½¿ç”¨çº¿ç¨‹æ± å‘é€å›å¤
        reply_thread_pool.submit(send_reply_delete, message, f"å¼€å§‹123è½¬å­˜å¤¸å…‹æ–‡ä»¶ä¸­çš„{len(files)}ä¸ªæ–‡ä»¶...")
        start_time = time.time()
        # åˆå§‹åŒ–123å®¢æˆ·ç«¯
        client = init_123_client()

        # è½¬å­˜æ–‡ä»¶
        results = []
        total_files = len(files)
        message_batch = []  # ç”¨äºå­˜å‚¨æ¯æ‰¹æ¶ˆæ¯(åŒ…æ‹¬æˆåŠŸå’Œå¤±è´¥)
        batch_size = 0      # æ‰¹æ¬¡å¤§å°è®¡æ•°å™¨
        total_size = 0      # ç´¯è®¡æˆåŠŸè½¬å­˜æ–‡ä»¶ä½“ç§¯(å­—èŠ‚)
        skip_count = 0      # è·³è¿‡çš„é‡å¤æ–‡ä»¶æ•°é‡
        last_etag = None    # ä¸Šä¸€ä¸ªæˆåŠŸè½¬å­˜æ–‡ä»¶çš„etag

        # åˆ›å»ºæ–‡ä»¶å¤¹ç¼“å­˜
        folder_cache = {}
        target_dir_name = common_path if common_path else 'JSONè½¬å­˜'
        # ä½¿ç”¨UPLOAD_TARGET_PIDä½œä¸ºæ ¹ç›®å½•
        target_dir_id = get_int_env("ENV_123_KUAKE_UPLOAD_PID", 0)

        for i, file_info in enumerate(files):
            file_path = file_info.get('path', '')
            
            # æ„å»ºå®Œæ•´æ–‡ä»¶è·¯å¾„
            if common_path:
                file_path = f"{common_path}/{file_path}"
            etag = file_info.get('etag', '')
            size = int(file_info.get('size', 0))

            if not all([file_path, etag, size]):
                results.append({
                    "success": False,
                    "file_name": file_path or "æœªçŸ¥æ–‡ä»¶",
                    "error": "æ–‡ä»¶ä¿¡æ¯ä¸å®Œæ•´"
                })
                continue

            try:
                # å¤„ç†æ–‡ä»¶è·¯å¾„
                path_parts = file_path.split('/')
                file_name = path_parts.pop()
                parent_id = target_dir_id

                # åˆ›å»ºç›®å½•ç»“æ„
                current_path = ""
                for part in path_parts:
                    if not part:
                        continue

                    current_path = f"{current_path}/{part}" if current_path else part
                    cache_key = f"{parent_id}/{current_path}"

                    # æ£€æŸ¥ç¼“å­˜
                    if cache_key in folder_cache:
                        parent_id = folder_cache[cache_key]
                        continue

                    # åˆ›å»ºæ–°æ–‡ä»¶å¤¹ï¼ˆå¸¦é‡è¯•ï¼‰
                    retry_count = 3
                    folder = None
                    while retry_count > 0:
                        try:
                            folder = client.fs_mkdir(part, parent_id=parent_id, duplicate=1)     
                            time.sleep(0.2)                  
                            check_response(folder)
                            break
                        except Exception as e:
                            retry_count -= 1
                            logger.warning(f"åˆ›å»ºæ–‡ä»¶å¤¹ {part} å¤±è´¥ (å‰©ä½™é‡è¯•: {retry_count}): {str(e)}")
                            time.sleep(31)

                    if not folder:
                        logger.warning(f"åˆ›å»ºæ–‡ä»¶å¤¹å¤±è´¥: {part}ï¼Œå°†ä½¿ç”¨å½“å‰ç›®å½•")
                    else:
                        folder_id = folder["data"]["Info"]["FileId"]
                        folder_cache[cache_key] = folder_id
                        parent_id = folder_id
                    #time.sleep(1/get_int_env("ENV_FILE_PER_SECOND", 5))  # é¿å…é™æµ

                # å¤„ç†ETag
                if uses_v2_etag:
                    # å®ç°Base62 ETagè½¬Hexï¼ˆå‚è€ƒ123pan_botä¸­çš„å®ç°ï¼‰
                    etag = optimized_etag_to_hex(etag, True)

                # ç§’ä¼ æ–‡ä»¶ï¼ˆå¸¦é‡è¯•ï¼‰
                retry_count = 3
                rapid_resp = None
                while retry_count > 0:
                    # æ£€æŸ¥etagæ˜¯å¦ä¸ä¸Šä¸€ä¸ªæˆåŠŸè½¬å­˜çš„æ–‡ä»¶ç›¸åŒ
                    if last_etag == etag:
                        skip_count += 1
                        logger.info(f"è·³è¿‡é‡å¤æ–‡ä»¶: {file_path}")
                        rapid_resp = {"data": {"Reuse": True, "Skip": True}, "code": 0}  # æ ‡è®°ä¸ºè·³è¿‡
                        break
                    
                    try:
                        rapid_resp = client.upload_file_fast(
                            file_name=file_name,
                            parent_id=parent_id,
                            file_md5=robust_normalize_md5(etag),
                            file_size=size,
                            duplicate=1
                        )
                        check_response(rapid_resp)
                        break
                    except Exception as e:
                        retry_count -= 1
                        logger.warning(f"è½¬å­˜æ–‡ä»¶ {file_name} å¤±è´¥ (å‰©ä½™é‡è¯•: {retry_count}): {str(e)}")
                        if rapid_resp and ("åŒåæ–‡ä»¶" in rapid_resp.get("message", {})):
                            reply_thread_pool.submit(send_reply, message, rapid_resp.get("message", {}))
                        if rapid_resp and ("Etag" in rapid_resp.get("message", {})):
                            break
                        if rapid_resp and ("æ–‡ä»¶ä¿¡æ¯" in rapid_resp.get("message", {})):
                            reply_thread_pool.submit(send_reply, message, "è¯·æ£€æŸ¥å¤¸å…‹çš„Cookieæ˜¯å¦è¿‡æœŸï¼Œæˆ–æ˜¯å¦æ·»åŠ - NO_PROXY=*.quark.cn")
                            break
                        time.sleep(31)

                if rapid_resp is None:
                    # å¤„ç†æ‰€æœ‰é‡è¯•å¤±è´¥ä¸” rapid_resp ä¸º None çš„åœºæ™¯
                    error_msg = "ç§’ä¼ å¤±è´¥ï¼šæ¥å£è¿”å›ç©ºå€¼ä¸”é‡è¯•è€—å°½"
                    results.append({
                        "success": False,
                        "file_name": file_path,
                        "error": error_msg
                    })
                    dir_path, file_name = os.path.split(file_path)
                    msg = {
                        'status': 'âŒ',
                        'dir': dir_path,
                        'file': f"{file_name} ({error_msg})"
                    }
                    message_batch.append(msg)
                    batch_size += 1
                    logger.error(f"{msg['status']}:{msg['dir']}/{msg['file']}")
                elif rapid_resp.get("code") == 0 and rapid_resp.get("data", {}) and rapid_resp.get("data", {}).get("Reuse", False):
                    # æ£€æŸ¥æ˜¯å¦æ˜¯è·³è¿‡çš„æ–‡ä»¶
                    if rapid_resp.get("data", {}).get("Skip"):
                        # è§£æè·¯å¾„ç»“æ„
                        dir_path, file_name = os.path.split(file_path)
                        msg = {
                            'status': 'ğŸ”„',
                            'dir': dir_path,
                            'file': f"{file_name} (é‡å¤è·³è¿‡)"
                        }
                        message_batch.append(msg)
                        batch_size += 1
                        logger.info(f"{msg['status']}:{msg['dir']}/{msg['file']}")
                    else:
                        # æ›´æ–°ä¸Šä¸€ä¸ªæˆåŠŸè½¬å­˜æ–‡ä»¶çš„etag
                        last_etag = etag
                        results.append({
                            "success": True,
                            "file_name": file_path,
                            "file_id": rapid_resp.get("data", {}).get("FileId", ""),
                            "size": size
                        })
                        total_size += size
                        # è§£æè·¯å¾„ç»“æ„
                        dir_path, file_name = os.path.split(file_path)
                        msg = {
                            'status': 'âœ…',
                            'dir': dir_path,
                            'file': file_name
                        }
                        message_batch.append(msg)
                        batch_size += 1
                        logger.info(f"{msg['status']}:{msg['dir']}/{msg['file']}")

                else:
                    results.append({
                        "success": False,
                        "file_name": file_path,
                        "error": "æ­¤æ–‡ä»¶åœ¨123æœåŠ¡å™¨ä¸å­˜åœ¨ï¼Œæ— æ³•ç§’ä¼ " if rapid_resp.get("data", {}) and (rapid_resp.get("data", {}).get("Reuse", True) == False) else rapid_resp.get("message", "æœªçŸ¥é”™è¯¯")
                    })
                    # è§£æè·¯å¾„ç»“æ„
                    dir_path, file_name = os.path.split(file_path)
                    msg = {
                        'status': 'âŒ',
                        'dir': dir_path,
                        'file': f"{file_name} ({"æ­¤æ–‡ä»¶åœ¨123æœåŠ¡å™¨ä¸å­˜åœ¨ï¼Œæ— æ³•ç§’ä¼ " if rapid_resp.get("data", {}) and (rapid_resp.get("data", {}).get("Reuse", True) == False) else rapid_resp.get("message", "æœªçŸ¥é”™è¯¯")})"
                    }
                    message_batch.append(msg)
                    batch_size += 1
                    logger.info(f"{msg['status']}:{msg['dir']}/{msg['file']}")
                                    
                # æ¯10æ¡æ¶ˆæ¯å‘é€ä¸€æ¬¡
                if batch_size % 10 == 0:
                    # ç”Ÿæˆæ ‘çŠ¶ç»“æ„æ¶ˆæ¯
                    tree_messages = defaultdict(lambda: {'âœ…': [], 'âŒ': [], 'ğŸ”„': []})
                    for entry in message_batch:
                        tree_messages[entry['dir']][entry['status']].append(entry['file'])
                    
                    batch_msg = []
                    for dir_path, status_files in tree_messages.items():
                        for status, files in status_files.items():
                            if files:
                                batch_msg.append(f"--- {status} {dir_path}")
                                for i, file in enumerate(files):
                                    prefix = '      â””â”€â”€' if i == len(files)-1 else '      â”œâ”€â”€'
                                    batch_msg.append(f"{prefix} {file}")
                    batch_msg = "\n".join(batch_msg)
                    reply_thread_pool.submit(send_reply_delete, message, f"ğŸ“Š {batch_size}/{total_files_count} ({int(batch_size/total_files_count*100)}%) ä¸ªæ–‡ä»¶å·²å¤„ç†\n\n{batch_msg}")
                    message_batch = []
                time.sleep(1/get_int_env("ENV_FILE_PER_SECOND", 5))  # é¿å…é™æµ

            except Exception as e:
                    # è§£æè·¯å¾„ç»“æ„
                    dir_path, file_name = os.path.split(file_path)
                    msg = {
                        'status': 'âŒ',
                        'dir': dir_path,
                        'file': f"{file_name} ({str(e)})"
                    }
                    message_batch.append(msg)
                    batch_size += 1
                    logger.info(f"{msg['status']}:{msg['dir']}/{msg['file']}")
                    results.append({
                        "success": False,
                        "file_name": file_path,
                        "error": str(e)
                    })
                    # æ¯10æ¡æ¶ˆæ¯å‘é€ä¸€æ¬¡
                    if batch_size % 10 == 0:
                        # ç”Ÿæˆæ ‘çŠ¶ç»“æ„æ¶ˆæ¯
                        tree_messages = defaultdict(lambda: {'âœ…': [], 'âŒ': [], 'ğŸ”„': []})
                        for entry in message_batch:
                            tree_messages[entry['dir']][entry['status']].append(entry['file'])
                        
                        batch_msg = []
                        for dir_path, status_files in tree_messages.items():
                            for status, files in status_files.items():
                                if files:
                                    batch_msg.append(f"--- {status} {dir_path}")
                                    for i, file in enumerate(files):
                                        prefix = '      â””â”€â”€' if i == len(files)-1 else '      â”œâ”€â”€'
                                        batch_msg.append(f"{prefix} {file}")
                        batch_msg = "\n".join(batch_msg)
                        reply_thread_pool.submit(send_reply_delete, message, f"ğŸ“Š {batch_size}/{total_files_count} ({int(batch_size/total_files_count*100)}%) ä¸ªæ–‡ä»¶å·²å¤„ç†\n\n{batch_msg}")
                        message_batch = []
                    time.sleep(1/get_int_env("ENV_FILE_PER_SECOND", 5))  # é¿å…é™æµ

        # å‘é€å‰©ä½™çš„æ¶ˆæ¯
        if message_batch:
            # ç”Ÿæˆæ ‘çŠ¶ç»“æ„æ¶ˆæ¯
            tree_messages = defaultdict(lambda: {'âœ…': [], 'âŒ': [], 'ğŸ”„': []})
            for entry in message_batch:
                tree_messages[entry['dir']][entry['status']].append(entry['file'])
            
            batch_msg = []
            for dir_path, status_files in tree_messages.items():
                for status, files in status_files.items():
                    if files:
                        batch_msg.append(f"--- {status} {dir_path}")
                        for i, file in enumerate(files):
                            prefix = '      â””â”€â”€' if i == len(files)-1 else '      â”œâ”€â”€'
                            batch_msg.append(f"{prefix} {file}")
            batch_msg = "\n".join(batch_msg)
            reply_thread_pool.submit(send_reply_delete, message, f"ğŸ“Š {batch_size}/{total_files_count} ({int(batch_size/total_files_count*100)}%) ä¸ªæ–‡ä»¶å·²å¤„ç†\n\n{batch_msg}")

        # ç»“æŸè®¡æ—¶å¹¶è®¡ç®—è€—æ—¶
        end_time = time.time()
        elapsed_time = end_time - start_time
        hours, remainder = divmod(int(elapsed_time), 3600)
        minutes, seconds = divmod(remainder, 60)
        time_str = f"{hours:02d}:{minutes:02d}:{seconds:02d}"

        # å‘é€è½¬å­˜ç»“æœ
        success_count = sum(1 for r in results if r['success'])
        fail_count = len(results) - success_count

        # å°†å­—èŠ‚è½¬æ¢ä¸ºGB (1GB = 1024^3 B)
        total_size_gb = total_size / (1024 ** 3)
        size_str = f"{total_size_gb:.2f}GB"

        # å¤„ç†JSONæ–‡ä»¶ä¸­çš„æ€»ä½“ç§¯
        total_size_json_gb = total_size_json / (1024 ** 3)
        total_size_json_str = f"{total_size_json_gb:.2f}GB"

        # è®¡ç®—å¹³å‡æ–‡ä»¶å¤§å°
        avg_size = total_size / success_count if success_count > 0 else 0
        avg_size_gb = avg_size / (1024 ** 3)
        avg_size_str = f"{avg_size_gb:.2f}GB" if avg_size_gb >= 0.01 else f"{avg_size / (1024 ** 2):.2f}MB"
        # æ·»åŠ è·³è¿‡çš„é‡å¤æ–‡ä»¶æ•°é‡æ˜¾ç¤º
        result_msg = f"âœ… 123è½¬å­˜å¤¸å…‹å®Œæˆï¼\nâœ…æˆåŠŸ: {success_count}ä¸ª\nâŒå¤±è´¥: {fail_count}ä¸ª\nğŸ”„è·³è¿‡åŒä¸€ç›®å½•ä¸‹çš„é‡å¤æ–‡ä»¶: {skip_count}ä¸ª\nğŸ“ŠæˆåŠŸè½¬å­˜ä½“ç§¯: {size_str}\nğŸ“Šå¹³å‡æ–‡ä»¶å¤§å°: {avg_size_str}\nğŸ“å¤¸å…‹åˆ†äº«ç†è®ºæ–‡ä»¶æ•°: {total_files_count}ä¸ª\nâ±ï¸è€—æ—¶: {time_str}"
        reply_thread_pool.submit(send_reply, message, f"{result_msg}")
        time.sleep(0.5)
        # æ·»åŠ å¤±è´¥æ–‡ä»¶è¯¦æƒ…
        if fail_count > 0:
            failed_files = []
            for result in results:
                if not result["success"]:
                    # ç®€åŒ–æ–‡ä»¶åæ˜¾ç¤º
                    file_name = result["file_name"]
                    failed_files.append(f"â€¢ {file_name}ï¼ˆå¤±è´¥åŸå› ï¼š{result['error']}ï¼‰")
            # åˆ†æ‰¹å‘é€æ‰€æœ‰å¤±è´¥æ–‡ä»¶ï¼Œæ¯æ‰¹æœ€å¤š10ä¸ª
            batch_size = 10

            for idx in range(0, len(failed_files), batch_size):
                batch = failed_files[idx:idx+batch_size]
                batch_msg = "âŒ å¤±è´¥æ–‡ä»¶ (æ‰¹æ¬¡ {}/{}):\n".format((idx//batch_size)+1, (len(failed_files)+batch_size-1)//batch_size) + "\n".join(batch)
                reply_thread_pool.submit(send_reply, message, batch_msg)
                time.sleep(0.5)
    except Exception as e:
        logger.error(f"å¤„ç†å¤¸å…‹æ–‡ä»¶å¤±è´¥: {str(e)}")
        reply_thread_pool.submit(send_reply, message, f"âŒ å¤„ç†å¤¸å…‹æ–‡ä»¶å¤±è´¥:\n{str(e)}")

# Base62å­—ç¬¦è¡¨ï¼ˆ123äº‘ç›˜V2 APIä½¿ç”¨ï¼‰
BASE62_CHARS = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'

def optimized_etag_to_hex(etag, is_v2=False):
    """å°†Base62ç¼–ç çš„ETagè½¬æ¢ä¸ºåå…­è¿›åˆ¶æ ¼å¼ï¼ˆå‚è€ƒ123pan_botä¸­çš„å®ç°ï¼‰"""
    if not is_v2:
        return etag
    
    try:
        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„MD5æ ¼å¼ï¼ˆ32ä½åå…­è¿›åˆ¶ï¼‰
        if len(etag) == 32 and all(c in '0123456789abcdefABCDEF' for c in etag):
            return etag.lower()
        
        # è½¬æ¢Base62åˆ°åå…­è¿›åˆ¶
        num = 0
        for char in etag:
            if char not in BASE62_CHARS:
                logger.error(f"âŒ ETagåŒ…å«æ— æ•ˆå­—ç¬¦: {char}")
                return etag
            num = num * 62 + BASE62_CHARS.index(char)
        
        # è½¬æ¢ä¸ºåå…­è¿›åˆ¶å¹¶ç¡®ä¿32ä½
        hex_str = hex(num)[2:].lower()
        if len(hex_str) > 32:
            # å–å32ä½
            hex_str = hex_str[-32:]
            logger.warning(f"ETagè½¬æ¢åé•¿åº¦è¶…è¿‡32ä½ï¼Œæˆªæ–­ä¸º: {hex_str}")
        elif len(hex_str) < 32:
            # å‰é¢è¡¥é›¶
            hex_str = hex_str.zfill(32)
            logger.warning(f"ETagè½¬æ¢åä¸è¶³32ä½ï¼Œè¡¥é›¶å: {hex_str}")
        
        # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„MD5
        if len(hex_str) != 32 or not all(c in '0123456789abcdef' for c in hex_str):
            logger.error(f"âŒ è½¬æ¢åETagæ ¼å¼æ— æ•ˆ: {hex_str}")
            return etag
        
        return hex_str
    except Exception as e:
        logger.error(f"âŒ ETagè½¬æ¢å¤±è´¥: {str(e)}")
        return etag

# æ³¨å†Œæ–‡æ¡£æ¶ˆæ¯å¤„ç†å™¨ï¼ˆå·²ç§»è‡³start_bot_threadå‡½æ•°å†…éƒ¨ï¼‰
# bot.message_handler(content_types=['document'])(process_json_file)

# å®šä¹‰botçº¿ç¨‹å˜é‡
bot_thread = None

def start_bot_thread():
    global bot
    # ç¡®ä¿botå®ä¾‹å­˜åœ¨
    if not bot:
        bot = telebot.TeleBot(TG_BOT_TOKEN)
    while True:
        try:
            #bot.polling(none_stop=True, interval=1)
            bot.infinity_polling(logger_level=logging.ERROR)
        except Exception as e:
            logger.warning(f"ä»£ç†ç½‘ç»œä¸ç¨³å®šï¼Œä¸TGå°è¯•é‡è¿ä¸­...\né”™è¯¯åŸå› :{str(e)}")
    return threading.current_thread()



def check_task():
    global bot_thread
    # æ£€æŸ¥botçº¿ç¨‹çŠ¶æ€ï¼ˆå›ºå®š20ç§’æ£€æŸ¥ä¸€æ¬¡ï¼‰
    if not bot_thread or not bot_thread.is_alive():
        logger.warning(f"ä»£ç†ç½‘ç»œä¸ç¨³å®šï¼Œä¸TGå°è¯•é‡è¿ä¸­...")
        bot_thread = threading.Thread(target=start_bot_thread, daemon=True)
        bot_thread.start()

if __name__ == "__mp_main__":
    from bot115 import tg_115monitor
    from bot189 import tg_189monitor,Cloud189
    client189 = Cloud189()
    ENV_189_CLIENT_ID = os.getenv("ENV_189_CLIENT_ID","")
    ENV_189_CLIENT_SECRET = os.getenv("ENV_189_CLIENT_SECRET","")

    if (ENV_189_CLIENT_ID and ENV_189_CLIENT_SECRET):
        logger.info("å¤©ç¿¼äº‘ç›˜æ­£åœ¨å°è¯•ç™»å½• ...")
        client189.login(ENV_189_CLIENT_ID, ENV_189_CLIENT_SECRET)

def main():     
    from server import app
    flask_thread = threading.Thread(target=lambda: app.run(host='0.0.0.0', port=12366, debug=False, use_reloader=False))
    flask_thread.daemon = True
    flask_thread.start()
    while (os.getenv("ENV_WEB_PASSPORT", "") == "") or (os.getenv("ENV_123_CLIENT_ID", "") == ""):
        try:
            logger.warning("è¯·æ£€æŸ¥docker-compose.ymlä¸­çš„ ENV_WEB_PASSPORT ä»¥åŠé…ç½®webé¡µé¢çš„ ENV_123_CLIENT_ID æ˜¯å¦å¡«å†™å®Œæ•´ï¼Œå¯å‰å¾€ https://hub.docker.com/r/dydydd/123bot æŸ¥çœ‹éƒ¨ç½²æ–¹æ³•")
            bot.send_message(TG_ADMIN_USER_ID,f"è¯·æ£€æŸ¥docker-compose.ymlä¸­çš„ ENV_WEB_PASSPORT ä»¥åŠé…ç½®webé¡µé¢çš„ ENV_123_CLIENT_ID æ˜¯å¦å¡«å†™å®Œæ•´ï¼Œå¯å‰å¾€ https://hub.docker.com/r/dydydd/123bot æŸ¥çœ‹éƒ¨ç½²æ–¹æ³•")
        except Exception as e:
            logger.error(f"å‘é€æ¶ˆæ¯å¤±è´¥: {str(e)}")
        time.sleep(60)
    threading.Thread(target=ptto123, daemon=True).start()
    #print(f"å¼€å§‹ç›‘æ§: {CHANNEL_URL}")
    logger.info(f"123è½¬å­˜ç›®æ ‡ç›®å½•ID: {UPLOAD_TARGET_PID} | æ£€æŸ¥é—´éš”: {CHECK_INTERVAL}åˆ†é’Ÿ")
    init_database()
    client = init_123_client()

    global bot_thread
    # åˆå§‹å¯åŠ¨botçº¿ç¨‹
    bot_thread = threading.Thread(target=start_bot_thread, daemon=True)
    bot_thread.start()
    schedule.every(20).seconds.do(check_task)

    if get_int_env("ENV_189_TGMONITOR_SWITCH", 0):
        
        try:            
            # è¯»å–189æ¸…ç†é…ç½®
            env_189_clear_pid = os.getenv("ENV_189_CLEAR_PID", "")
            env_189_clear_period = get_int_env("ENV_189_CLEAR_PERIOD", 6)
            clear_folder_ids = [pid.strip() for pid in env_189_clear_pid.split(",") if pid.strip()]
            
            # å®šä¹‰å®šæ—¶æ¸…ç†å‡½æ•°
            def clear_189_folders():
                logger.info(f"===== å¼€å§‹æ‰§è¡Œå¤©ç¿¼äº‘ç›˜æ–‡ä»¶å¤¹æ¸…ç†ä»»åŠ¡ï¼ˆ{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}ï¼‰=====")
                try:
                    # å°è¯•åˆ é™¤æ–‡ä»¶å¤¹å†…å®¹ï¼ˆä¸æ£€æŸ¥ç™»å½•çŠ¶æ€ï¼Œä¾èµ–æ–¹æ³•å†…éƒ¨å¤„ç†ï¼‰
                    for folder_id in clear_folder_ids:
                        logger.info(f"åˆ é™¤æ–‡ä»¶å¤¹ {folder_id} ä¸­çš„å†…å®¹...")
                        try:
                            client189.delete_folder_contents(folder_id)
                            logger.info(f"æˆåŠŸåˆ é™¤æ–‡ä»¶å¤¹ {folder_id} ä¸­çš„å†…å®¹")
                        except Exception as e:
                            logger.error(f"åˆ é™¤æ–‡ä»¶å¤¹ {folder_id} å†…å®¹å¤±è´¥: {str(e)}")
                    
                    # æ¸…ç©ºå›æ”¶ç«™
                    logger.info("æ¸…ç©ºå›æ”¶ç«™...")
                    try:
                        if client189.empty_recycle_bin():
                            logger.info("æˆåŠŸæ‰§è¡Œå¤©ç¿¼ç½‘ç›˜æ–‡ä»¶æ¸…ç†ä»»åŠ¡")
                            reply_thread_pool.submit(send_message, f"âœ…æˆåŠŸæ‰§è¡Œå¤©ç¿¼ç½‘ç›˜æ¸…ç©ºå›æ”¶ç«™ä»»åŠ¡ï¼ˆ{datetime.now().strftime('%Y-%m-%d %H:%M:%S')})")
                        else:
                            logger.info("å¤©ç¿¼ç½‘ç›˜æ–‡ä»¶æ¸…ç†å¤±è´¥")
                            reply_thread_pool.submit(send_message, f"âŒå¤©ç¿¼ç½‘ç›˜æ¸…ç©ºå›æ”¶ç«™å¤±è´¥ï¼ˆ{datetime.now().strftime('%Y-%m-%d %H:%M:%S')})")
                    except Exception as e:
                        logger.error(f"æ¸…ç©ºå›æ”¶ç«™å¤±è´¥: {str(e)}")
                        reply_thread_pool.submit(send_message, f"âŒå¤©ç¿¼ç½‘ç›˜æ¸…ç©ºå›æ”¶ç«™å¤±è´¥: {str(e)}ï¼ˆ{datetime.now().strftime('%Y-%m-%d %H:%M:%S')})")
                except Exception as e:
                    logger.error(f"å¤©ç¿¼äº‘ç›˜æ¸…ç†ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}")
                logger.info("===== å¤©ç¿¼äº‘ç›˜æ–‡ä»¶å¤¹æ¸…ç†ä»»åŠ¡æ‰§è¡Œå®Œæ¯• =====")
            
            # è®¾ç½®å®šæ—¶ä»»åŠ¡ï¼Œæ¯env_189_clear_periodå°æ—¶æ‰§è¡Œä¸€æ¬¡
            if clear_folder_ids:
                logger.info(f"è®¾ç½®å¤©ç¿¼äº‘ç›˜æ–‡ä»¶å¤¹å®šæ—¶æ¸…ç†ä»»åŠ¡ï¼Œæ¯{env_189_clear_period}å°æ—¶æ‰§è¡Œä¸€æ¬¡")
                schedule.every(env_189_clear_period).hours.do(clear_189_folders)
                # ç«‹å³æ‰§è¡Œä¸€æ¬¡æ¸…ç†ä»»åŠ¡
                clear_189_folders()
            else:
                logger.info("æœªé…ç½®ENV_189_CLEAR_PIDï¼Œè·³è¿‡å¤©ç¿¼äº‘ç›˜æ–‡ä»¶å¤¹å®šæ—¶æ¸…ç†ä»»åŠ¡")
        except Exception as e:
            logger.error(f"ç™»å½•å‡ºç°é”™è¯¯: {e}")

    try:
        while True:
            logger.info(f"===== å¼€å§‹æ£€æŸ¥ï¼ˆ{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}ï¼‰ï¼Œå½“å‰ç‰ˆæœ¬ {version}=====")
            if AUTHORIZATION:
                client = init_123_client()
                new_messages = get_latest_messages()
                schedule.run_pending()
                if new_messages:
                    for msg in new_messages:
                        message_id, date_str, message_url, target_url, message_text = msg
                        logger.info(f"å¤„ç†æ–°æ¶ˆæ¯: {message_id} | {target_url}")
                        # è·å–æ’é™¤å…³é”®è¯ç¯å¢ƒå˜é‡ï¼ˆå¤šä¸ªå…³é”®è¯ç”¨|åˆ†éš”ï¼‰
                        # å½“æ’é™¤å…³é”®è¯ä¸ºç©ºæ—¶ï¼Œå…¨éƒ½ä¸æ’é™¤
                        exclude_filter = os.environ.get('ENV_EXCLUDE_FILTER', '')
                        exclude_pattern = re.compile(exclude_filter) if exclude_filter else None

                        # æ£€æŸ¥æ˜¯å¦åŒ¹é…è¿‡æ»¤æ¡ä»¶ä¸”ä¸åŒ…å«æ’é™¤å…³é”®è¯
                        is_match = filter_pattern.search(target_url) or filter_pattern.search(message_text)
                        is_excluded = exclude_pattern and (exclude_pattern.search(target_url) or exclude_pattern.search(message_text))

                        if not is_match:
                            status = "æœªè½¬å­˜"
                            result_msg = f"æœªåŒ¹é…è¿‡æ»¤æ¡ä»¶ï¼ˆ{FILTER}ï¼‰ï¼Œè·³è¿‡è½¬å­˜"
                            logger.info(result_msg)
                            time.sleep(1)
                        elif is_excluded:
                            status = "æœªè½¬å­˜"
                            result_msg = f"åŒ…å«æ’é™¤å…³é”®è¯ï¼ˆ{exclude_filter}ï¼‰ï¼Œè·³è¿‡è½¬å­˜"
                            logger.info(result_msg)
                            time.sleep(1)
                        else:
                            logger.info(f"æ¶ˆæ¯åŒ¹é…è¿‡æ»¤æ¡ä»¶ï¼ˆ{FILTER}ï¼‰ï¼Œå¼€å§‹è½¬å­˜...")
                            
                            # äºŒæ¬¡è¿‡æ»¤å…³é”®è¯é…ç½®ï¼ˆå½“æŸæ¡æ¶ˆæ¯è§¦å‘è½¬å­˜åï¼Œå¦‚è¿›ä¸€æ­¥æ»¡è¶³ä¸‹é¢çš„è¦æ±‚ï¼Œåˆ™è½¬ç§»åˆ°ç‰¹å®šçš„æ–‡ä»¶å¤¹ï¼‰
                            # æ ¼å¼ä¸ºï¼šDV:1,DOLBY VISION:2,SSTA:3 å³æ»¡è¶³DVå…³é”®è¯è½¬ç§»åˆ°IDä¸º1çš„æ–‡ä»¶å¤¹ï¼Œæ»¡è¶³SSTAå…³é”®è¯è½¬ç§»åˆ°IDä¸º3çš„æ–‡ä»¶å¤¹
                            # å¦‚æœENV_SECOND_FILTERä¸ºç©ºï¼Œåˆ™å…¨éƒ¨è½¬ç§»è‡³ENV_123_UPLOAD_PID
                            ENV_SECOND_FILTER = os.getenv("ENV_SECOND_FILTER", "")
                            transfer_id=UPLOAD_TARGET_PID
                            
                            # æ ¹æ®å…³é”®è¯ç­›é€‰å¹¶è®¾ç½®transfer_id
                            # ENV_SECOND_FILTER.strip() ç”¨äºå»é™¤å­—ç¬¦ä¸²å‰åçš„ç©ºç™½å­—ç¬¦ï¼ˆç©ºæ ¼ã€åˆ¶è¡¨ç¬¦ã€æ¢è¡Œç¬¦ç­‰ï¼‰
                            # è¿™æ ·å¯ä»¥ç¡®ä¿å³ä½¿ç¯å¢ƒå˜é‡å€¼å‰åæœ‰ç©ºæ ¼ä¹Ÿèƒ½æ­£ç¡®å¤„ç†ï¼Œé¿å…å› ç©ºç™½å­—ç¬¦å¯¼è‡´çš„é€»è¾‘é”™è¯¯
                            # å¦‚æœå»é™¤ç©ºç™½åå­—ç¬¦ä¸²ä¸ä¸ºç©ºï¼Œåˆ™æ‰§è¡ŒäºŒæ¬¡è¿‡æ»¤é€»è¾‘
                            if ENV_SECOND_FILTER.strip():
                                try:
                                    # è§£æäºŒæ¬¡è¿‡æ»¤è§„åˆ™ï¼Œæ ¼å¼ä¸ºï¼šå…³é”®è¯:æ–‡ä»¶å¤¹ID,å…³é”®è¯:æ–‡ä»¶å¤¹ID,...
                                    filter_rules = ENV_SECOND_FILTER.split(',')
                                    for rule in filter_rules:
                                        if ':' in rule:
                                            # åˆ†å‰²å…³é”®è¯å’Œæ–‡ä»¶å¤¹IDï¼Œä½†ä¿ç•™å…³é”®è¯ä¸­çš„ç©ºæ ¼ï¼ˆå¦‚"DOLBY VISION"ä¸­çš„ç©ºæ ¼ä¼šè¢«ä¿ç•™ï¼‰
                                            keyword, folder_id = rule.split(':', 1)
                                            # keyword.strip() ç”¨äºç¡®ä¿å…³é”®è¯ä¸ä¸ºç©ºå­—ç¬¦ä¸²
                                            # æ³¨æ„ï¼šå…³é”®è¯å†…éƒ¨çš„ç©ºæ ¼ï¼ˆå¦‚"DOLBY VISION"ä¸­çš„ç©ºæ ¼ï¼‰ä¸ä¼šè¢«å»é™¤ï¼Œä¼šä½œä¸ºå…³é”®è¯çš„ä¸€éƒ¨åˆ†è¿›è¡ŒåŒ¹é…
                                            if (keyword.strip() and 
                                                (keyword in message_text or 
                                                 (target_url and keyword in target_url))):
                                                transfer_id = int(folder_id.strip())
                                                logger.info(f"æ¶ˆæ¯åŒ¹é…äºŒæ¬¡è¿‡æ»¤å…³é”®è¯ '{keyword}'ï¼Œå°†è½¬å­˜åˆ°æ–‡ä»¶å¤¹ID: {folder_id}")
                                                reply_thread_pool.submit(send_message, f"æ¶ˆæ¯åŒ¹é…äºŒæ¬¡è¿‡æ»¤å…³é”®è¯ '{keyword}'ï¼Œå°†è½¬å­˜åˆ°æ–‡ä»¶å¤¹ID: {folder_id}")
                                                break
                                except Exception as e:
                                    logger.error(f"è§£æäºŒæ¬¡è¿‡æ»¤è§„åˆ™å¤±è´¥: {e}")
                                    reply_thread_pool.submit(send_message, f"è§£æäºŒæ¬¡è¿‡æ»¤è§„åˆ™å¤±è´¥: {e}")
                            if target_url:                                
                                result = transfer_shared_link_optimize(client, target_url, transfer_id)
                                if result:
                                    status = "è½¬å­˜æˆåŠŸ"
                                    result_msg = f"âœ…123äº‘ç›˜è½¬å­˜æˆåŠŸ\næ¶ˆæ¯å†…å®¹: {message_url}\né“¾æ¥: {target_url}"
                                    reply_thread_pool.submit(send_message, result_msg)
                                else:                               
                                    status = "è½¬å­˜å¤±è´¥"
                                    result_msg = f"âŒ123äº‘ç›˜è½¬å­˜å¤±è´¥\næ¶ˆæ¯å†…å®¹: {message_url}\né“¾æ¥: {target_url}"
                                    reply_thread_pool.submit(send_message, result_msg)
                            else:
                                full_links = extract_123_links_from_full_text(message_text)
                                if full_links:
                                    for link in full_links:
                                        if parse_share_link(message_text, link, transfer_id, False):
                                            status = "è½¬å­˜æˆåŠŸ"
                                            result_msg = f"âœ…123äº‘ç›˜ç§’ä¼ é“¾æ¥è½¬å­˜æˆåŠŸ\næ¶ˆæ¯å†…å®¹: {message_url}\n"
                                            reply_thread_pool.submit(send_message, result_msg)
                                        else:
                                            status = "è½¬å­˜å¤±è´¥"
                                            result_msg = f"âŒ123äº‘ç›˜ç§’ä¼ é“¾æ¥è½¬å­˜å¤±è´¥\næ¶ˆæ¯å†…å®¹: {message_url}\n"  
                                            #notifier.send_message(result_msg)     
                                else:
                                    status = "è½¬å­˜å¤±è´¥"
                                    result_msg = f"âŒ123äº‘ç›˜ç§’ä¼ é“¾æ¥è½¬å­˜å¤±è´¥\næ¶ˆæ¯å†…å®¹: {message_url}\n"  
                                    #notifier.send_message(result_msg)     
                            time.sleep(2)
                        save_message(message_id, date_str, message_url, target_url, status, result_msg)
                else:
                    logger.info("æœªå‘ç°æ–°çš„123åˆ†äº«é“¾æ¥")
            if get_int_env("ENV_115_TGMONITOR_SWITCH", 0):
                tg_115monitor()
            if get_int_env("ENV_189_TGMONITOR_SWITCH", 0):
                tg_189monitor(client189)
            logger.info(f"ä¼‘æ¯{CHECK_INTERVAL}åˆ†é’Ÿï¼Œå½“å‰ç‰ˆæœ¬ {version}...")
            total_wait_seconds = CHECK_INTERVAL * 60
            elapsed_seconds = 0
            # æ‹†åˆ†ç­‰å¾…æ—¶é—´ï¼Œæ¯1ç§’æ£€æŸ¥ä¸€æ¬¡å®šæ—¶ä»»åŠ¡ï¼ˆ20ç§’å†…ä¼šæ£€æŸ¥20æ¬¡ï¼Œæ»¡è¶³20ç§’æ£€æŸ¥ä¸€æ¬¡çš„éœ€æ±‚ï¼‰
            exit=0
            while elapsed_seconds < total_wait_seconds:
                # æ£€æŸ¥æ˜¯å¦éœ€è¦é€€å‡ºï¼ˆåœ¨ä¼‘æ¯å‰æ£€æŸ¥ï¼Œç¡®ä¿åªåœ¨è®°å½•æ—¥å¿—åé€€å‡ºï¼‰
                try:
                    # ç›´æ¥è®¿é—®should_exitå˜é‡è€Œä¸æ˜¯é€šè¿‡globals()æ£€æŸ¥
                    with should_exit.get_lock():
                        if link_process_lock.acquire(blocking=False):
                            try:
                                if should_exit.value:
                                    logger.info("æ£€æµ‹åˆ°é€€å‡ºæ ‡å¿—ï¼Œå­è¿›ç¨‹å°†åœ¨ä¼‘æ¯å‰é€€å‡º")
                                    exit=1
                                    break   
                            finally:
                                link_process_lock.release()
                except Exception as e:
                    logger.error(f"æ£€æŸ¥é€€å‡ºæ ‡å¿—æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                time.sleep(1)  # çŸ­é—´éš”ä¼‘çœ ï¼Œä¿è¯20ç§’å†…è‡³å°‘æ£€æŸ¥ä¸€æ¬¡
                elapsed_seconds += 1
            if exit:
                break
            # ç‰ˆæœ¬æ£€æŸ¥å·²ç¦ç”¨
            # try:
            #     channel_chat = bot.get_chat('@tgto123update')
            #     # è·å–ç½®é¡¶æ¶ˆæ¯ï¼ˆç›´æ¥è®¿é—®å¯¹è±¡å±æ€§ï¼Œè€Œéå­—å…¸getï¼‰
            #     pinned_message = channel_chat.pinned_message
            #     if pinned_message.message_id != newest_id:
            #         bot.send_message(TG_ADMIN_USER_ID, f"ğŸš€ tgto123 å½“å‰ç‰ˆæœ¬ä¸º{version}ï¼Œæœ€æ–°ç‰ˆæœ¬è¯·è§ï¼š\nhttps://t.me/tgto123update/{pinned_message.message_id}")
            #         bot.send_message(TG_ADMIN_USER_ID, f"è¯·æ›´æ–°æœ€æ–°ç‰ˆæœ¬")
            # except Exception as e:
            #     logger.error(f"è½¬å‘é¢‘é“æ¶ˆæ¯å¤±è´¥: {str(e)}")
    except KeyboardInterrupt:
        logger.info("ç¨‹åºå·²åœæ­¢")
    except Exception as e:
        logger.error(f"ç¨‹åºå¼‚å¸¸ç»ˆæ­¢: {str(e)}")
        #notifier.send_message(f"tgto123ï¼šç¨‹åºå¼‚å¸¸ç»ˆæ­¢: {str(e)}")
from ptto115 import ptto123process
def ptto123():
    while get_int_env("ENV_PTTO123_SWITCH", 0) or get_int_env("ENV_PTTO115_SWITCH", 0):
        try:
            ptto123process()
        except Exception as e:
            logger.error(f"ptto123çº¿ç¨‹å¼‚å¸¸ç»ˆæ­¢: {str(e)}")
            bot.send_message(TG_ADMIN_USER_ID, f"ptto123çº¿ç¨‹å¼‚å¸¸ç»ˆæ­¢: {str(e)}")
            time.sleep(300)

import threading
import multiprocessing
import signal

if __name__ == "__main__":
    # è®¾ç½®å…¨å±€é»˜è®¤æ¨¡å¼ä¸º spawn
    multiprocessing.set_start_method('spawn')
# å…¨å±€å…±äº«æ ‡å¿—ï¼Œç”¨äºé€šçŸ¥å­è¿›ç¨‹é€€å‡º
should_exit = multiprocessing.Value('b', False)

# å­è¿›ç¨‹è¿è¡Œçš„å‡½æ•°
def run_main(exit_flag):
    # å°†å…±äº«å˜é‡è®¾ç½®ä¸ºå…¨å±€å˜é‡ï¼Œä»¥ä¾¿mainå‡½æ•°å¯ä»¥è®¿é—®
    global should_exit
    should_exit = exit_flag
    try:
        main()
    except Exception as e:
        logger.error(f"å­è¿›ç¨‹è¿è¡Œå¼‚å¸¸: {str(e)}")

if __name__ == "__main__":
    # æ£€æŸ¥db\user.envæ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä»templete.envåˆ›å»º
    user_state_manager.clear_state(TG_ADMIN_USER_ID)
    user_env_path = os.path.join('db', 'user.env')
    if not os.path.exists(user_env_path):
        logger.info(f"user.envæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä»templete.envåˆ›å»º...")
        # ç¡®ä¿dbç›®å½•å­˜åœ¨
        os.makedirs('db', exist_ok=True)
        # å¤åˆ¶templete.envåˆ°dbç›®å½•å¹¶é‡å‘½åä¸ºuser.env
        if os.path.exists('templete.env'):
            shutil.copy2('templete.env', user_env_path)
            logger.info(f"æˆåŠŸåˆ›å»ºuser.envæ–‡ä»¶")
        else:
            logger.warning(f"è­¦å‘Š: templete.envæ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•åˆ›å»ºuser.env")
    os.makedirs('templates', exist_ok=True)
    os.makedirs('static', exist_ok=True)
    disclaimer_text = '''âš ï¸ å…è´£å£°æ˜ & åˆè§„è¯´æ˜

        æœ¬å·¥å…·ä»…ä¸ºæ–¹ä¾¿ç½‘ç›˜åˆ†äº«ä¸è½¬å­˜ï¼Œæ‰€æœ‰èµ„æºå‡æ¥è‡ªç½‘ç»œç”¨æˆ·çš„å…¬å¼€åˆ†äº«å†…å®¹ï¼š
        - å¼€å‘è€…éèµ„æºçš„ä¸Šä¼ è€…ã€æ‰€æœ‰è€…æˆ–ç‰ˆæƒæ–¹ï¼Œä¸å¯¹èµ„æºçš„åˆæ³•æ€§ã€å‡†ç¡®æ€§ã€å®Œæ•´æ€§æ‰¿æ‹…è´£ä»»ã€‚
        - å·¥å…·å†…ç½®AIå†…å®¹è¯†åˆ«æœºåˆ¶ï¼Œè‡ªåŠ¨è¿‡æ»¤æ¶‰æ”¿ã€è‰²æƒ…ã€æš´åŠ›ç­‰è¿è§„èµ„æºçš„åˆ†äº«åˆ›å»ºï¼Œåšå†³æŠµåˆ¶éæ³•å†…å®¹ä¼ æ’­ã€‚

        ç”¨æˆ·åœ¨ä½¿ç”¨æœ¬å·¥å…·æ—¶éœ€çŸ¥æ‚‰ï¼š
        - éœ€è‡ªè¡Œæ ¸å®èµ„æºç‰ˆæƒå½’å±ï¼Œç¡®ä¿åˆè§„ä½¿ç”¨ï¼Œé¿å…ä¾µçŠ¯ç¬¬ä¸‰æ–¹æƒç›Šï¼›
        - å¯¹ä¸‹è½½ã€å­˜å‚¨ã€ä¼ æ’­èµ„æºå¯èƒ½å¼•å‘çš„æ³•å¾‹çº çº·ã€æ•°æ®å®‰å…¨é£é™©ï¼ˆå¦‚ç—…æ¯’æ„ŸæŸ“ï¼‰ç­‰ï¼Œç”±ç”¨æˆ·è‡ªè¡Œæ‰¿æ‹…å…¨éƒ¨è´£ä»»ï¼›
        - å¼€å‘è€…ä¸å¯¹ä¸Šè¿°é£é™©å¯¼è‡´çš„ä»»ä½•æŸå¤±æ‰¿æ‹…è´£ä»»ï¼›

        - å¦‚æ‚¨ç»§ç»­ä½¿ç”¨æœ¬å·¥å…·ï¼Œåˆ™è§†ä¸ºå·²å®Œæ•´é˜…è¯»ã€ç†è§£å¹¶æ¥å—ä»¥ä¸Šæ‰€æœ‰å£°æ˜å†…å®¹ã€‚'''

    while True:
        try:            
            bot.send_message(TG_ADMIN_USER_ID,f"ğŸš€ 123botï¼šå½“å‰ç‰ˆæœ¬ {version}\né¡¹ç›®åœ°å€ï¼šhttps://github.com/dydydd/123bot è§‰å¾—å¥½ç”¨èƒ½å¦å¸®å¿™ç‚¹ä¸ªå°æ˜Ÿæ˜Ÿ\n\n{USE_METHOD}")
            bot.send_message(TG_ADMIN_USER_ID,disclaimer_text)
            # ç‰ˆæœ¬æ£€æŸ¥å·²ç¦ç”¨ï¼ˆå¼ºåˆ¶æ›´æ–°å¾ªç¯å·²ç§»é™¤ï¼‰
            # try:
            # # ç­‰å¾…botå¯¹è±¡åˆå§‹åŒ–å®Œæˆ
            #     if bot:
            #         # è·å–é¢‘é“ä¿¡æ¯ï¼ˆè¿”å›Chatå¯¹è±¡ï¼Œè€Œéå­—å…¸ï¼‰
            #         channel_chat = bot.get_chat('@tgto123update')
            #         # è·å–ç½®é¡¶æ¶ˆæ¯ï¼ˆç›´æ¥è®¿é—®å¯¹è±¡å±æ€§ï¼Œè€Œéå­—å…¸getï¼‰
            #         pinned_message = channel_chat.pinned_message
            #         while True:                    
            #             bot.send_message(TG_ADMIN_USER_ID, f"ğŸš€ tgto123 å½“å‰ç‰ˆæœ¬ä¸º{version}ï¼Œæœ€æ–°ç‰ˆæœ¬è¯·è§ï¼š\nhttps://t.me/tgto123update/{pinned_message.message_id}")
            #             bot.send_message(TG_ADMIN_USER_ID,disclaimer_text)
            #             if pinned_message.message_id == newest_id:
            #                 break
            #             logger.warning(f"è¯·æ›´æ–°æœ€æ–°ç‰ˆæœ¬ï¼Œå¦åˆ™æ— æ³•æ­£å¸¸ä½¿ç”¨")
            #             bot.send_message(TG_ADMIN_USER_ID, f"è¯·æ›´æ–°æœ€æ–°ç‰ˆæœ¬ï¼Œå¦åˆ™æ— æ³•æ­£å¸¸ä½¿ç”¨")
            #             time.sleep(60)
            break
            # except Exception as e:
            #     logger.error(f"è½¬å‘é¢‘é“æ¶ˆæ¯å¤±è´¥: {str(e)}")
        except Exception as e:
            logger.error(f"ç”±äºç½‘ç»œç­‰åŸå› æ— æ³•ä¸TG Botå»ºç«‹é€šä¿¡ï¼Œ30ç§’åé‡è¯•...: {str(e)}")
            time.sleep(30)
    # ä¸»è¿›ç¨‹æ§åˆ¶é€»è¾‘
    restart_time = time_datetime(3, 0, 0)  # è®¾ç½®åœ¨æ¯å¤©ä¸‹åˆ6:50:00é‡å¯
    
    # è®¡ç®—åˆå§‹çš„ä¸‹ä¸€æ¬¡é‡å¯æ—¶é—´æˆ³
    def calculate_next_restart_time():
        today = datetime.now().date()
        # è®¡ç®—ä»Šå¤©çš„é‡å¯æ—¶é—´æ—¶é—´æˆ³
        today_restart_time = datetime.combine(today, restart_time).timestamp()
        # å½“å‰æ—¶é—´æˆ³
        now = datetime.now().timestamp()
        # å¦‚æœå½“å‰æ—¶é—´åœ¨ä»Šå¤©çš„é‡å¯æ—¶é—´ä¹‹å‰ï¼Œåˆ™ä¸‹ä¸€æ¬¡é‡å¯æ—¶é—´ä¸ºä»Šå¤©é‡å¯æ—¶é—´
        # å¦‚æœå½“å‰æ—¶é—´å·²è¿‡ä»Šå¤©çš„é‡å¯æ—¶é—´ï¼Œåˆ™ä¸‹ä¸€æ¬¡é‡å¯æ—¶é—´ä¸ºæ˜å¤©é‡å¯æ—¶é—´
        if now < today_restart_time:
            next_restart = today_restart_time
        else:
            next_restart = datetime.combine(today + timedelta(days=1), restart_time).timestamp()
        return next_restart
    
    next_restart_time = calculate_next_restart_time()
    
    while True:
        try:
            # åˆ›å»ºå¹¶å¯åŠ¨å­è¿›ç¨‹ï¼Œä¼ é€’å…±äº«å˜é‡
            main_process = multiprocessing.Process(target=run_main, args=(should_exit,))
            main_process.daemon = False
            main_process.start()
            logger.info(f"å­è¿›ç¨‹ {main_process.pid} å·²å¯åŠ¨")
            logger.info(f"ä¸‹ä¸€æ¬¡è®¡åˆ’æ¸…ç†å†…å­˜æ—¶é—´: {datetime.fromtimestamp(next_restart_time).strftime('%Y-%m-%d %H:%M:%S')}")
            
            # ç›‘æ§å­è¿›ç¨‹å’Œé‡å¯æ—¶é—´
            while main_process.is_alive():
                # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾é‡å¯æ—¶é—´
                now = datetime.now().timestamp()
                
                if now >= next_restart_time:
                    # è®¾ç½®é€€å‡ºæ ‡å¿—ï¼Œé€šçŸ¥å­è¿›ç¨‹
                    with should_exit.get_lock():
                        should_exit.value = True
                    
                    # ç­‰å¾…å­è¿›ç¨‹é€€å‡ºï¼Œæœ€å¤šç­‰å¾…60ç§’
                    wait_time = 0
                    max_wait = 1800
                    while main_process.is_alive() and wait_time < max_wait:
                        time.sleep(1)
                        wait_time += 1
                    
                    # å¦‚æœå­è¿›ç¨‹è¿˜åœ¨è¿è¡Œï¼Œè·³è¿‡æ­¤æ¬¡é‡å¯
                    if main_process.is_alive():
                        logger.warning(f"å­è¿›ç¨‹ {main_process.pid} æœªèƒ½åœ¨è§„å®šæ—¶é—´å†…è‡ªè¡Œé€€å‡º,è·³è¿‡æ­¤æ¬¡é‡å¯")
                        with should_exit.get_lock():
                            should_exit.value = False
                        next_restart_time = calculate_next_restart_time()
                        logger.info(f"ä¸‹ä¸€æ¬¡è®¡åˆ’æ¸…ç†å†…å­˜æ—¶é—´: {datetime.fromtimestamp(next_restart_time).strftime('%Y-%m-%d %H:%M:%S')}")
                        continue

                    # é‡ç½®é€€å‡ºæ ‡å¿—
                    with should_exit.get_lock():
                        should_exit.value = False                    
                    # è®¡ç®—ä¸‹ä¸€æ¬¡é‡å¯æ—¶é—´
                    next_restart_time = calculate_next_restart_time()
                    logger.info(f"å·²å®Œæˆæ¸…ç†å†…å­˜ï¼Œä¸‹ä¸€æ¬¡è®¡åˆ’æ¸…ç†å†…å­˜æ—¶é—´: {datetime.fromtimestamp(next_restart_time).strftime('%Y-%m-%d %H:%M:%S')}")
                    break
                
                # æ¯10ç§’æ£€æŸ¥ä¸€æ¬¡
                time.sleep(10)
            
            # å­è¿›ç¨‹é€€å‡ºåï¼Œç­‰å¾…ä¸€æ®µæ—¶é—´å†é‡å¯
            if not main_process.is_alive():
                logger.info(f"å­è¿›ç¨‹ {main_process.pid} å·²é€€å‡ºï¼Œç­‰å¾…5ç§’åé‡å¯")
                time.sleep(5)
            
        except KeyboardInterrupt:
            logger.info("æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨ç»ˆæ­¢å­è¿›ç¨‹...")
            if 'main_process' in locals() and main_process.is_alive():
                try:
                    main_process.terminate()
                    main_process.join(timeout=10)
                except Exception as e:
                    logger.error(f"ç»ˆæ­¢å­è¿›ç¨‹æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            logger.info("ç¨‹åºå·²åœæ­¢")
            break
